{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sunset-general",
   "metadata": {},
   "source": [
    "# Classification Experiment: Mentions\n",
    "---\n",
    "This Notebook, includes a series of experiments, on using a node's Friends for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "homeless-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "weekly-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_credentials = []\n",
    "with open('../../../../twitter_credentials.json', 'r') as f:\n",
    "    twitter_credentials = json.load(f)\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_credentials['consumer_key'], twitter_credentials['consumer_secret'])\n",
    "auth.set_access_token(twitter_credentials['access_token_key'],twitter_credentials['access_token_secret'])\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, timeout=60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alive-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function For Text Normalization\n",
    "def clean_text(data):\n",
    "    urls = r'http\\S+'\n",
    "    non_unicode_char = r'\\W'\n",
    "    numbers = r'[0-9_]'\n",
    "    fix_whitespace = r'\\s+'\n",
    "    single_whitespace = ' '\n",
    "    \n",
    "    data = (data.replace([urls], single_whitespace, regex=True)\n",
    "                    .replace([non_unicode_char, numbers], single_whitespace, regex=True)\n",
    "                    .replace(fix_whitespace, single_whitespace, regex=True))\n",
    "    data = data.apply(lambda s: s.lower() if type(s) == str else s)\n",
    "    return data\n",
    "\n",
    "# NLP Functions\n",
    "nlp_el = spacy.load('el_core_news_md')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "STOPWORDS = set(list(spacy.lang.en.STOP_WORDS) + list(spacy.lang.el.STOP_WORDS))\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    row = [str(token) for token in nlp_el(row)]\n",
    "    return [w for w in row if w not in STOPWORDS]\n",
    "\n",
    "def tokenize_lemmatize(row):\n",
    "    return [str(token.lemma_) for token in nlp_el(row)]\n",
    "\n",
    "def tokenize_lemmatize_en(row):\n",
    "    return [str(token.lemma_) for token in nlp_en(row)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "decent-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data_hotel_nd(df):\n",
    "    df['textdata'] = clean_text(df['name'] + ' ' + df['description'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "def mentions_hotel_count(training_set):\n",
    "    model_nd = joblib.load('../classifiers/classifier_hotel_nd.sav')\n",
    "    counts = []\n",
    "    mentions = training_set['recent_100_statuses'].str.findall(r'@\\w+')\n",
    "    \n",
    "    for accs in tqdm(mentions):\n",
    "        print(f'\\nNode: {len(accs)}')\n",
    "        users = []\n",
    "        count = 0\n",
    "        \n",
    "        try:\n",
    "            for acc in accs:\n",
    "                users.append(API.get_user(screen_name=acc))\n",
    "        except tweepy.TweepError as err:\n",
    "            print(f'Error get_user: {err}')\n",
    "            \n",
    "    \n",
    "        textdata = pd.DataFrame()         \n",
    "        for user in users:\n",
    "            textdata = textdata.append({'name':user.name, 'description':user.description}, ignore_index=True)\n",
    "    \n",
    "        if not textdata.empty:\n",
    "            #Predict\n",
    "            labels = model_nd.predict(textdata)\n",
    "            textdata['labels'] = labels\n",
    "                  \n",
    "            #COUNT\n",
    "            count = len(textdata[textdata['labels'] == 1])\n",
    "        \n",
    "        counts.append(count)\n",
    "        print(f'Percentage: {count}')\n",
    "                  \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-italian",
   "metadata": {},
   "source": [
    "## Calculate Hotel Followers Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-chest",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brief-career",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "      <th>friends_hotel_count_1000</th>\n",
       "      <th>followers_hotel_count_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SophiaSuites</td>\n",
       "      <td>Sophia Collection Santorini</td>\n",
       "      <td>Luxury Suites, hotels and villas Santorini com...</td>\n",
       "      <td>513</td>\n",
       "      <td>41</td>\n",
       "      <td>127</td>\n",
       "      <td>\"The tans will fade but the memories will las...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnthiMariaApart</td>\n",
       "      <td>AnthiMariaApartments</td>\n",
       "      <td>Anthi Maria Beach Apartments is a self-caterin...</td>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "      <td>110</td>\n",
       "      <td>Our fantastic New and Improved abc online web...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wEndowproject</td>\n",
       "      <td>wEndow project</td>\n",
       "      <td>WEndow Escape Resort &amp; Villas | Tailor-made Ad...</td>\n",
       "      <td>350</td>\n",
       "      <td>344</td>\n",
       "      <td>103</td>\n",
       "      <td>https://t.co/DHuXrG8G6o For those who still d...</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paphotels</td>\n",
       "      <td>paphotels</td>\n",
       "      <td>The best of Greek hospitality! Follow us, visi...</td>\n",
       "      <td>975</td>\n",
       "      <td>1182</td>\n",
       "      <td>475</td>\n",
       "      <td>@AlbertBourla üíØüíØüíØüíØüíØ Happy Easter !!!üê£ @paphot...</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medpalace</td>\n",
       "      <td>Mediterranean Palace</td>\n",
       "      <td>A cozy 5 star hotel in the city center with an...</td>\n",
       "      <td>269</td>\n",
       "      <td>543</td>\n",
       "      <td>381</td>\n",
       "      <td>https://t.co/WPCR6KSnw2 New era!\\nNew Brand! ...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                         name  \\\n",
       "0     SophiaSuites  Sophia Collection Santorini   \n",
       "1  AnthiMariaApart         AnthiMariaApartments   \n",
       "2    wEndowproject               wEndow project   \n",
       "3        paphotels                    paphotels   \n",
       "4        medpalace         Mediterranean Palace   \n",
       "\n",
       "                                         description  statuses_count  \\\n",
       "0  Luxury Suites, hotels and villas Santorini com...             513   \n",
       "1  Anthi Maria Beach Apartments is a self-caterin...             102   \n",
       "2  WEndow Escape Resort & Villas | Tailor-made Ad...             350   \n",
       "3  The best of Greek hospitality! Follow us, visi...             975   \n",
       "4  A cozy 5 star hotel in the city center with an...             269   \n",
       "\n",
       "   friends_count  followers_count  \\\n",
       "0             41              127   \n",
       "1             25              110   \n",
       "2            344              103   \n",
       "3           1182              475   \n",
       "4            543              381   \n",
       "\n",
       "                                 recent_100_statuses  hotel  \\\n",
       "0   \"The tans will fade but the memories will las...      1   \n",
       "1   Our fantastic New and Improved abc online web...      1   \n",
       "2   https://t.co/DHuXrG8G6o For those who still d...      1   \n",
       "3   @AlbertBourla üíØüíØüíØüíØüíØ Happy Easter !!!üê£ @paphot...      1   \n",
       "4   https://t.co/WPCR6KSnw2 New era!\\nNew Brand! ...      1   \n",
       "\n",
       "   friends_hotel_count_1000  followers_hotel_count_1000  \n",
       "0                         4                          15  \n",
       "1                         5                          12  \n",
       "2                        33                          14  \n",
       "3                        67                          58  \n",
       "4                        49                          23  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-validation-set-enhanced.csv')\n",
    "validation_set = validation_set.replace(np.nan, '')\n",
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "opposite-repair",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe735d53cf34b4e8c83b98236910636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node: 25\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 22\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 14\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 102\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 5\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 115\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 16\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 101\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 2\n",
      "\n",
      "Node: 55\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 20\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 8\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 24\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 104\n",
      "Percentage: 1\n",
      "\n",
      "Node: 44\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 12\n",
      "Percentage: 11\n",
      "\n",
      "Node: 16\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 134\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 68\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 5\n",
      "Percentage: 0\n",
      "\n",
      "Node: 115\n",
      "Percentage: 4\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 3\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 109\n",
      "Percentage: 0\n",
      "\n",
      "Node: 25\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 9\n",
      "\n",
      "Node: 31\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 38\n",
      "Percentage: 0\n",
      "\n",
      "Node: 11\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 113\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 3\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 1\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 6\n",
      "Percentage: 0\n",
      "\n",
      "Node: 4\n",
      "Percentage: 0\n",
      "\n",
      "Node: 25\n",
      "Percentage: 12\n",
      "\n",
      "Node: 2\n",
      "Percentage: 1\n"
     ]
    }
   ],
   "source": [
    "hotel_mentions = mentions_hotel_count(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wired-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set['mention_hotel_count'] = hotel_mentions\n",
    "validation_set.to_csv('../../../../datasets/Hotels/classification/hotels-validation-set-enhanced.csv', index=False)\n",
    "del validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-lewis",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "enabling-regulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "      <th>friends_hotel_count_1000</th>\n",
       "      <th>followers_hotel_count_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aldemar_resorts</td>\n",
       "      <td>Aldemar Resorts</td>\n",
       "      <td>Guest satisfaction is our top priority! *Luxur...</td>\n",
       "      <td>1832</td>\n",
       "      <td>1569</td>\n",
       "      <td>2229</td>\n",
       "      <td>Summer vacation is meant to make you feel ‚õ± r...</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AquaVistaHotels</td>\n",
       "      <td>Aqua Vista Hotels</td>\n",
       "      <td>A compilation of extraordinary hotels catering...</td>\n",
       "      <td>5924</td>\n",
       "      <td>1650</td>\n",
       "      <td>2116</td>\n",
       "      <td>Thank you Greek Travel Pages for highlighting...</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eurobank_Group</td>\n",
       "      <td>Eurobank</td>\n",
       "      <td>ŒöŒ±ŒªœâœÉŒÆœÅŒ∏Œ±œÑŒµ œÉœÑŒ∑ŒΩ ŒµœÄŒØœÉŒ∑ŒºŒ∑ œÉŒµŒªŒØŒ¥Œ± œÑŒ∑œÇ Eurobank œÉ...</td>\n",
       "      <td>3284</td>\n",
       "      <td>0</td>\n",
       "      <td>2691</td>\n",
       "      <td>Œó¬†Eurobank¬†ŒµŒΩŒ∑ŒºŒµœÅœéŒΩŒµŒπ œåœÑŒπ œÑŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ¨¬†œÑŒ∑œÇ,¬†Œ∫...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white_suites</td>\n",
       "      <td>White¬†Suites Resort</td>\n",
       "      <td>White Suites Resort is a luxury beach hotel in...</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>18</td>\n",
       "      <td>Sea side holidays in Afytos, Halikidiki White...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KarenMillen</td>\n",
       "      <td>Karen Millen</td>\n",
       "      <td>Timeless, elevated ready-to-wear style for women.</td>\n",
       "      <td>10908</td>\n",
       "      <td>1409</td>\n",
       "      <td>35679</td>\n",
       "      <td>The future's bright.\\nhttps://t.co/XLpskBYi4u...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                 name  \\\n",
       "0  aldemar_resorts      Aldemar Resorts   \n",
       "1  AquaVistaHotels    Aqua Vista Hotels   \n",
       "2   Eurobank_Group             Eurobank   \n",
       "3     white_suites  White¬†Suites Resort   \n",
       "4      KarenMillen         Karen Millen   \n",
       "\n",
       "                                         description  statuses_count  \\\n",
       "0  Guest satisfaction is our top priority! *Luxur...            1832   \n",
       "1  A compilation of extraordinary hotels catering...            5924   \n",
       "2  ŒöŒ±ŒªœâœÉŒÆœÅŒ∏Œ±œÑŒµ œÉœÑŒ∑ŒΩ ŒµœÄŒØœÉŒ∑ŒºŒ∑ œÉŒµŒªŒØŒ¥Œ± œÑŒ∑œÇ Eurobank œÉ...            3284   \n",
       "3  White Suites Resort is a luxury beach hotel in...               2   \n",
       "4  Timeless, elevated ready-to-wear style for women.           10908   \n",
       "\n",
       "   friends_count  followers_count  \\\n",
       "0           1569             2229   \n",
       "1           1650             2116   \n",
       "2              0             2691   \n",
       "3             93               18   \n",
       "4           1409            35679   \n",
       "\n",
       "                                 recent_100_statuses  hotel  \\\n",
       "0   Summer vacation is meant to make you feel ‚õ± r...      1   \n",
       "1   Thank you Greek Travel Pages for highlighting...      1   \n",
       "2   Œó¬†Eurobank¬†ŒµŒΩŒ∑ŒºŒµœÅœéŒΩŒµŒπ œåœÑŒπ œÑŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ¨¬†œÑŒ∑œÇ,¬†Œ∫...      0   \n",
       "3   Sea side holidays in Afytos, Halikidiki White...      1   \n",
       "4   The future's bright.\\nhttps://t.co/XLpskBYi4u...      0   \n",
       "\n",
       "   friends_hotel_count_1000  followers_hotel_count_1000  \n",
       "0                        83                         116  \n",
       "1                       118                          99  \n",
       "2                         0                          10  \n",
       "3                         6                           7  \n",
       "4                        30                          10  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv')\n",
    "training_set = training_set.replace(np.nan, '')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "twelve-cemetery",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f92cb59ec24489d8026889c1c698db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node: 11\n",
      "Percentage: 4\n",
      "\n",
      "Node: 3\n",
      "Percentage: 0\n",
      "\n",
      "Node: 3\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 2\n",
      "\n",
      "Node: 107\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 2\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 5\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 29\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 5\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 94\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 4\n",
      "Percentage: 0\n",
      "\n",
      "Node: 15\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 1\n",
      "\n",
      "Node: 124\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 5\n",
      "\n",
      "Node: 21\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 32\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 30\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 57\n",
      "Percentage: 6\n",
      "\n",
      "Node: 105\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 26\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 7\n",
      "\n",
      "Node: 17\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 8\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 48\n",
      "Percentage: 0\n",
      "\n",
      "Node: 11\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 10\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 42\n",
      "Percentage: 38\n",
      "\n",
      "Node: 92\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 102\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 7\n",
      "Percentage: 0\n",
      "\n",
      "Node: 25\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 38\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 87\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 4\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 11\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 40\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 7\n",
      "Percentage: 3\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 52\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 25\n",
      "Percentage: 1\n",
      "\n",
      "Node: 84\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 20\n",
      "Percentage: 10\n",
      "\n",
      "Node: 62\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 39\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 19\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 2\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 35\n",
      "Percentage: 0\n",
      "\n",
      "Node: 61\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 1\n",
      "\n",
      "Node: 60\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 2\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 41\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 50\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 2\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 10\n",
      "Percentage: 0\n",
      "\n",
      "Node: 107\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 149\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 4\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 4\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 114\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 69\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 5\n",
      "\n",
      "Node: 20\n",
      "Percentage: 0\n",
      "\n",
      "Node: 9\n",
      "Percentage: 0\n",
      "\n",
      "Node: 13\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 4\n",
      "Percentage: 4\n",
      "\n",
      "Node: 5\n",
      "Percentage: 0\n",
      "\n",
      "Node: 3\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 63\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 137\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 94\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 6\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 24\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 30\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 86\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 22\n",
      "\n",
      "Node: 19\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 7\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 10\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 3\n",
      "Percentage: 0\n",
      "\n",
      "Node: 5\n",
      "Percentage: 0\n",
      "\n",
      "Node: 39\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 8\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 63\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 15\n",
      "Percentage: 7\n",
      "\n",
      "Node: 3\n",
      "Percentage: 2\n",
      "\n",
      "Node: 29\n",
      "Percentage: 1\n",
      "\n",
      "Node: 31\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 45\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 42\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 4\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Percentage: 0\n",
      "\n",
      "Node: 118\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 8\n",
      "Percentage: 0\n",
      "\n",
      "Node: 78\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 2\n",
      "\n",
      "Node: 6\n",
      "Percentage: 5\n",
      "\n",
      "Node: 92\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 10\n",
      "\n",
      "Node: 27\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 7\n",
      "Percentage: 0\n",
      "\n",
      "Node: 52\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 111\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 6\n",
      "\n",
      "Node: 35\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 2\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 6\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 96\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 2\n",
      "\n",
      "Node: 2\n",
      "Percentage: 2\n",
      "\n",
      "Node: 48\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 7\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 4\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 49\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 102\n",
      "Error get_user: [{'code': 63, 'message': 'User has been suspended.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 15\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 3\n",
      "Percentage: 0\n",
      "\n",
      "Node: 26\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 1\n",
      "Percentage: 0\n",
      "\n",
      "Node: 34\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 9\n",
      "\n",
      "Node: 35\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 7\n",
      "\n",
      "Node: 32\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 88\n",
      "Percentage: 0\n",
      "\n",
      "Node: 96\n",
      "Percentage: 6\n",
      "\n",
      "Node: 13\n",
      "Percentage: 0\n",
      "\n",
      "Node: 10\n",
      "Percentage: 2\n",
      "\n",
      "Node: 67\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 36\n",
      "Percentage: 0\n",
      "\n",
      "Node: 43\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 14\n",
      "Percentage: 14\n",
      "\n",
      "Node: 24\n",
      "Percentage: 2\n",
      "\n",
      "Node: 2\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 0\n",
      "\n",
      "Node: 0\n",
      "Percentage: 0\n",
      "\n",
      "Node: 9\n",
      "Error get_user: [{'code': 50, 'message': 'User not found.'}]\n",
      "Percentage: 1\n",
      "\n",
      "Node: 14\n",
      "Percentage: 12\n"
     ]
    }
   ],
   "source": [
    "hotel_mentions = mentions_hotel_count(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "naval-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['mention_hotel_count'] = hotel_mentions\n",
    "training_set.to_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv', index=False)\n",
    "del training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-tribute",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "detected-vancouver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "      <th>friends_hotel_count_1000</th>\n",
       "      <th>followers_hotel_count_1000</th>\n",
       "      <th>mention_hotel_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>AlexanderHaus</td>\n",
       "      <td>Alexander Haus</td>\n",
       "      <td>#Studio #Rooms to Let, in #Halkidiki, #Sithoni...</td>\n",
       "      <td>492</td>\n",
       "      <td>232</td>\n",
       "      <td>277</td>\n",
       "      <td>Though is winter, summer is coming! https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>artsoundgr</td>\n",
       "      <td>ArtSound &amp; Lights</td>\n",
       "      <td>Art Sound &amp; Lights Professional Audio/Video Se...</td>\n",
       "      <td>443</td>\n",
       "      <td>497</td>\n",
       "      <td>191</td>\n",
       "      <td>Œ†œÅŒøœÉœÜŒøœÅŒ¨ STROBE 1500W DMX ARTLIGHT ST1500W Œºœå...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>JOHNMARYRHODES</td>\n",
       "      <td>JOHNMARY FALIRAKI</td>\n",
       "      <td>John Mary is a famly hotel and is located at F...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>http://t.co/DNz6I3s0Sh http://t.co/9eqsMrL4MK</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>THEMETHOTEL</td>\n",
       "      <td>THE MET HOTEL</td>\n",
       "      <td>https://t.co/fi814NlnxK\\r\\nhttp://t.co/AlYUMI5...</td>\n",
       "      <td>2816</td>\n",
       "      <td>181</td>\n",
       "      <td>1136</td>\n",
       "      <td>Let the LOVE sparkle at The MET Hotel!!\\n\\n#T...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>EvitaResort</td>\n",
       "      <td>SunConnect Evita</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>Zumba time @evitaresort @evitaresort #sunconn...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        screen_name               name  \\\n",
       "195   AlexanderHaus     Alexander Haus   \n",
       "196      artsoundgr  ArtSound & Lights   \n",
       "197  JOHNMARYRHODES  JOHNMARY FALIRAKI   \n",
       "198     THEMETHOTEL      THE MET HOTEL   \n",
       "199     EvitaResort   SunConnect Evita   \n",
       "\n",
       "                                           description  statuses_count  \\\n",
       "195  #Studio #Rooms to Let, in #Halkidiki, #Sithoni...             492   \n",
       "196  Art Sound & Lights Professional Audio/Video Se...             443   \n",
       "197  John Mary is a famly hotel and is located at F...               2   \n",
       "198  https://t.co/fi814NlnxK\\r\\nhttp://t.co/AlYUMI5...            2816   \n",
       "199                                                                 25   \n",
       "\n",
       "     friends_count  followers_count  \\\n",
       "195            232              277   \n",
       "196            497              191   \n",
       "197             15                7   \n",
       "198            181             1136   \n",
       "199             24               37   \n",
       "\n",
       "                                   recent_100_statuses  hotel  \\\n",
       "195   Though is winter, summer is coming! https://t...      1   \n",
       "196   Œ†œÅŒøœÉœÜŒøœÅŒ¨ STROBE 1500W DMX ARTLIGHT ST1500W Œºœå...      0   \n",
       "197      http://t.co/DNz6I3s0Sh http://t.co/9eqsMrL4MK      1   \n",
       "198   Let the LOVE sparkle at The MET Hotel!!\\n\\n#T...      1   \n",
       "199   Zumba time @evitaresort @evitaresort #sunconn...      1   \n",
       "\n",
       "     friends_hotel_count_1000  followers_hotel_count_1000  mention_hotel_count  \n",
       "195                        14                          24                    2  \n",
       "196                        13                           5                    0  \n",
       "197                         2                           3                    0  \n",
       "198                         9                          68                    1  \n",
       "199                         1                           3                   12  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Training Set\n",
    "training_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv')\n",
    "training_set = training_set.replace(np.nan, '')\n",
    "training_set.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-ending",
   "metadata": {},
   "source": [
    "# Only Mention Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "comparative-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wrong-petite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      " Best Params: {'svm__C': 1, 'svm__kernel': 'rbf'}.\n",
      " Score: 0.605\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan   nan 0.465 0.465 0.455 0.45  0.47  0.47  0.475 0.47  0.485 0.495\n",
      " 0.48  0.495 0.485 0.495 0.565 0.58  0.58  0.58  0.58  0.58  0.58  0.58\n",
      " 0.575 0.58  0.5   0.495 0.495 0.495 0.5   0.495 0.49  0.495 0.5   0.495\n",
      " 0.495 0.495 0.5   0.495]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'knn__n_neighbors': 8, 'knn__weights': 'distance'}.\n",
      " Score: 0.5800000000000001\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      " Best Params: {'lr__C': 0.1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.605   nan   nan 0.605   nan   nan 0.605   nan   nan 0.605   nan\n",
      "   nan 0.605   nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df['mention_hotel_count'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'knn__n_neighbors': [i for i in range(20)],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-indicator",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "uniform-ticket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 FunctionTransformer(func=<function get_data_ at 0x7fbb3e1923a0>)),\n",
       "                ('svm', SVC(C=1))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel\n",
    "\n",
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df['mention_hotel_count'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC(C=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "extensive-subject",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../classifiers/classifier_hotel_me.sav']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../classifiers/classifier_hotel_me.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-plane",
   "metadata": {},
   "source": [
    "# Statuses and Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-welding",
   "metadata": {},
   "source": [
    "## Without NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "still-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['recent_100_statuses'])\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fitted-uzbekistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.785 0.54  0.555 0.55  0.825 0.535 0.68  0.565 0.815 0.535 0.78  0.565\n",
      " 0.785 0.545 0.815 0.73  0.77  0.545 0.815 0.745 0.795 0.535 0.565 0.55\n",
      " 0.83  0.535 0.73  0.575 0.835 0.535 0.79  0.61  0.79  0.545 0.81  0.725\n",
      " 0.8   0.545 0.835 0.72  0.795 0.535 0.56  0.555 0.82  0.535 0.785 0.57\n",
      " 0.83  0.535 0.79  0.635 0.8   0.545 0.79  0.725 0.795 0.55  0.83  0.725\n",
      " 0.775 0.54  0.55  0.55  0.81  0.535 0.64  0.565 0.805 0.535 0.755 0.565\n",
      " 0.78  0.545 0.785 0.74  0.78  0.545 0.8   0.735 0.79  0.535 0.56  0.55\n",
      " 0.825 0.535 0.74  0.585 0.84  0.535 0.775 0.605 0.805 0.545 0.805 0.73\n",
      " 0.8   0.545 0.825 0.72  0.795 0.535 0.56  0.555 0.82  0.535 0.785 0.57\n",
      " 0.83  0.535 0.79  0.635 0.8   0.545 0.79  0.725 0.795 0.55  0.83  0.725\n",
      " 0.705 0.54  0.54  0.54  0.81  0.535 0.595 0.56  0.83  0.535 0.71  0.56\n",
      " 0.805 0.545 0.785 0.73  0.795 0.545 0.805 0.745 0.79  0.535 0.56  0.55\n",
      " 0.825 0.535 0.74  0.59  0.84  0.535 0.775 0.605 0.805 0.545 0.805 0.73\n",
      " 0.8   0.545 0.825 0.72  0.795 0.535 0.56  0.555 0.82  0.535 0.785 0.57\n",
      " 0.83  0.535 0.79  0.635 0.8   0.545 0.79  0.725 0.795 0.55  0.83  0.725\n",
      " 0.77  0.54  0.56  0.555 0.795 0.535 0.715 0.58  0.815 0.535 0.745 0.605\n",
      " 0.79  0.545 0.77  0.705 0.775 0.545 0.795 0.73  0.755 0.54  0.56  0.555\n",
      " 0.79  0.535 0.75  0.62  0.835 0.535 0.745 0.635 0.815 0.54  0.77  0.73\n",
      " 0.81  0.545 0.815 0.72  0.77  0.54  0.565 0.565 0.795 0.535 0.73  0.575\n",
      " 0.83  0.535 0.735 0.65  0.815 0.545 0.78  0.7   0.81  0.545 0.8   0.725\n",
      " 0.79  0.54  0.555 0.55  0.775 0.535 0.68  0.575 0.81  0.535 0.765 0.615\n",
      " 0.78  0.545 0.76  0.705 0.78  0.545 0.795 0.72  0.755 0.54  0.565 0.555\n",
      " 0.79  0.535 0.75  0.61  0.83  0.535 0.75  0.625 0.815 0.54  0.77  0.705\n",
      " 0.805 0.545 0.815 0.72  0.77  0.54  0.565 0.565 0.795 0.535 0.73  0.575\n",
      " 0.83  0.535 0.735 0.65  0.815 0.545 0.78  0.7   0.81  0.545 0.8   0.725\n",
      " 0.78  0.54  0.545 0.545 0.78  0.535 0.645 0.57  0.805 0.535 0.77  0.61\n",
      " 0.785 0.545 0.77  0.72  0.79  0.545 0.785 0.715 0.755 0.54  0.565 0.555\n",
      " 0.79  0.535 0.75  0.61  0.83  0.535 0.75  0.625 0.81  0.54  0.77  0.705\n",
      " 0.805 0.545 0.815 0.72  0.77  0.54  0.565 0.565 0.795 0.535 0.73  0.575\n",
      " 0.83  0.535 0.735 0.65  0.815 0.545 0.78  0.7   0.81  0.545 0.8   0.725\n",
      " 0.54  0.54  0.54  0.54  0.54  0.535 0.57  0.56  0.625 0.535 0.57  0.56\n",
      " 0.685 0.535 0.585 0.56  0.625 0.54  0.62  0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.54  0.54  0.54  0.54  0.54  0.535 0.57  0.56  0.66  0.535 0.57  0.56\n",
      " 0.695 0.535 0.59  0.56  0.615 0.54  0.625 0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.54  0.54  0.54  0.54  0.54  0.535 0.57  0.56  0.695 0.535 0.57  0.56\n",
      " 0.67  0.54  0.58  0.56  0.55  0.54  0.64  0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 2000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.8399999999999999\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.645 0.645 0.645 0.645 0.695 0.695 0.675 0.67  0.68  0.68  0.665 0.675\n",
      " 0.675 0.675 0.665 0.675 0.67  0.665 0.67  0.67  0.685 0.685 0.655 0.685\n",
      " 0.705 0.705 0.675 0.695 0.695 0.695 0.695 0.7   0.685 0.685 0.685 0.69\n",
      " 0.675 0.67  0.675 0.68  0.685 0.685 0.66  0.685 0.72  0.72  0.695 0.71\n",
      " 0.71  0.72  0.715 0.72  0.705 0.71  0.71  0.715 0.71  0.705 0.705 0.71\n",
      " 0.64  0.64  0.62  0.64  0.65  0.65  0.64  0.645 0.61  0.61  0.61  0.615\n",
      " 0.585 0.585 0.575 0.58  0.58  0.575 0.575 0.57  0.665 0.665 0.64  0.665\n",
      " 0.69  0.69  0.65  0.67  0.675 0.675 0.68  0.685 0.685 0.685 0.69  0.695\n",
      " 0.675 0.67  0.67  0.675 0.685 0.685 0.66  0.685 0.72  0.72  0.695 0.71\n",
      " 0.71  0.72  0.715 0.72  0.705 0.71  0.71  0.715 0.71  0.705 0.705 0.71\n",
      " 0.535 0.535 0.53  0.535 0.535 0.535 0.555 0.55  0.505 0.505 0.495 0.495\n",
      " 0.5   0.5   0.5   0.5   0.505 0.5   0.5   0.495 0.665 0.665 0.64  0.665\n",
      " 0.69  0.69  0.66  0.68  0.68  0.68  0.685 0.69  0.685 0.685 0.69  0.695\n",
      " 0.675 0.67  0.67  0.675 0.685 0.685 0.66  0.685 0.72  0.72  0.695 0.71\n",
      " 0.71  0.72  0.715 0.72  0.705 0.71  0.71  0.715 0.71  0.705 0.705 0.71\n",
      " 0.685 0.685 0.65  0.685 0.68  0.68  0.68  0.67  0.68  0.68  0.68  0.69\n",
      " 0.675 0.675 0.67  0.675 0.67  0.665 0.665 0.66  0.715 0.715 0.68  0.715\n",
      " 0.73  0.73  0.71  0.72  0.705 0.705 0.695 0.705 0.69  0.69  0.69  0.69\n",
      " 0.695 0.69  0.69  0.69  0.725 0.725 0.7   0.725 0.745 0.745 0.685 0.7\n",
      " 0.71  0.71  0.71  0.72  0.705 0.705 0.705 0.705 0.705 0.7   0.7   0.7\n",
      " 0.66  0.66  0.635 0.66  0.63  0.63  0.61  0.63  0.61  0.61  0.625 0.63\n",
      " 0.58  0.58  0.6   0.6   0.57  0.565 0.57  0.565 0.705 0.705 0.665 0.705\n",
      " 0.71  0.71  0.705 0.71  0.69  0.69  0.68  0.685 0.67  0.67  0.67  0.67\n",
      " 0.665 0.66  0.665 0.665 0.725 0.725 0.7   0.725 0.745 0.745 0.685 0.7\n",
      " 0.71  0.71  0.71  0.72  0.705 0.705 0.705 0.705 0.705 0.7   0.7   0.7\n",
      " 0.54  0.54  0.53  0.54  0.51  0.51  0.515 0.51  0.505 0.505 0.505 0.505\n",
      " 0.495 0.495 0.495 0.495 0.505 0.5   0.505 0.5   0.705 0.705 0.66  0.705\n",
      " 0.71  0.71  0.69  0.7   0.685 0.685 0.675 0.68  0.67  0.67  0.67  0.67\n",
      " 0.665 0.66  0.665 0.665 0.725 0.725 0.7   0.725 0.745 0.745 0.685 0.7\n",
      " 0.71  0.71  0.71  0.72  0.705 0.705 0.705 0.705 0.705 0.7   0.7   0.7\n",
      " 0.595 0.595 0.6   0.625 0.535 0.55  0.555 0.555 0.53  0.535 0.545 0.535\n",
      " 0.485 0.5   0.515 0.52  0.51  0.515 0.515 0.52    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.535 0.535 0.57  0.57  0.495 0.495 0.535 0.525 0.505 0.505 0.495 0.495\n",
      " 0.485 0.485 0.485 0.485 0.495 0.49  0.515 0.5     nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.5   0.5   0.465 0.5   0.475 0.475 0.48  0.48  0.49  0.49  0.485 0.485\n",
      " 0.495 0.495 0.495 0.49  0.495 0.49  0.49  0.48    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 10, 'knn__n_neighbors': 3, 'knn__weights': 'uniform'}.\n",
      " Score: 0.7449999999999999\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.775   nan 0.84    nan 0.805   nan 0.84    nan 0.825   nan 0.84\n",
      "   nan 0.835   nan 0.84    nan 0.825   nan 0.84    nan 0.785   nan 0.825\n",
      "   nan 0.795   nan 0.825   nan 0.815   nan 0.825   nan 0.83    nan 0.825\n",
      "   nan 0.83    nan 0.825   nan 0.79    nan 0.85    nan 0.79    nan 0.85\n",
      "   nan 0.81    nan 0.85    nan 0.835   nan 0.85    nan 0.83    nan 0.85\n",
      "   nan 0.76    nan 0.825   nan 0.795   nan 0.825   nan 0.81    nan 0.825\n",
      "   nan 0.825   nan 0.825   nan 0.82    nan 0.825   nan 0.79    nan 0.825\n",
      "   nan 0.79    nan 0.825   nan 0.82    nan 0.825   nan 0.825   nan 0.825\n",
      "   nan 0.825   nan 0.825   nan 0.79    nan 0.85    nan 0.79    nan 0.85\n",
      "   nan 0.81    nan 0.85    nan 0.835   nan 0.85    nan 0.83    nan 0.85\n",
      "   nan 0.74    nan 0.82    nan 0.8     nan 0.82    nan 0.805   nan 0.82\n",
      "   nan 0.825   nan 0.82    nan 0.83    nan 0.82    nan 0.79    nan 0.825\n",
      "   nan 0.79    nan 0.825   nan 0.82    nan 0.825   nan 0.825   nan 0.825\n",
      "   nan 0.825   nan 0.825   nan 0.79    nan 0.85    nan 0.79    nan 0.85\n",
      "   nan 0.81    nan 0.85    nan 0.835   nan 0.85    nan 0.83    nan 0.85\n",
      "   nan 0.79    nan 0.81    nan 0.775   nan 0.81    nan 0.78    nan 0.81\n",
      "   nan 0.83    nan 0.81    nan 0.825   nan 0.81    nan 0.79    nan 0.815\n",
      "   nan 0.775   nan 0.815   nan 0.785   nan 0.815   nan 0.835   nan 0.815\n",
      "   nan 0.84    nan 0.815   nan 0.79    nan 0.81    nan 0.78    nan 0.81\n",
      "   nan 0.775   nan 0.81    nan 0.835   nan 0.81    nan 0.845   nan 0.81\n",
      "   nan 0.78    nan 0.82    nan 0.77    nan 0.82    nan 0.78    nan 0.82\n",
      "   nan 0.81    nan 0.82    nan 0.825   nan 0.82    nan 0.795   nan 0.83\n",
      "   nan 0.775   nan 0.83    nan 0.78    nan 0.83    nan 0.83    nan 0.83\n",
      "   nan 0.83    nan 0.83    nan 0.79    nan 0.81    nan 0.78    nan 0.81\n",
      "   nan 0.775   nan 0.81    nan 0.835   nan 0.81    nan 0.845   nan 0.81\n",
      "   nan 0.77    nan 0.815   nan 0.785   nan 0.815   nan 0.77    nan 0.815\n",
      "   nan 0.82    nan 0.815   nan 0.815   nan 0.815   nan 0.795   nan 0.825\n",
      "   nan 0.77    nan 0.825   nan 0.775   nan 0.825   nan 0.83    nan 0.825\n",
      "   nan 0.83    nan 0.825   nan 0.79    nan 0.81    nan 0.78    nan 0.81\n",
      "   nan 0.775   nan 0.81    nan 0.835   nan 0.81    nan 0.845   nan 0.81\n",
      "   nan 0.615   nan 0.69    nan 0.63    nan 0.69    nan 0.645   nan 0.69\n",
      "   nan 0.635   nan 0.69    nan 0.655   nan 0.69    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.61    nan 0.71    nan 0.625   nan 0.71    nan 0.645   nan 0.71\n",
      "   nan 0.67    nan 0.71    nan 0.725   nan 0.71    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.61    nan 0.725   nan 0.62    nan 0.725   nan 0.63    nan 0.725\n",
      "   nan 0.685   nan 0.725   nan 0.735   nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 10, 'lr__C': 0.1, 'lr__penalty': 'none'}.\n",
      " Score: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['mention_hotel_count'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-picking",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "portuguese-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mounted-gilbert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.595 0.54  0.54  0.54  0.845 0.535 0.585 0.56  0.84  0.535 0.65  0.57\n",
      " 0.835 0.545 0.81  0.705 0.79  0.545 0.82  0.755 0.735 0.535 0.54  0.54\n",
      " 0.855 0.535 0.61  0.565 0.85  0.535 0.74  0.56  0.825 0.545 0.825 0.715\n",
      " 0.775 0.55  0.825 0.745 0.795 0.535 0.54  0.545 0.85  0.535 0.655 0.565\n",
      " 0.85  0.535 0.775 0.575 0.805 0.55  0.83  0.725 0.795 0.55  0.82  0.745\n",
      " 0.58  0.54  0.54  0.54  0.84  0.535 0.585 0.56  0.845 0.535 0.63  0.565\n",
      " 0.84  0.545 0.815 0.69  0.79  0.545 0.825 0.73  0.705 0.535 0.54  0.54\n",
      " 0.85  0.535 0.605 0.565 0.85  0.535 0.725 0.56  0.815 0.545 0.82  0.71\n",
      " 0.775 0.545 0.825 0.74  0.795 0.535 0.54  0.545 0.85  0.535 0.655 0.565\n",
      " 0.85  0.535 0.775 0.575 0.805 0.55  0.83  0.725 0.795 0.55  0.82  0.745\n",
      " 0.565 0.54  0.54  0.54  0.855 0.535 0.575 0.56  0.84  0.535 0.625 0.56\n",
      " 0.835 0.545 0.8   0.685 0.71  0.545 0.815 0.73  0.705 0.535 0.54  0.54\n",
      " 0.85  0.535 0.605 0.565 0.85  0.535 0.725 0.56  0.815 0.545 0.82  0.71\n",
      " 0.775 0.545 0.825 0.74  0.795 0.535 0.54  0.545 0.85  0.535 0.655 0.565\n",
      " 0.85  0.535 0.775 0.575 0.805 0.55  0.83  0.725 0.795 0.55  0.82  0.745\n",
      " 0.6   0.54  0.54  0.54  0.845 0.535 0.585 0.56  0.84  0.535 0.65  0.57\n",
      " 0.83  0.545 0.81  0.705 0.78  0.545 0.82  0.755 0.73  0.535 0.54  0.54\n",
      " 0.86  0.535 0.61  0.56  0.845 0.535 0.73  0.56  0.815 0.545 0.82  0.715\n",
      " 0.77  0.55  0.825 0.745 0.795 0.535 0.54  0.54  0.85  0.535 0.65  0.565\n",
      " 0.84  0.535 0.765 0.57  0.815 0.55  0.825 0.72  0.805 0.545 0.81  0.745\n",
      " 0.585 0.54  0.54  0.54  0.84  0.535 0.585 0.56  0.845 0.535 0.63  0.565\n",
      " 0.83  0.545 0.81  0.69  0.795 0.545 0.825 0.735 0.7   0.535 0.54  0.54\n",
      " 0.85  0.535 0.605 0.56  0.85  0.535 0.725 0.56  0.815 0.545 0.82  0.71\n",
      " 0.78  0.545 0.825 0.745 0.795 0.535 0.54  0.54  0.85  0.535 0.65  0.565\n",
      " 0.84  0.535 0.765 0.57  0.815 0.55  0.825 0.72  0.805 0.545 0.81  0.745\n",
      " 0.565 0.54  0.54  0.54  0.855 0.535 0.575 0.56  0.84  0.535 0.625 0.56\n",
      " 0.815 0.545 0.795 0.685 0.725 0.545 0.815 0.73  0.7   0.535 0.54  0.54\n",
      " 0.85  0.535 0.605 0.56  0.85  0.535 0.725 0.56  0.815 0.545 0.82  0.71\n",
      " 0.78  0.545 0.825 0.745 0.795 0.535 0.54  0.54  0.85  0.535 0.65  0.565\n",
      " 0.84  0.535 0.765 0.57  0.815 0.55  0.825 0.72  0.805 0.545 0.81  0.745\n",
      " 0.54  0.54  0.54  0.54  0.545 0.535 0.57  0.56  0.64  0.535 0.57  0.56\n",
      " 0.68  0.535 0.595 0.56  0.655 0.54  0.615 0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.54  0.54  0.54  0.54  0.54  0.535 0.57  0.56  0.665 0.535 0.57  0.56\n",
      " 0.675 0.535 0.59  0.56  0.62  0.54  0.635 0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.54  0.54  0.54  0.54  0.54  0.535 0.57  0.56  0.69  0.535 0.57  0.56\n",
      " 0.67  0.54  0.58  0.56  0.545 0.54  0.645 0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 0.5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.86\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.61  0.61  0.595 0.61  0.515 0.52  0.515 0.52  0.51  0.51  0.52  0.525\n",
      " 0.505 0.505 0.5   0.505 0.5   0.495 0.5   0.5   0.585 0.585 0.545 0.585\n",
      " 0.505 0.51  0.465 0.485 0.53  0.53  0.53  0.535 0.5   0.5   0.5   0.505\n",
      " 0.5   0.495 0.495 0.5   0.605 0.605 0.57  0.605 0.605 0.615 0.585 0.595\n",
      " 0.565 0.58  0.58  0.58  0.525 0.535 0.535 0.54  0.525 0.53  0.52  0.525\n",
      " 0.49  0.49  0.475 0.49  0.49  0.49  0.48  0.475 0.48  0.48  0.48  0.485\n",
      " 0.495 0.495 0.485 0.49  0.5   0.495 0.5   0.495 0.525 0.525 0.485 0.525\n",
      " 0.505 0.505 0.485 0.51  0.505 0.505 0.51  0.515 0.495 0.495 0.495 0.505\n",
      " 0.5   0.495 0.495 0.5   0.605 0.605 0.57  0.605 0.605 0.615 0.585 0.595\n",
      " 0.565 0.58  0.58  0.58  0.525 0.535 0.535 0.54  0.525 0.53  0.52  0.525\n",
      " 0.5   0.5   0.51  0.5   0.515 0.515 0.49  0.505 0.505 0.505 0.5   0.5\n",
      " 0.495 0.495 0.495 0.495 0.5   0.495 0.5   0.49  0.525 0.525 0.485 0.525\n",
      " 0.505 0.505 0.485 0.51  0.505 0.505 0.51  0.515 0.495 0.495 0.495 0.505\n",
      " 0.5   0.495 0.495 0.5   0.605 0.605 0.57  0.605 0.605 0.615 0.585 0.595\n",
      " 0.565 0.58  0.58  0.58  0.525 0.535 0.535 0.54  0.525 0.53  0.52  0.525\n",
      " 0.595 0.595 0.59  0.595 0.515 0.52  0.51  0.51  0.505 0.505 0.505 0.515\n",
      " 0.5   0.5   0.515 0.52  0.5   0.495 0.495 0.495 0.585 0.585 0.54  0.585\n",
      " 0.505 0.51  0.495 0.515 0.52  0.52  0.51  0.525 0.49  0.49  0.49  0.495\n",
      " 0.495 0.49  0.49  0.495 0.595 0.595 0.555 0.595 0.6   0.61  0.57  0.59\n",
      " 0.575 0.59  0.585 0.585 0.545 0.55  0.555 0.555 0.53  0.535 0.53  0.535\n",
      " 0.49  0.49  0.475 0.49  0.49  0.49  0.475 0.48  0.495 0.495 0.48  0.485\n",
      " 0.495 0.495 0.495 0.5   0.5   0.495 0.5   0.495 0.525 0.525 0.485 0.525\n",
      " 0.505 0.505 0.455 0.48  0.495 0.495 0.495 0.5   0.5   0.5   0.495 0.505\n",
      " 0.5   0.495 0.495 0.5   0.595 0.595 0.555 0.595 0.6   0.61  0.57  0.59\n",
      " 0.575 0.59  0.585 0.585 0.545 0.55  0.555 0.555 0.53  0.535 0.53  0.535\n",
      " 0.505 0.505 0.505 0.505 0.52  0.52  0.495 0.505 0.505 0.505 0.5   0.5\n",
      " 0.495 0.495 0.495 0.495 0.5   0.495 0.5   0.49  0.525 0.525 0.485 0.525\n",
      " 0.505 0.505 0.455 0.48  0.495 0.495 0.495 0.5   0.5   0.5   0.495 0.505\n",
      " 0.5   0.495 0.495 0.5   0.595 0.595 0.555 0.595 0.6   0.61  0.57  0.59\n",
      " 0.575 0.59  0.585 0.585 0.545 0.55  0.555 0.555 0.53  0.535 0.53  0.535\n",
      " 0.585 0.585 0.59  0.61  0.56  0.56  0.52  0.525 0.535 0.54  0.51  0.515\n",
      " 0.495 0.505 0.515 0.515 0.495 0.495 0.5   0.505   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.51  0.51  0.525 0.545 0.465 0.475 0.535 0.56  0.5   0.51  0.515 0.53\n",
      " 0.495 0.495 0.515 0.51  0.49  0.485 0.485 0.48    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.495 0.495 0.465 0.495 0.48  0.48  0.475 0.48  0.48  0.48  0.47  0.48\n",
      " 0.5   0.5   0.5   0.495 0.495 0.49  0.5   0.49    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 10, 'knn__n_neighbors': 3, 'knn__weights': 'distance'}.\n",
      " Score: 0.615\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.73    nan 0.84    nan 0.82    nan 0.84    nan 0.82    nan 0.84\n",
      "   nan 0.845   nan 0.84    nan 0.84    nan 0.84    nan 0.765   nan 0.86\n",
      "   nan 0.82    nan 0.86    nan 0.83    nan 0.86    nan 0.845   nan 0.86\n",
      "   nan 0.845   nan 0.86    nan 0.785   nan 0.845   nan 0.82    nan 0.845\n",
      "   nan 0.835   nan 0.845   nan 0.84    nan 0.845   nan 0.85    nan 0.845\n",
      "   nan 0.725   nan 0.83    nan 0.815   nan 0.83    nan 0.83    nan 0.83\n",
      "   nan 0.84    nan 0.83    nan 0.845   nan 0.83    nan 0.745   nan 0.87\n",
      "   nan 0.82    nan 0.87    nan 0.83    nan 0.87    nan 0.845   nan 0.87\n",
      "   nan 0.87    nan 0.87    nan 0.785   nan 0.845   nan 0.82    nan 0.845\n",
      "   nan 0.835   nan 0.845   nan 0.84    nan 0.845   nan 0.85    nan 0.845\n",
      "   nan 0.68    nan 0.835   nan 0.815   nan 0.835   nan 0.83    nan 0.835\n",
      "   nan 0.825   nan 0.835   nan 0.835   nan 0.835   nan 0.745   nan 0.87\n",
      "   nan 0.82    nan 0.87    nan 0.83    nan 0.87    nan 0.845   nan 0.87\n",
      "   nan 0.87    nan 0.87    nan 0.785   nan 0.845   nan 0.82    nan 0.845\n",
      "   nan 0.835   nan 0.845   nan 0.84    nan 0.845   nan 0.85    nan 0.845\n",
      "   nan 0.73    nan 0.84    nan 0.82    nan 0.84    nan 0.82    nan 0.84\n",
      "   nan 0.84    nan 0.84    nan 0.845   nan 0.84    nan 0.765   nan 0.86\n",
      "   nan 0.82    nan 0.86    nan 0.83    nan 0.86    nan 0.845   nan 0.86\n",
      "   nan 0.845   nan 0.86    nan 0.785   nan 0.865   nan 0.82    nan 0.865\n",
      "   nan 0.83    nan 0.865   nan 0.85    nan 0.865   nan 0.87    nan 0.865\n",
      "   nan 0.725   nan 0.82    nan 0.815   nan 0.82    nan 0.83    nan 0.82\n",
      "   nan 0.835   nan 0.82    nan 0.845   nan 0.82    nan 0.74    nan 0.875\n",
      "   nan 0.82    nan 0.875   nan 0.83    nan 0.875   nan 0.845   nan 0.875\n",
      "   nan 0.88    nan 0.875   nan 0.785   nan 0.865   nan 0.82    nan 0.865\n",
      "   nan 0.83    nan 0.865   nan 0.85    nan 0.865   nan 0.87    nan 0.865\n",
      "   nan 0.68    nan 0.84    nan 0.815   nan 0.84    nan 0.83    nan 0.84\n",
      "   nan 0.825   nan 0.84    nan 0.835   nan 0.84    nan 0.74    nan 0.875\n",
      "   nan 0.82    nan 0.875   nan 0.83    nan 0.875   nan 0.845   nan 0.875\n",
      "   nan 0.88    nan 0.875   nan 0.785   nan 0.865   nan 0.82    nan 0.865\n",
      "   nan 0.83    nan 0.865   nan 0.85    nan 0.865   nan 0.87    nan 0.865\n",
      "   nan 0.615   nan 0.68    nan 0.63    nan 0.68    nan 0.635   nan 0.68\n",
      "   nan 0.625   nan 0.68    nan 0.66    nan 0.68    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.61    nan 0.725   nan 0.625   nan 0.725   nan 0.65    nan 0.725\n",
      "   nan 0.685   nan 0.725   nan 0.73    nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.61    nan 0.725   nan 0.62    nan 0.725   nan 0.625   nan 0.725\n",
      "   nan 0.685   nan 0.725   nan 0.735   nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 2000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.8800000000000001\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['mention_hotel_count'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-fluid",
   "metadata": {},
   "source": [
    "## Lemmatization and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "found-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "mental-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.63  0.54  0.54  0.54  0.85  0.535 0.595 0.555 0.855 0.535 0.68  0.565\n",
      " 0.825 0.545 0.83  0.71  0.77  0.545 0.825 0.75  0.755 0.535 0.54  0.54\n",
      " 0.865 0.535 0.62  0.565 0.865 0.535 0.745 0.56  0.82  0.545 0.83  0.72\n",
      " 0.76  0.55  0.84  0.745 0.8   0.535 0.55  0.55  0.865 0.535 0.655 0.565\n",
      " 0.855 0.535 0.79  0.595 0.805 0.545 0.825 0.73  0.8   0.55  0.825 0.745\n",
      " 0.59  0.54  0.54  0.54  0.845 0.535 0.585 0.56  0.85  0.535 0.645 0.565\n",
      " 0.83  0.545 0.825 0.7   0.78  0.545 0.83  0.74  0.735 0.535 0.54  0.54\n",
      " 0.855 0.535 0.61  0.565 0.865 0.535 0.74  0.56  0.825 0.545 0.825 0.715\n",
      " 0.77  0.545 0.835 0.74  0.8   0.535 0.55  0.55  0.865 0.535 0.655 0.565\n",
      " 0.855 0.535 0.79  0.595 0.805 0.545 0.825 0.73  0.8   0.55  0.825 0.745\n",
      " 0.575 0.54  0.54  0.54  0.855 0.535 0.58  0.56  0.845 0.535 0.625 0.565\n",
      " 0.825 0.545 0.82  0.68  0.7   0.545 0.82  0.75  0.735 0.535 0.54  0.54\n",
      " 0.855 0.535 0.61  0.565 0.865 0.535 0.74  0.56  0.825 0.545 0.825 0.715\n",
      " 0.77  0.545 0.835 0.74  0.8   0.535 0.55  0.55  0.865 0.535 0.655 0.565\n",
      " 0.855 0.535 0.79  0.595 0.805 0.545 0.825 0.73  0.8   0.55  0.825 0.745\n",
      " 0.63  0.54  0.54  0.54  0.855 0.535 0.595 0.555 0.855 0.535 0.68  0.565\n",
      " 0.83  0.545 0.83  0.715 0.77  0.545 0.825 0.75  0.755 0.535 0.54  0.54\n",
      " 0.865 0.535 0.62  0.565 0.86  0.535 0.745 0.56  0.825 0.545 0.83  0.72\n",
      " 0.77  0.55  0.83  0.745 0.8   0.535 0.55  0.55  0.865 0.535 0.66  0.565\n",
      " 0.85  0.535 0.79  0.595 0.82  0.545 0.825 0.725 0.815 0.55  0.825 0.745\n",
      " 0.59  0.54  0.54  0.54  0.845 0.535 0.585 0.56  0.85  0.535 0.645 0.565\n",
      " 0.83  0.545 0.82  0.695 0.785 0.545 0.83  0.74  0.735 0.535 0.54  0.54\n",
      " 0.855 0.535 0.61  0.565 0.865 0.535 0.74  0.56  0.825 0.545 0.825 0.715\n",
      " 0.77  0.55  0.83  0.74  0.8   0.535 0.55  0.55  0.865 0.535 0.66  0.565\n",
      " 0.85  0.535 0.79  0.595 0.82  0.545 0.825 0.725 0.815 0.55  0.825 0.745\n",
      " 0.575 0.54  0.54  0.54  0.855 0.535 0.58  0.56  0.845 0.535 0.625 0.565\n",
      " 0.825 0.545 0.815 0.68  0.705 0.545 0.815 0.75  0.735 0.535 0.54  0.54\n",
      " 0.855 0.535 0.61  0.565 0.865 0.535 0.74  0.56  0.825 0.545 0.825 0.715\n",
      " 0.77  0.55  0.83  0.74  0.8   0.535 0.55  0.55  0.865 0.535 0.66  0.565\n",
      " 0.85  0.535 0.79  0.595 0.82  0.545 0.825 0.725 0.815 0.55  0.825 0.745\n",
      " 0.54  0.54  0.54  0.54  0.54  0.535 0.57  0.56  0.635 0.535 0.57  0.56\n",
      " 0.7   0.535 0.59  0.56  0.655 0.54  0.61  0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.54  0.54  0.54  0.54  0.54  0.535 0.57  0.56  0.67  0.535 0.57  0.56\n",
      " 0.7   0.54  0.585 0.56  0.575 0.54  0.615 0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.54  0.54  0.54  0.54  0.545 0.535 0.57  0.56  0.675 0.535 0.57  0.56\n",
      " 0.685 0.54  0.58  0.56  0.54  0.54  0.61  0.56    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 0.5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.865\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.62  0.62  0.585 0.62  0.495 0.5   0.53  0.53  0.5   0.5   0.51  0.515\n",
      " 0.485 0.485 0.49  0.495 0.5   0.495 0.49  0.49  0.575 0.575 0.555 0.575\n",
      " 0.515 0.52  0.51  0.53  0.505 0.505 0.52  0.53  0.5   0.5   0.505 0.51\n",
      " 0.5   0.495 0.5   0.5   0.57  0.57  0.555 0.57  0.575 0.585 0.57  0.585\n",
      " 0.54  0.555 0.535 0.535 0.505 0.505 0.505 0.51  0.51  0.505 0.51  0.515\n",
      " 0.49  0.49  0.48  0.49  0.485 0.485 0.475 0.47  0.5   0.5   0.485 0.49\n",
      " 0.5   0.5   0.495 0.495 0.5   0.495 0.5   0.495 0.52  0.52  0.495 0.52\n",
      " 0.49  0.49  0.495 0.5   0.485 0.485 0.49  0.495 0.49  0.49  0.49  0.5\n",
      " 0.5   0.495 0.5   0.5   0.57  0.57  0.555 0.57  0.575 0.585 0.57  0.585\n",
      " 0.54  0.555 0.535 0.535 0.505 0.505 0.505 0.51  0.51  0.505 0.51  0.515\n",
      " 0.5   0.5   0.505 0.5   0.52  0.52  0.5   0.505 0.505 0.505 0.49  0.495\n",
      " 0.5   0.5   0.495 0.495 0.5   0.495 0.5   0.49  0.52  0.52  0.495 0.52\n",
      " 0.49  0.49  0.495 0.5   0.485 0.485 0.49  0.495 0.49  0.49  0.49  0.5\n",
      " 0.5   0.495 0.5   0.5   0.57  0.57  0.555 0.57  0.575 0.585 0.57  0.585\n",
      " 0.54  0.555 0.535 0.535 0.505 0.505 0.505 0.51  0.51  0.505 0.51  0.515\n",
      " 0.62  0.62  0.58  0.62  0.495 0.5   0.53  0.52  0.525 0.525 0.515 0.525\n",
      " 0.49  0.49  0.495 0.5   0.5   0.495 0.495 0.495 0.575 0.575 0.545 0.575\n",
      " 0.515 0.52  0.485 0.5   0.52  0.52  0.51  0.515 0.495 0.495 0.505 0.51\n",
      " 0.495 0.49  0.495 0.495 0.585 0.585 0.555 0.585 0.57  0.58  0.555 0.575\n",
      " 0.515 0.53  0.52  0.53  0.51  0.51  0.52  0.525 0.51  0.51  0.51  0.515\n",
      " 0.495 0.495 0.485 0.495 0.485 0.485 0.49  0.485 0.495 0.495 0.485 0.49\n",
      " 0.5   0.5   0.495 0.5   0.5   0.495 0.5   0.495 0.52  0.52  0.495 0.52\n",
      " 0.49  0.49  0.49  0.5   0.495 0.495 0.485 0.505 0.5   0.5   0.49  0.5\n",
      " 0.5   0.495 0.495 0.495 0.585 0.585 0.555 0.585 0.57  0.58  0.555 0.575\n",
      " 0.515 0.53  0.52  0.53  0.51  0.51  0.52  0.525 0.51  0.51  0.51  0.515\n",
      " 0.5   0.5   0.505 0.5   0.515 0.515 0.52  0.525 0.5   0.5   0.49  0.495\n",
      " 0.5   0.5   0.5   0.5   0.5   0.495 0.5   0.49  0.52  0.52  0.495 0.52\n",
      " 0.49  0.49  0.49  0.5   0.495 0.495 0.485 0.505 0.5   0.5   0.49  0.5\n",
      " 0.5   0.495 0.495 0.495 0.585 0.585 0.555 0.585 0.57  0.58  0.555 0.575\n",
      " 0.515 0.53  0.52  0.53  0.51  0.51  0.52  0.525 0.51  0.51  0.51  0.515\n",
      " 0.585 0.585 0.605 0.615 0.515 0.515 0.6   0.59  0.54  0.54  0.525 0.525\n",
      " 0.505 0.51  0.51  0.51  0.52  0.52  0.49  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.535 0.535 0.555 0.565 0.485 0.485 0.49  0.495 0.46  0.46  0.485 0.485\n",
      " 0.495 0.49  0.495 0.49  0.5   0.49  0.49  0.49    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.49  0.49  0.44  0.49  0.46  0.46  0.45  0.48  0.465 0.465 0.465 0.475\n",
      " 0.47  0.47  0.465 0.495 0.485 0.485 0.47  0.485   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 1, 'knn__weights': 'uniform'}.\n",
      " Score: 0.62\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.75    nan 0.845   nan 0.83    nan 0.845   nan 0.84    nan 0.845\n",
      "   nan 0.845   nan 0.845   nan 0.84    nan 0.845   nan 0.775   nan 0.885\n",
      "   nan 0.83    nan 0.885   nan 0.845   nan 0.885   nan 0.845   nan 0.885\n",
      "   nan 0.855   nan 0.885   nan 0.79    nan 0.875   nan 0.83    nan 0.875\n",
      "   nan 0.845   nan 0.875   nan 0.84    nan 0.875   nan 0.86    nan 0.875\n",
      "   nan 0.735   nan 0.84    nan 0.825   nan 0.84    nan 0.835   nan 0.84\n",
      "   nan 0.84    nan 0.84    nan 0.84    nan 0.84    nan 0.77    nan 0.875\n",
      "   nan 0.825   nan 0.875   nan 0.84    nan 0.875   nan 0.845   nan 0.875\n",
      "   nan 0.855   nan 0.875   nan 0.79    nan 0.875   nan 0.83    nan 0.875\n",
      "   nan 0.845   nan 0.875   nan 0.84    nan 0.875   nan 0.86    nan 0.875\n",
      "   nan 0.71    nan 0.825   nan 0.83    nan 0.825   nan 0.83    nan 0.825\n",
      "   nan 0.835   nan 0.825   nan 0.845   nan 0.825   nan 0.77    nan 0.875\n",
      "   nan 0.825   nan 0.875   nan 0.84    nan 0.875   nan 0.845   nan 0.875\n",
      "   nan 0.855   nan 0.875   nan 0.79    nan 0.875   nan 0.83    nan 0.875\n",
      "   nan 0.845   nan 0.875   nan 0.84    nan 0.875   nan 0.86    nan 0.875\n",
      "   nan 0.75    nan 0.84    nan 0.83    nan 0.84    nan 0.84    nan 0.84\n",
      "   nan 0.845   nan 0.84    nan 0.84    nan 0.84    nan 0.775   nan 0.885\n",
      "   nan 0.83    nan 0.885   nan 0.845   nan 0.885   nan 0.84    nan 0.885\n",
      "   nan 0.845   nan 0.885   nan 0.79    nan 0.88    nan 0.84    nan 0.88\n",
      "   nan 0.845   nan 0.88    nan 0.84    nan 0.88    nan 0.87    nan 0.88\n",
      "   nan 0.735   nan 0.85    nan 0.825   nan 0.85    nan 0.835   nan 0.85\n",
      "   nan 0.84    nan 0.85    nan 0.835   nan 0.85    nan 0.77    nan 0.875\n",
      "   nan 0.825   nan 0.875   nan 0.84    nan 0.875   nan 0.845   nan 0.875\n",
      "   nan 0.86    nan 0.875   nan 0.79    nan 0.88    nan 0.84    nan 0.88\n",
      "   nan 0.845   nan 0.88    nan 0.84    nan 0.88    nan 0.87    nan 0.88\n",
      "   nan 0.71    nan 0.815   nan 0.83    nan 0.815   nan 0.83    nan 0.815\n",
      "   nan 0.83    nan 0.815   nan 0.845   nan 0.815   nan 0.77    nan 0.875\n",
      "   nan 0.825   nan 0.875   nan 0.84    nan 0.875   nan 0.845   nan 0.875\n",
      "   nan 0.86    nan 0.875   nan 0.79    nan 0.88    nan 0.84    nan 0.88\n",
      "   nan 0.845   nan 0.88    nan 0.84    nan 0.88    nan 0.87    nan 0.88\n",
      "   nan 0.61    nan 0.69    nan 0.6     nan 0.69    nan 0.62    nan 0.69\n",
      "   nan 0.665   nan 0.69    nan 0.67    nan 0.69    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.61    nan 0.725   nan 0.61    nan 0.725   nan 0.62    nan 0.725\n",
      "   nan 0.69    nan 0.725   nan 0.73    nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.61    nan 0.7     nan 0.615   nan 0.7     nan 0.63    nan 0.7\n",
      "   nan 0.735   nan 0.7     nan 0.75    nan 0.7     nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 0.1, 'lr__penalty': 'none'}.\n",
      " Score: 0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['mention_hotel_count'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-induction",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "premier-norman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('numeric_features',\n",
       "                                                 Pipeline(steps=[('selector_num',\n",
       "                                                                  FunctionTransformer(func=<function get_numeric_data_hotel_tme at 0x7fbb38dbf670>))])),\n",
       "                                                ('text_features',\n",
       "                                                 Pipeline(steps=[('selector_text',\n",
       "                                                                  FunctionTransformer(func=<function get_text_data_hotel_tme at 0x7fbb36975790>)),\n",
       "                                                                 ('vectorizer',\n",
       "                                                                  TfidfVectorizer(max_df=0.5,\n",
       "                                                                                  max_features=1000,\n",
       "                                                                                  min_df=5))]))])),\n",
       "                ('lr', LogisticRegression(max_iter=1000, penalty='none'))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel\n",
    "\n",
    "\n",
    "def get_text_data_hotel_tme(df):\n",
    "    df = df.copy()\n",
    "    df['textdata'] = clean_text(df['recent_100_statuses'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_hotel_tme)\n",
    "\n",
    "\n",
    "def get_numeric_data_hotel_tme(df):\n",
    "    data = df['mention_hotel_count'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_hotel_tme)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=1000, min_df=5)),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000, penalty='none'))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "palestinian-tennis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_hotel_tme.sav']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'classifier_hotel_tme.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-water",
   "metadata": {},
   "source": [
    "# Name Description Tweets and Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-impossible",
   "metadata": {},
   "source": [
    "## Without NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "crazy-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "central-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.735 0.65  0.69  0.635 0.87  0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.82  0.68  0.69  0.575 0.805 0.675 0.69  0.575 0.745 0.65  0.69  0.635\n",
      " 0.88  0.67  0.69  0.5   0.88  0.67  0.69  0.5   0.845 0.68  0.69  0.575\n",
      " 0.81  0.67  0.69  0.575 0.765 0.65  0.69  0.635 0.87  0.67  0.69  0.5\n",
      " 0.87  0.67  0.69  0.5   0.83  0.68  0.69  0.575 0.81  0.67  0.69  0.575\n",
      " 0.725 0.65  0.69  0.635 0.865 0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.825 0.68  0.69  0.575 0.805 0.675 0.69  0.575 0.74  0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.5   0.88  0.67  0.69  0.5   0.845 0.68  0.69  0.575\n",
      " 0.835 0.67  0.69  0.575 0.765 0.65  0.69  0.635 0.87  0.67  0.69  0.5\n",
      " 0.87  0.67  0.69  0.5   0.83  0.68  0.69  0.575 0.81  0.67  0.69  0.575\n",
      " 0.705 0.65  0.69  0.635 0.86  0.67  0.69  0.5   0.875 0.67  0.69  0.5\n",
      " 0.87  0.68  0.69  0.575 0.87  0.675 0.69  0.575 0.74  0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.5   0.875 0.67  0.69  0.5   0.85  0.68  0.69  0.575\n",
      " 0.835 0.67  0.69  0.575 0.765 0.65  0.69  0.635 0.87  0.67  0.69  0.5\n",
      " 0.87  0.67  0.69  0.5   0.83  0.68  0.69  0.575 0.81  0.67  0.69  0.575\n",
      " 0.74  0.65  0.69  0.635 0.865 0.67  0.69  0.5   0.87  0.67  0.69  0.5\n",
      " 0.845 0.68  0.69  0.575 0.78  0.675 0.69  0.575 0.76  0.65  0.69  0.635\n",
      " 0.875 0.67  0.69  0.5   0.865 0.67  0.69  0.5   0.88  0.68  0.69  0.575\n",
      " 0.81  0.67  0.69  0.575 0.77  0.65  0.69  0.635 0.875 0.67  0.69  0.5\n",
      " 0.865 0.67  0.69  0.5   0.85  0.68  0.69  0.575 0.825 0.67  0.69  0.575\n",
      " 0.735 0.65  0.69  0.635 0.86  0.67  0.69  0.5   0.86  0.67  0.69  0.5\n",
      " 0.835 0.68  0.69  0.575 0.78  0.675 0.69  0.575 0.75  0.65  0.69  0.635\n",
      " 0.865 0.67  0.69  0.5   0.865 0.67  0.69  0.5   0.85  0.68  0.69  0.575\n",
      " 0.82  0.67  0.69  0.575 0.77  0.65  0.69  0.635 0.875 0.67  0.69  0.5\n",
      " 0.865 0.67  0.69  0.5   0.85  0.68  0.69  0.575 0.825 0.67  0.69  0.575\n",
      " 0.715 0.65  0.69  0.635 0.85  0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.845 0.68  0.69  0.495 0.845 0.675 0.69  0.495 0.75  0.65  0.69  0.635\n",
      " 0.865 0.67  0.69  0.5   0.865 0.67  0.69  0.5   0.85  0.68  0.69  0.575\n",
      " 0.81  0.67  0.69  0.575 0.77  0.65  0.69  0.635 0.875 0.67  0.69  0.5\n",
      " 0.865 0.67  0.69  0.5   0.85  0.68  0.69  0.575 0.825 0.67  0.69  0.575\n",
      " 0.685 0.65  0.69  0.635 0.68  0.67  0.69  0.5   0.69  0.67  0.69  0.5\n",
      " 0.725 0.68  0.69  0.495 0.665 0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.65  0.69  0.635 0.685 0.67  0.69  0.5   0.685 0.67  0.69  0.5\n",
      " 0.72  0.68  0.69  0.495 0.65  0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.65  0.69  0.635 0.69  0.67  0.69  0.5   0.695 0.67  0.69  0.5\n",
      " 0.71  0.68  0.69  0.495 0.7   0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.8800000000000001\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.65  0.65  0.68  0.685 0.74  0.725 0.715 0.735 0.73  0.735 0.745 0.73\n",
      " 0.735 0.735 0.765 0.74  0.76  0.755 0.765 0.74  0.645 0.645 0.675 0.68\n",
      " 0.745 0.73  0.71  0.74  0.735 0.74  0.745 0.745 0.75  0.75  0.765 0.745\n",
      " 0.76  0.755 0.77  0.74  0.64  0.64  0.675 0.68  0.745 0.73  0.715 0.745\n",
      " 0.745 0.75  0.75  0.75  0.75  0.75  0.765 0.76  0.76  0.755 0.77  0.745\n",
      " 0.635 0.635 0.675 0.67  0.74  0.725 0.71  0.73  0.73  0.735 0.755 0.73\n",
      " 0.755 0.755 0.765 0.745 0.76  0.755 0.77  0.74  0.645 0.645 0.68  0.68\n",
      " 0.745 0.73  0.71  0.745 0.74  0.745 0.755 0.74  0.76  0.76  0.77  0.755\n",
      " 0.76  0.755 0.77  0.74  0.64  0.64  0.675 0.68  0.745 0.73  0.715 0.745\n",
      " 0.745 0.75  0.75  0.75  0.75  0.75  0.765 0.76  0.76  0.755 0.77  0.745\n",
      " 0.67  0.67  0.665 0.675 0.74  0.73  0.715 0.73  0.75  0.76  0.75  0.74\n",
      " 0.76  0.76  0.775 0.76  0.77  0.77  0.77  0.755 0.645 0.645 0.68  0.68\n",
      " 0.74  0.725 0.715 0.74  0.735 0.74  0.755 0.74  0.76  0.76  0.77  0.755\n",
      " 0.76  0.755 0.77  0.74  0.64  0.64  0.675 0.68  0.745 0.73  0.715 0.745\n",
      " 0.745 0.75  0.75  0.75  0.75  0.75  0.765 0.76  0.76  0.755 0.77  0.745\n",
      " 0.65  0.65  0.675 0.65  0.74  0.73  0.72  0.735 0.735 0.745 0.74  0.73\n",
      " 0.735 0.74  0.76  0.74  0.76  0.76  0.775 0.745 0.65  0.65  0.68  0.655\n",
      " 0.755 0.745 0.72  0.74  0.745 0.755 0.745 0.745 0.745 0.75  0.76  0.75\n",
      " 0.765 0.765 0.78  0.755 0.65  0.65  0.665 0.65  0.745 0.735 0.71  0.725\n",
      " 0.735 0.745 0.745 0.74  0.745 0.75  0.76  0.75  0.76  0.76  0.775 0.745\n",
      " 0.65  0.65  0.675 0.655 0.745 0.735 0.72  0.735 0.735 0.745 0.745 0.725\n",
      " 0.74  0.745 0.76  0.735 0.76  0.76  0.775 0.745 0.655 0.655 0.675 0.655\n",
      " 0.755 0.745 0.715 0.74  0.735 0.745 0.75  0.735 0.745 0.75  0.76  0.745\n",
      " 0.76  0.76  0.775 0.75  0.65  0.65  0.665 0.65  0.745 0.735 0.71  0.725\n",
      " 0.735 0.745 0.745 0.74  0.745 0.75  0.76  0.75  0.76  0.76  0.775 0.745\n",
      " 0.685 0.685 0.67  0.685 0.74  0.73  0.72  0.73  0.735 0.745 0.745 0.74\n",
      " 0.755 0.755 0.765 0.75  0.77  0.77  0.775 0.745 0.66  0.66  0.675 0.66\n",
      " 0.755 0.745 0.715 0.74  0.735 0.745 0.75  0.735 0.745 0.75  0.76  0.75\n",
      " 0.76  0.76  0.775 0.75  0.65  0.65  0.665 0.65  0.745 0.735 0.71  0.725\n",
      " 0.735 0.745 0.745 0.74  0.745 0.75  0.76  0.75  0.76  0.76  0.775 0.745\n",
      " 0.595 0.595 0.665 0.635 0.675 0.665 0.68  0.69  0.7   0.715 0.735 0.69\n",
      " 0.71  0.715 0.75  0.71  0.745 0.73  0.765 0.72    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.6   0.6   0.665 0.645 0.685 0.695 0.685 0.7   0.69  0.725 0.73  0.73\n",
      " 0.715 0.73  0.75  0.73  0.75  0.74  0.765 0.735   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.67  0.67  0.68  0.67  0.74  0.735 0.705 0.735 0.735 0.745 0.75  0.735\n",
      " 0.755 0.755 0.755 0.75  0.75  0.745 0.775 0.74    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'knn__n_neighbors': 10, 'knn__weights': 'uniform'}.\n",
      " Score: 0.78\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.735   nan 0.85    nan 0.8     nan 0.85    nan 0.855   nan 0.85\n",
      "   nan 0.855   nan 0.85    nan 0.86    nan 0.85    nan 0.735   nan 0.86\n",
      "   nan 0.825   nan 0.86    nan 0.875   nan 0.86    nan 0.87    nan 0.86\n",
      "   nan 0.865   nan 0.86    nan 0.735   nan 0.855   nan 0.825   nan 0.855\n",
      "   nan 0.865   nan 0.855   nan 0.86    nan 0.855   nan 0.855   nan 0.855\n",
      "   nan 0.725   nan 0.83    nan 0.8     nan 0.83    nan 0.84    nan 0.83\n",
      "   nan 0.87    nan 0.83    nan 0.855   nan 0.83    nan 0.735   nan 0.855\n",
      "   nan 0.815   nan 0.855   nan 0.86    nan 0.855   nan 0.865   nan 0.855\n",
      "   nan 0.86    nan 0.855   nan 0.735   nan 0.855   nan 0.825   nan 0.855\n",
      "   nan 0.865   nan 0.855   nan 0.86    nan 0.855   nan 0.855   nan 0.855\n",
      "   nan 0.705   nan 0.835   nan 0.77    nan 0.835   nan 0.805   nan 0.835\n",
      "   nan 0.875   nan 0.835   nan 0.865   nan 0.835   nan 0.735   nan 0.865\n",
      "   nan 0.815   nan 0.865   nan 0.86    nan 0.865   nan 0.865   nan 0.865\n",
      "   nan 0.86    nan 0.865   nan 0.735   nan 0.855   nan 0.825   nan 0.855\n",
      "   nan 0.865   nan 0.855   nan 0.86    nan 0.855   nan 0.855   nan 0.855\n",
      "   nan 0.725   nan 0.815   nan 0.8     nan 0.815   nan 0.845   nan 0.815\n",
      "   nan 0.85    nan 0.815   nan 0.85    nan 0.815   nan 0.73    nan 0.85\n",
      "   nan 0.82    nan 0.85    nan 0.845   nan 0.85    nan 0.855   nan 0.85\n",
      "   nan 0.85    nan 0.85    nan 0.725   nan 0.835   nan 0.82    nan 0.835\n",
      "   nan 0.84    nan 0.835   nan 0.855   nan 0.835   nan 0.85    nan 0.835\n",
      "   nan 0.715   nan 0.84    nan 0.79    nan 0.84    nan 0.84    nan 0.84\n",
      "   nan 0.85    nan 0.84    nan 0.85    nan 0.84    nan 0.73    nan 0.85\n",
      "   nan 0.82    nan 0.85    nan 0.845   nan 0.85    nan 0.855   nan 0.85\n",
      "   nan 0.855   nan 0.85    nan 0.725   nan 0.835   nan 0.82    nan 0.835\n",
      "   nan 0.84    nan 0.835   nan 0.855   nan 0.835   nan 0.85    nan 0.835\n",
      "   nan 0.71    nan 0.845   nan 0.775   nan 0.845   nan 0.81    nan 0.845\n",
      "   nan 0.86    nan 0.845   nan 0.85    nan 0.845   nan 0.73    nan 0.87\n",
      "   nan 0.82    nan 0.87    nan 0.84    nan 0.87    nan 0.855   nan 0.87\n",
      "   nan 0.855   nan 0.87    nan 0.725   nan 0.835   nan 0.82    nan 0.835\n",
      "   nan 0.84    nan 0.835   nan 0.855   nan 0.835   nan 0.85    nan 0.835\n",
      "   nan 0.69    nan 0.655   nan 0.695   nan 0.655   nan 0.7     nan 0.655\n",
      "   nan 0.725   nan 0.655   nan 0.745   nan 0.655   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.685   nan 0.725   nan 0.69    nan 0.725   nan 0.695   nan 0.725\n",
      "   nan 0.725   nan 0.725   nan 0.735   nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.695   nan 0.715   nan 0.695   nan 0.715   nan 0.69    nan 0.715\n",
      "   nan 0.715   nan 0.715   nan 0.71    nan 0.715   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.875\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000', 'mention_hotel_count']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-uruguay",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "guided-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "spare-twelve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.745 0.65  0.69  0.635 0.875 0.67  0.69  0.5   0.885 0.67  0.69  0.5\n",
      " 0.865 0.68  0.69  0.575 0.845 0.675 0.69  0.575 0.735 0.65  0.69  0.635\n",
      " 0.885 0.67  0.69  0.5   0.89  0.67  0.69  0.5   0.86  0.68  0.69  0.575\n",
      " 0.83  0.675 0.69  0.575 0.755 0.65  0.69  0.635 0.88  0.67  0.69  0.5\n",
      " 0.885 0.67  0.69  0.5   0.855 0.68  0.69  0.575 0.835 0.675 0.69  0.575\n",
      " 0.73  0.65  0.69  0.635 0.85  0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.865 0.68  0.69  0.575 0.85  0.675 0.69  0.575 0.735 0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.5   0.89  0.67  0.69  0.5   0.87  0.68  0.69  0.495\n",
      " 0.845 0.675 0.69  0.495 0.755 0.65  0.69  0.635 0.88  0.67  0.69  0.5\n",
      " 0.885 0.67  0.69  0.5   0.855 0.68  0.69  0.575 0.835 0.675 0.69  0.575\n",
      " 0.7   0.65  0.69  0.635 0.85  0.67  0.69  0.5   0.87  0.67  0.69  0.5\n",
      " 0.86  0.68  0.69  0.495 0.86  0.675 0.69  0.495 0.735 0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.5   0.89  0.67  0.69  0.5   0.87  0.68  0.69  0.495\n",
      " 0.845 0.675 0.69  0.495 0.755 0.65  0.69  0.635 0.88  0.67  0.69  0.5\n",
      " 0.885 0.67  0.69  0.5   0.855 0.68  0.69  0.575 0.835 0.675 0.69  0.575\n",
      " 0.745 0.65  0.69  0.635 0.875 0.67  0.69  0.5   0.88  0.67  0.69  0.5\n",
      " 0.86  0.68  0.69  0.575 0.85  0.675 0.69  0.575 0.735 0.65  0.69  0.635\n",
      " 0.88  0.67  0.69  0.5   0.89  0.67  0.69  0.5   0.86  0.68  0.69  0.575\n",
      " 0.825 0.675 0.69  0.575 0.76  0.65  0.69  0.635 0.88  0.67  0.69  0.5\n",
      " 0.88  0.67  0.69  0.5   0.86  0.68  0.69  0.575 0.815 0.675 0.69  0.575\n",
      " 0.725 0.65  0.69  0.635 0.85  0.67  0.69  0.5   0.855 0.67  0.69  0.5\n",
      " 0.86  0.68  0.69  0.575 0.86  0.675 0.69  0.575 0.735 0.65  0.69  0.635\n",
      " 0.875 0.67  0.69  0.5   0.885 0.67  0.69  0.5   0.87  0.68  0.69  0.495\n",
      " 0.84  0.675 0.69  0.495 0.76  0.65  0.69  0.635 0.88  0.67  0.69  0.5\n",
      " 0.88  0.67  0.69  0.5   0.86  0.68  0.69  0.575 0.815 0.675 0.69  0.575\n",
      " 0.705 0.65  0.69  0.635 0.85  0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.85  0.68  0.69  0.495 0.855 0.675 0.69  0.495 0.735 0.65  0.69  0.635\n",
      " 0.875 0.67  0.69  0.5   0.885 0.67  0.69  0.5   0.87  0.68  0.69  0.495\n",
      " 0.84  0.675 0.69  0.495 0.76  0.65  0.69  0.635 0.88  0.67  0.69  0.5\n",
      " 0.88  0.67  0.69  0.5   0.86  0.68  0.69  0.575 0.815 0.675 0.69  0.575\n",
      " 0.685 0.65  0.69  0.635 0.68  0.67  0.69  0.5   0.68  0.67  0.69  0.5\n",
      " 0.73  0.68  0.69  0.495 0.66  0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.65  0.69  0.635 0.69  0.67  0.69  0.5   0.685 0.67  0.69  0.5\n",
      " 0.715 0.68  0.69  0.495 0.655 0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.65  0.69  0.635 0.69  0.67  0.69  0.5   0.695 0.67  0.69  0.5\n",
      " 0.71  0.68  0.69  0.495 0.7   0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.89\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.635 0.635 0.67  0.675 0.745 0.73  0.71  0.735 0.735 0.74  0.75  0.735\n",
      " 0.745 0.745 0.765 0.75  0.755 0.745 0.77  0.745 0.635 0.635 0.67  0.67\n",
      " 0.73  0.715 0.715 0.745 0.735 0.74  0.75  0.73  0.75  0.75  0.77  0.75\n",
      " 0.765 0.755 0.765 0.74  0.645 0.645 0.67  0.68  0.735 0.72  0.715 0.74\n",
      " 0.73  0.735 0.75  0.73  0.745 0.745 0.765 0.745 0.75  0.74  0.77  0.745\n",
      " 0.635 0.635 0.67  0.675 0.72  0.705 0.7   0.72  0.735 0.74  0.755 0.73\n",
      " 0.755 0.755 0.76  0.755 0.76  0.75  0.765 0.745 0.63  0.63  0.675 0.66\n",
      " 0.74  0.725 0.715 0.745 0.74  0.745 0.755 0.735 0.755 0.755 0.765 0.755\n",
      " 0.76  0.75  0.765 0.74  0.645 0.645 0.67  0.68  0.735 0.72  0.715 0.74\n",
      " 0.73  0.735 0.75  0.73  0.745 0.745 0.765 0.745 0.75  0.74  0.77  0.745\n",
      " 0.67  0.67  0.68  0.68  0.735 0.725 0.71  0.725 0.745 0.755 0.75  0.74\n",
      " 0.75  0.755 0.765 0.75  0.76  0.755 0.77  0.745 0.63  0.63  0.675 0.66\n",
      " 0.74  0.725 0.715 0.745 0.74  0.745 0.755 0.735 0.755 0.755 0.765 0.755\n",
      " 0.76  0.75  0.765 0.74  0.645 0.645 0.67  0.68  0.735 0.72  0.715 0.74\n",
      " 0.73  0.735 0.75  0.73  0.745 0.745 0.765 0.745 0.75  0.74  0.77  0.745\n",
      " 0.64  0.64  0.675 0.645 0.745 0.735 0.71  0.735 0.735 0.745 0.75  0.74\n",
      " 0.74  0.745 0.755 0.75  0.75  0.745 0.765 0.75  0.64  0.64  0.67  0.64\n",
      " 0.735 0.725 0.705 0.735 0.735 0.745 0.75  0.735 0.74  0.745 0.755 0.75\n",
      " 0.755 0.75  0.765 0.745 0.65  0.65  0.67  0.645 0.74  0.73  0.72  0.74\n",
      " 0.73  0.74  0.755 0.73  0.74  0.745 0.76  0.755 0.755 0.75  0.765 0.755\n",
      " 0.64  0.64  0.675 0.645 0.735 0.725 0.71  0.73  0.735 0.745 0.755 0.73\n",
      " 0.75  0.755 0.755 0.76  0.755 0.75  0.765 0.745 0.625 0.625 0.66  0.62\n",
      " 0.735 0.725 0.71  0.735 0.735 0.745 0.755 0.735 0.75  0.755 0.76  0.76\n",
      " 0.755 0.75  0.765 0.745 0.65  0.65  0.67  0.645 0.74  0.73  0.72  0.74\n",
      " 0.73  0.74  0.755 0.73  0.74  0.745 0.76  0.755 0.755 0.75  0.765 0.755\n",
      " 0.68  0.68  0.675 0.685 0.74  0.73  0.715 0.73  0.745 0.755 0.745 0.74\n",
      " 0.74  0.745 0.765 0.75  0.76  0.755 0.765 0.745 0.625 0.625 0.66  0.62\n",
      " 0.735 0.725 0.71  0.735 0.735 0.745 0.755 0.735 0.75  0.755 0.76  0.76\n",
      " 0.755 0.75  0.765 0.745 0.65  0.65  0.67  0.645 0.74  0.73  0.72  0.74\n",
      " 0.73  0.74  0.755 0.73  0.74  0.745 0.76  0.755 0.755 0.75  0.765 0.755\n",
      " 0.585 0.585 0.665 0.635 0.675 0.66  0.71  0.7   0.735 0.72  0.76  0.71\n",
      " 0.725 0.705 0.755 0.71  0.75  0.715 0.76  0.72    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.585 0.585 0.66  0.635 0.7   0.69  0.705 0.705 0.72  0.735 0.74  0.73\n",
      " 0.71  0.725 0.75  0.725 0.74  0.73  0.765 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.67  0.67  0.68  0.67  0.735 0.73  0.7   0.73  0.74  0.75  0.755 0.74\n",
      " 0.75  0.755 0.755 0.74  0.75  0.745 0.775 0.74    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 1, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 10, 'knn__weights': 'uniform'}.\n",
      " Score: 0.775\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.715   nan 0.815   nan 0.805   nan 0.815   nan 0.835   nan 0.815\n",
      "   nan 0.875   nan 0.815   nan 0.87    nan 0.815   nan 0.725   nan 0.875\n",
      "   nan 0.82    nan 0.875   nan 0.87    nan 0.875   nan 0.88    nan 0.875\n",
      "   nan 0.885   nan 0.875   nan 0.735   nan 0.87    nan 0.85    nan 0.87\n",
      "   nan 0.865   nan 0.87    nan 0.885   nan 0.87    nan 0.885   nan 0.87\n",
      "   nan 0.715   nan 0.845   nan 0.78    nan 0.845   nan 0.835   nan 0.845\n",
      "   nan 0.865   nan 0.845   nan 0.86    nan 0.845   nan 0.725   nan 0.885\n",
      "   nan 0.815   nan 0.885   nan 0.87    nan 0.885   nan 0.875   nan 0.885\n",
      "   nan 0.885   nan 0.885   nan 0.735   nan 0.87    nan 0.85    nan 0.87\n",
      "   nan 0.865   nan 0.87    nan 0.885   nan 0.87    nan 0.885   nan 0.87\n",
      "   nan 0.705   nan 0.855   nan 0.75    nan 0.855   nan 0.81    nan 0.855\n",
      "   nan 0.865   nan 0.855   nan 0.865   nan 0.855   nan 0.725   nan 0.885\n",
      "   nan 0.815   nan 0.885   nan 0.87    nan 0.885   nan 0.875   nan 0.885\n",
      "   nan 0.885   nan 0.885   nan 0.735   nan 0.87    nan 0.85    nan 0.87\n",
      "   nan 0.865   nan 0.87    nan 0.885   nan 0.87    nan 0.885   nan 0.87\n",
      "   nan 0.715   nan 0.8     nan 0.81    nan 0.8     nan 0.845   nan 0.8\n",
      "   nan 0.885   nan 0.8     nan 0.875   nan 0.8     nan 0.725   nan 0.83\n",
      "   nan 0.83    nan 0.83    nan 0.875   nan 0.83    nan 0.875   nan 0.83\n",
      "   nan 0.875   nan 0.83    nan 0.735   nan 0.86    nan 0.835   nan 0.86\n",
      "   nan 0.86    nan 0.86    nan 0.88    nan 0.86    nan 0.88    nan 0.86\n",
      "   nan 0.715   nan 0.8     nan 0.78    nan 0.8     nan 0.835   nan 0.8\n",
      "   nan 0.865   nan 0.8     nan 0.86    nan 0.8     nan 0.725   nan 0.845\n",
      "   nan 0.815   nan 0.845   nan 0.865   nan 0.845   nan 0.87    nan 0.845\n",
      "   nan 0.875   nan 0.845   nan 0.735   nan 0.86    nan 0.835   nan 0.86\n",
      "   nan 0.86    nan 0.86    nan 0.88    nan 0.86    nan 0.88    nan 0.86\n",
      "   nan 0.705   nan 0.835   nan 0.75    nan 0.835   nan 0.805   nan 0.835\n",
      "   nan 0.86    nan 0.835   nan 0.86    nan 0.835   nan 0.725   nan 0.845\n",
      "   nan 0.815   nan 0.845   nan 0.865   nan 0.845   nan 0.87    nan 0.845\n",
      "   nan 0.875   nan 0.845   nan 0.735   nan 0.86    nan 0.835   nan 0.86\n",
      "   nan 0.86    nan 0.86    nan 0.88    nan 0.86    nan 0.88    nan 0.86\n",
      "   nan 0.69    nan 0.615   nan 0.69    nan 0.615   nan 0.7     nan 0.615\n",
      "   nan 0.72    nan 0.615   nan 0.735   nan 0.615   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.685   nan 0.725   nan 0.69    nan 0.725   nan 0.695   nan 0.725\n",
      "   nan 0.725   nan 0.725   nan 0.73    nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.69    nan 0.715   nan 0.695   nan 0.715   nan 0.69    nan 0.715\n",
      "   nan 0.715   nan 0.715   nan 0.71    nan 0.715   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.885\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000', 'mention_hotel_count']].to_numpy()\n",
    "    return data\n",
    "\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-retention",
   "metadata": {},
   "source": [
    "## Lemmatization and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "measured-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "gorgeous-couple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.74  0.65  0.69  0.635 0.88  0.67  0.69  0.5   0.885 0.67  0.69  0.5\n",
      " 0.855 0.68  0.69  0.575 0.835 0.675 0.69  0.575 0.75  0.65  0.69  0.635\n",
      " 0.89  0.67  0.69  0.5   0.89  0.67  0.69  0.5   0.855 0.68  0.69  0.575\n",
      " 0.845 0.675 0.69  0.575 0.765 0.65  0.69  0.635 0.885 0.67  0.69  0.5\n",
      " 0.885 0.67  0.69  0.5   0.86  0.68  0.69  0.575 0.84  0.675 0.69  0.575\n",
      " 0.735 0.65  0.69  0.635 0.865 0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.865 0.68  0.69  0.575 0.865 0.675 0.69  0.575 0.745 0.65  0.69  0.635\n",
      " 0.875 0.67  0.69  0.5   0.89  0.67  0.69  0.5   0.855 0.68  0.69  0.575\n",
      " 0.84  0.675 0.69  0.575 0.765 0.65  0.69  0.635 0.885 0.67  0.69  0.5\n",
      " 0.885 0.67  0.69  0.5   0.86  0.68  0.69  0.575 0.84  0.675 0.69  0.575\n",
      " 0.71  0.65  0.69  0.635 0.86  0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.865 0.68  0.69  0.495 0.865 0.675 0.69  0.495 0.745 0.65  0.69  0.635\n",
      " 0.875 0.67  0.69  0.5   0.89  0.67  0.69  0.5   0.855 0.68  0.69  0.575\n",
      " 0.84  0.675 0.69  0.575 0.765 0.65  0.69  0.635 0.885 0.67  0.69  0.5\n",
      " 0.885 0.67  0.69  0.5   0.86  0.68  0.69  0.575 0.84  0.675 0.69  0.575\n",
      " 0.74  0.65  0.69  0.635 0.875 0.67  0.69  0.5   0.875 0.67  0.69  0.5\n",
      " 0.845 0.68  0.69  0.575 0.84  0.675 0.69  0.575 0.75  0.65  0.69  0.635\n",
      " 0.89  0.67  0.69  0.5   0.885 0.67  0.69  0.5   0.86  0.68  0.69  0.575\n",
      " 0.865 0.675 0.69  0.575 0.765 0.65  0.69  0.635 0.89  0.67  0.69  0.5\n",
      " 0.875 0.67  0.69  0.5   0.855 0.68  0.69  0.575 0.845 0.675 0.69  0.575\n",
      " 0.735 0.65  0.69  0.635 0.865 0.67  0.69  0.5   0.87  0.67  0.69  0.5\n",
      " 0.86  0.68  0.69  0.575 0.86  0.675 0.69  0.575 0.745 0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.5   0.88  0.67  0.69  0.5   0.865 0.68  0.69  0.575\n",
      " 0.87  0.675 0.69  0.575 0.765 0.65  0.69  0.635 0.89  0.67  0.69  0.5\n",
      " 0.875 0.67  0.69  0.5   0.855 0.68  0.69  0.575 0.845 0.675 0.69  0.575\n",
      " 0.715 0.65  0.69  0.635 0.86  0.67  0.69  0.5   0.865 0.67  0.69  0.5\n",
      " 0.86  0.68  0.69  0.495 0.86  0.675 0.69  0.495 0.745 0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.5   0.88  0.67  0.69  0.5   0.865 0.68  0.69  0.575\n",
      " 0.87  0.675 0.69  0.575 0.765 0.65  0.69  0.635 0.89  0.67  0.69  0.5\n",
      " 0.875 0.67  0.69  0.5   0.855 0.68  0.69  0.575 0.845 0.675 0.69  0.575\n",
      " 0.685 0.65  0.69  0.635 0.685 0.67  0.69  0.5   0.68  0.67  0.69  0.5\n",
      " 0.72  0.68  0.69  0.495 0.665 0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.65  0.69  0.635 0.69  0.67  0.69  0.5   0.685 0.67  0.69  0.5\n",
      " 0.74  0.68  0.69  0.495 0.65  0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.65  0.69  0.635 0.69  0.67  0.69  0.5   0.695 0.67  0.69  0.5\n",
      " 0.71  0.68  0.69  0.495 0.7   0.675 0.69  0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 0.5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.89\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.65  0.65  0.675 0.685 0.74  0.725 0.71  0.735 0.735 0.74  0.75  0.735\n",
      " 0.745 0.745 0.76  0.745 0.76  0.75  0.77  0.74  0.635 0.635 0.675 0.675\n",
      " 0.745 0.73  0.71  0.74  0.74  0.745 0.755 0.74  0.75  0.75  0.76  0.745\n",
      " 0.76  0.75  0.77  0.74  0.65  0.65  0.665 0.68  0.735 0.72  0.71  0.73\n",
      " 0.73  0.735 0.75  0.73  0.75  0.75  0.76  0.745 0.76  0.75  0.775 0.755\n",
      " 0.635 0.635 0.67  0.67  0.73  0.715 0.71  0.73  0.73  0.735 0.75  0.725\n",
      " 0.75  0.75  0.765 0.745 0.755 0.745 0.77  0.735 0.64  0.64  0.68  0.68\n",
      " 0.745 0.73  0.71  0.735 0.73  0.735 0.75  0.73  0.75  0.75  0.765 0.75\n",
      " 0.755 0.745 0.77  0.74  0.65  0.65  0.665 0.68  0.735 0.72  0.71  0.73\n",
      " 0.73  0.735 0.75  0.73  0.75  0.75  0.76  0.745 0.76  0.75  0.775 0.755\n",
      " 0.655 0.655 0.675 0.665 0.745 0.735 0.72  0.73  0.745 0.755 0.75  0.74\n",
      " 0.75  0.755 0.765 0.75  0.755 0.75  0.77  0.745 0.64  0.64  0.68  0.68\n",
      " 0.745 0.73  0.71  0.735 0.73  0.735 0.75  0.73  0.75  0.75  0.765 0.75\n",
      " 0.755 0.745 0.77  0.74  0.65  0.65  0.665 0.68  0.735 0.72  0.71  0.73\n",
      " 0.73  0.735 0.75  0.73  0.75  0.75  0.76  0.745 0.76  0.75  0.775 0.755\n",
      " 0.645 0.645 0.67  0.65  0.735 0.725 0.71  0.73  0.73  0.74  0.75  0.735\n",
      " 0.75  0.755 0.76  0.755 0.75  0.745 0.765 0.745 0.635 0.635 0.675 0.64\n",
      " 0.735 0.725 0.71  0.735 0.735 0.745 0.75  0.735 0.75  0.755 0.76  0.755\n",
      " 0.755 0.75  0.77  0.745 0.64  0.64  0.665 0.64  0.745 0.735 0.725 0.74\n",
      " 0.74  0.75  0.76  0.735 0.755 0.76  0.76  0.755 0.755 0.75  0.775 0.755\n",
      " 0.64  0.64  0.67  0.635 0.73  0.72  0.71  0.725 0.735 0.745 0.75  0.725\n",
      " 0.74  0.745 0.755 0.75  0.755 0.75  0.77  0.74  0.64  0.64  0.675 0.64\n",
      " 0.745 0.735 0.71  0.74  0.735 0.745 0.75  0.73  0.75  0.755 0.76  0.76\n",
      " 0.755 0.75  0.765 0.745 0.64  0.64  0.665 0.64  0.745 0.735 0.725 0.74\n",
      " 0.74  0.75  0.76  0.735 0.755 0.76  0.76  0.755 0.755 0.75  0.775 0.755\n",
      " 0.655 0.655 0.68  0.665 0.735 0.725 0.715 0.725 0.745 0.755 0.745 0.74\n",
      " 0.75  0.755 0.76  0.75  0.76  0.755 0.77  0.745 0.64  0.64  0.675 0.64\n",
      " 0.745 0.735 0.71  0.74  0.735 0.745 0.75  0.73  0.75  0.755 0.76  0.76\n",
      " 0.755 0.75  0.765 0.745 0.64  0.64  0.665 0.64  0.745 0.735 0.725 0.74\n",
      " 0.74  0.75  0.76  0.735 0.755 0.76  0.76  0.755 0.755 0.75  0.775 0.755\n",
      " 0.595 0.595 0.665 0.64  0.665 0.66  0.705 0.69  0.725 0.71  0.745 0.7\n",
      " 0.73  0.71  0.755 0.71  0.745 0.71  0.76  0.71    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.595 0.595 0.655 0.635 0.7   0.705 0.705 0.7   0.715 0.735 0.74  0.725\n",
      " 0.72  0.73  0.75  0.73  0.745 0.735 0.755 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.665 0.665 0.67  0.67  0.735 0.73  0.71  0.725 0.735 0.745 0.755 0.735\n",
      " 0.75  0.75  0.765 0.75  0.755 0.75  0.77  0.745   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 10, 'knn__n_neighbors': 10, 'knn__weights': 'uniform'}.\n",
      " Score: 0.775\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.72    nan 0.815   nan 0.825   nan 0.815   nan 0.85    nan 0.815\n",
      "   nan 0.885   nan 0.815   nan 0.88    nan 0.815   nan 0.725   nan 0.85\n",
      "   nan 0.825   nan 0.85    nan 0.865   nan 0.85    nan 0.88    nan 0.85\n",
      "   nan 0.89    nan 0.85    nan 0.735   nan 0.85    nan 0.845   nan 0.85\n",
      "   nan 0.87    nan 0.85    nan 0.875   nan 0.85    nan 0.88    nan 0.85\n",
      "   nan 0.72    nan 0.805   nan 0.8     nan 0.805   nan 0.83    nan 0.805\n",
      "   nan 0.87    nan 0.805   nan 0.88    nan 0.805   nan 0.725   nan 0.86\n",
      "   nan 0.825   nan 0.86    nan 0.865   nan 0.86    nan 0.88    nan 0.86\n",
      "   nan 0.88    nan 0.86    nan 0.735   nan 0.85    nan 0.845   nan 0.85\n",
      "   nan 0.87    nan 0.85    nan 0.875   nan 0.85    nan 0.88    nan 0.85\n",
      "   nan 0.705   nan 0.865   nan 0.765   nan 0.865   nan 0.82    nan 0.865\n",
      "   nan 0.865   nan 0.865   nan 0.87    nan 0.865   nan 0.725   nan 0.86\n",
      "   nan 0.825   nan 0.86    nan 0.865   nan 0.86    nan 0.88    nan 0.86\n",
      "   nan 0.88    nan 0.86    nan 0.735   nan 0.85    nan 0.845   nan 0.85\n",
      "   nan 0.87    nan 0.85    nan 0.875   nan 0.85    nan 0.88    nan 0.85\n",
      "   nan 0.725   nan 0.81    nan 0.825   nan 0.81    nan 0.85    nan 0.81\n",
      "   nan 0.88    nan 0.81    nan 0.875   nan 0.81    nan 0.73    nan 0.84\n",
      "   nan 0.835   nan 0.84    nan 0.865   nan 0.84    nan 0.875   nan 0.84\n",
      "   nan 0.885   nan 0.84    nan 0.735   nan 0.835   nan 0.84    nan 0.835\n",
      "   nan 0.87    nan 0.835   nan 0.875   nan 0.835   nan 0.875   nan 0.835\n",
      "   nan 0.72    nan 0.82    nan 0.795   nan 0.82    nan 0.845   nan 0.82\n",
      "   nan 0.87    nan 0.82    nan 0.87    nan 0.82    nan 0.73    nan 0.85\n",
      "   nan 0.825   nan 0.85    nan 0.865   nan 0.85    nan 0.875   nan 0.85\n",
      "   nan 0.875   nan 0.85    nan 0.735   nan 0.835   nan 0.84    nan 0.835\n",
      "   nan 0.87    nan 0.835   nan 0.875   nan 0.835   nan 0.875   nan 0.835\n",
      "   nan 0.705   nan 0.88    nan 0.765   nan 0.88    nan 0.83    nan 0.88\n",
      "   nan 0.87    nan 0.88    nan 0.87    nan 0.88    nan 0.73    nan 0.85\n",
      "   nan 0.825   nan 0.85    nan 0.865   nan 0.85    nan 0.875   nan 0.85\n",
      "   nan 0.875   nan 0.85    nan 0.735   nan 0.835   nan 0.84    nan 0.835\n",
      "   nan 0.87    nan 0.835   nan 0.875   nan 0.835   nan 0.875   nan 0.835\n",
      "   nan 0.69    nan 0.645   nan 0.69    nan 0.645   nan 0.69    nan 0.645\n",
      "   nan 0.725   nan 0.645   nan 0.74    nan 0.645   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.685   nan 0.705   nan 0.69    nan 0.705   nan 0.7     nan 0.705\n",
      "   nan 0.71    nan 0.705   nan 0.73    nan 0.705   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.695   nan 0.71    nan 0.695   nan 0.71    nan 0.69    nan 0.71\n",
      "   nan 0.705   nan 0.71    nan 0.71    nan 0.71    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000', 'mention_hotel_count']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-injection",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "increased-transportation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('numeric_features',\n",
       "                                                 Pipeline(steps=[('selector_num',\n",
       "                                                                  FunctionTransformer(func=<function get_numeric_data_ at 0x7fbb3c8e9550>))])),\n",
       "                                                ('text_features',\n",
       "                                                 Pipeline(steps=[('selector_text',\n",
       "                                                                  FunctionTransformer(func=<function get_text_data_ at 0x7fbb3c8e94c0>)),\n",
       "                                                                 ('vectorizer',\n",
       "                                                                  TfidfVectorizer(max_df=0.5,\n",
       "                                                                                  max_features=1000,\n",
       "                                                                                  min_df=5))]))])),\n",
       "                ('svm', SVC(C=0.5, kernel='linear'))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel\n",
    "\n",
    "\n",
    "def get_text_data_(df):\n",
    "    df = df.copy()\n",
    "    df['textdata'] = clean_text(df['name'] + ' ' + df['description'] + ' ' + df['recent_100_statuses'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000', 'mention_hotel_count']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=1000, min_df=5)),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC(C=0.5, kernel='linear'))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "saving-sleeve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_hotel_ndtfrfome.sav']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'classifier_hotel_ndtfrfome.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-present",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
