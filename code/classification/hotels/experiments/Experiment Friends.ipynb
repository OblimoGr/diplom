{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "express-initial",
   "metadata": {},
   "source": [
    "# Classification Experiment: Friends\n",
    "---\n",
    "This Notebook, includes a series of experiments, on using a node's Friends for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "earned-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integrated-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_credentials = []\n",
    "with open('../../../../twitter_credentials.json', 'r') as f:\n",
    "    twitter_credentials = json.load(f)\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_credentials['consumer_key'], twitter_credentials['consumer_secret'])\n",
    "auth.set_access_token(twitter_credentials['access_token_key'],twitter_credentials['access_token_secret'])\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, timeout=60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adequate-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function For Text Normalization\n",
    "def clean_text(data):\n",
    "    urls = r'http\\S+'\n",
    "    non_unicode_char = r'\\W'\n",
    "    numbers = r'[0-9_]'\n",
    "    fix_whitespace = r'\\s+'\n",
    "    single_whitespace = ' '\n",
    "    \n",
    "    data = (data.replace([urls], single_whitespace, regex=True)\n",
    "                    .replace([non_unicode_char, numbers], single_whitespace, regex=True)\n",
    "                    .replace(fix_whitespace, single_whitespace, regex=True))\n",
    "    data = data.apply(lambda s: s.lower() if type(s) == str else s)\n",
    "    return data\n",
    "\n",
    "# NLP Functions\n",
    "nlp_el = spacy.load('el_core_news_md')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "STOPWORDS = set(list(spacy.lang.en.STOP_WORDS) + list(spacy.lang.el.STOP_WORDS))\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    row = [str(token) for token in nlp_el(row)]\n",
    "    return [w for w in row if w not in STOPWORDS]\n",
    "\n",
    "def tokenize_lemmatize(row):\n",
    "    return [str(token.lemma_) for token in nlp_el(row)]\n",
    "\n",
    "def tokenize_lemmatize_en(row):\n",
    "    return [str(token.lemma_) for token in nlp_en(row)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unable-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data_hotel_nd(df):\n",
    "    df['textdata'] = clean_text(df['name'] + ' ' + df['description'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "def fetch_friends(node, count=1000):\n",
    "    # Fetch friend IDs\n",
    "    friend_ids = []\n",
    "    try:\n",
    "        for ids in tweepy.Cursor(API.friends_ids, node).items(count):\n",
    "            friend_ids.append(ids)\n",
    "\n",
    "    except tweepy.error.TweepError as err:\n",
    "        return pd.DataFrame(columns=['name', 'description'])\n",
    "\n",
    "    except Exception as err:\n",
    "        raise Exception(f'An unknown Error has occurred.\\n{err}')\n",
    "\n",
    "    # If node has zero friends\n",
    "    if not friend_ids:\n",
    "        return pd.DataFrame(columns=['name', 'description'])\n",
    "\n",
    "    # Calculate Iteration Required, to iterate per 100 ids\n",
    "    if (int(len(friend_ids)) % 100) == 0:\n",
    "        it_num = int(len(friend_ids) / 100)\n",
    "    else:\n",
    "        it_num = (int(len(friend_ids) / 100) + 1)\n",
    "\n",
    "    # Transform IDs to User Objects\n",
    "    users = list()\n",
    "    try:\n",
    "        for i in range(it_num):\n",
    "            users.append(API.lookup_users(friend_ids[100 * i: 100 * (1 + i)]))\n",
    "    except Exception as err:\n",
    "        raise Exception(f'An unknown Error has occurred.\\n{err}')\n",
    "\n",
    "    # Extract Profile Name and Description for each friends and save it to a DataFrame\n",
    "    results = pd.DataFrame()\n",
    "    for items in users:\n",
    "        for user in items:\n",
    "            results = results.append(pd.DataFrame([user.name, user.description]).T)\n",
    "    results.columns = ['name', 'description']\n",
    "    results = results.reset_index().drop('index', axis=1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_hotel_friends(nodes):\n",
    "\n",
    "    counts = []\n",
    "    for node in tqdm(nodes, leave=False):\n",
    "        # Get Required Data\n",
    "        data = fetch_friends(node=node, count=1000)\n",
    "\n",
    "        # Get Labels\n",
    "        if not data.empty:\n",
    "            model_nd = joblib.load('../classifiers/classifier_hotel_nd.sav')\n",
    "            data['label'] = model_nd.predict(data)\n",
    "\n",
    "            count = len(data[data['label'] == 1])\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        counts.append(count)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-bargain",
   "metadata": {},
   "source": [
    "## Calculate Hotel Friends Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-agency",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unlikely-rally",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SophiaSuites</td>\n",
       "      <td>Sophia Collection Santorini</td>\n",
       "      <td>Luxury Suites, hotels and villas Santorini com...</td>\n",
       "      <td>513</td>\n",
       "      <td>41</td>\n",
       "      <td>127</td>\n",
       "      <td>\"The tans will fade but the memories will las...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnthiMariaApart</td>\n",
       "      <td>AnthiMariaApartments</td>\n",
       "      <td>Anthi Maria Beach Apartments is a self-caterin...</td>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "      <td>110</td>\n",
       "      <td>Our fantastic New and Improved abc online web...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wEndowproject</td>\n",
       "      <td>wEndow project</td>\n",
       "      <td>WEndow Escape Resort &amp; Villas | Tailor-made Ad...</td>\n",
       "      <td>350</td>\n",
       "      <td>344</td>\n",
       "      <td>103</td>\n",
       "      <td>https://t.co/DHuXrG8G6o For those who still d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paphotels</td>\n",
       "      <td>paphotels</td>\n",
       "      <td>The best of Greek hospitality! Follow us, visi...</td>\n",
       "      <td>975</td>\n",
       "      <td>1182</td>\n",
       "      <td>475</td>\n",
       "      <td>@AlbertBourla üíØüíØüíØüíØüíØ Happy Easter !!!üê£ @paphot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medpalace</td>\n",
       "      <td>Mediterranean Palace</td>\n",
       "      <td>A cozy 5 star hotel in the city center with an...</td>\n",
       "      <td>269</td>\n",
       "      <td>543</td>\n",
       "      <td>381</td>\n",
       "      <td>https://t.co/WPCR6KSnw2 New era!\\nNew Brand! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                         name  \\\n",
       "0     SophiaSuites  Sophia Collection Santorini   \n",
       "1  AnthiMariaApart         AnthiMariaApartments   \n",
       "2    wEndowproject               wEndow project   \n",
       "3        paphotels                    paphotels   \n",
       "4        medpalace         Mediterranean Palace   \n",
       "\n",
       "                                         description  statuses_count  \\\n",
       "0  Luxury Suites, hotels and villas Santorini com...             513   \n",
       "1  Anthi Maria Beach Apartments is a self-caterin...             102   \n",
       "2  WEndow Escape Resort & Villas | Tailor-made Ad...             350   \n",
       "3  The best of Greek hospitality! Follow us, visi...             975   \n",
       "4  A cozy 5 star hotel in the city center with an...             269   \n",
       "\n",
       "   friends_count  followers_count  \\\n",
       "0             41              127   \n",
       "1             25              110   \n",
       "2            344              103   \n",
       "3           1182              475   \n",
       "4            543              381   \n",
       "\n",
       "                                 recent_100_statuses  hotel  \n",
       "0   \"The tans will fade but the memories will las...      1  \n",
       "1   Our fantastic New and Improved abc online web...      1  \n",
       "2   https://t.co/DHuXrG8G6o For those who still d...      1  \n",
       "3   @AlbertBourla üíØüíØüíØüíØüíØ Happy Easter !!!üê£ @paphot...      1  \n",
       "4   https://t.co/WPCR6KSnw2 New era!\\nNew Brand! ...      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-validation-set.csv')\n",
    "validation_set = validation_set.replace(np.nan, '')\n",
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "loaded-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 650\n",
      "Rate limit reached. Sleeping for: 760\n",
      "Rate limit reached. Sleeping for: 792\n",
      "Rate limit reached. Sleeping for: 736\n"
     ]
    }
   ],
   "source": [
    "hotel_friends = calculate_hotel_friends(validation_set.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "focal-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set['friends_hotel_count_1000'] = hotel_friends\n",
    "validation_set.to_csv('../../../../datasets/Hotels/classification/hotels-validation-set-enhanced.csv', index=False)\n",
    "del validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-triangle",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "corrected-crowd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aldemar_resorts</td>\n",
       "      <td>Aldemar Resorts</td>\n",
       "      <td>Guest satisfaction is our top priority! *Luxur...</td>\n",
       "      <td>1832</td>\n",
       "      <td>1569</td>\n",
       "      <td>2229</td>\n",
       "      <td>Summer vacation is meant to make you feel ‚õ± r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AquaVistaHotels</td>\n",
       "      <td>Aqua Vista Hotels</td>\n",
       "      <td>A compilation of extraordinary hotels catering...</td>\n",
       "      <td>5924</td>\n",
       "      <td>1650</td>\n",
       "      <td>2116</td>\n",
       "      <td>Thank you Greek Travel Pages for highlighting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eurobank_Group</td>\n",
       "      <td>Eurobank</td>\n",
       "      <td>ŒöŒ±ŒªœâœÉŒÆœÅŒ∏Œ±œÑŒµ œÉœÑŒ∑ŒΩ ŒµœÄŒØœÉŒ∑ŒºŒ∑ œÉŒµŒªŒØŒ¥Œ± œÑŒ∑œÇ Eurobank œÉ...</td>\n",
       "      <td>3284</td>\n",
       "      <td>0</td>\n",
       "      <td>2691</td>\n",
       "      <td>Œó¬†Eurobank¬†ŒµŒΩŒ∑ŒºŒµœÅœéŒΩŒµŒπ œåœÑŒπ œÑŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ¨¬†œÑŒ∑œÇ,¬†Œ∫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white_suites</td>\n",
       "      <td>White¬†Suites Resort</td>\n",
       "      <td>White Suites Resort is a luxury beach hotel in...</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>18</td>\n",
       "      <td>Sea side holidays in Afytos, Halikidiki White...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KarenMillen</td>\n",
       "      <td>Karen Millen</td>\n",
       "      <td>Timeless, elevated ready-to-wear style for women.</td>\n",
       "      <td>10908</td>\n",
       "      <td>1409</td>\n",
       "      <td>35679</td>\n",
       "      <td>The future's bright.\\nhttps://t.co/XLpskBYi4u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                 name  \\\n",
       "0  aldemar_resorts      Aldemar Resorts   \n",
       "1  AquaVistaHotels    Aqua Vista Hotels   \n",
       "2   Eurobank_Group             Eurobank   \n",
       "3     white_suites  White¬†Suites Resort   \n",
       "4      KarenMillen         Karen Millen   \n",
       "\n",
       "                                         description  statuses_count  \\\n",
       "0  Guest satisfaction is our top priority! *Luxur...            1832   \n",
       "1  A compilation of extraordinary hotels catering...            5924   \n",
       "2  ŒöŒ±ŒªœâœÉŒÆœÅŒ∏Œ±œÑŒµ œÉœÑŒ∑ŒΩ ŒµœÄŒØœÉŒ∑ŒºŒ∑ œÉŒµŒªŒØŒ¥Œ± œÑŒ∑œÇ Eurobank œÉ...            3284   \n",
       "3  White Suites Resort is a luxury beach hotel in...               2   \n",
       "4  Timeless, elevated ready-to-wear style for women.           10908   \n",
       "\n",
       "   friends_count  followers_count  \\\n",
       "0           1569             2229   \n",
       "1           1650             2116   \n",
       "2              0             2691   \n",
       "3             93               18   \n",
       "4           1409            35679   \n",
       "\n",
       "                                 recent_100_statuses  hotel  \n",
       "0   Summer vacation is meant to make you feel ‚õ± r...      1  \n",
       "1   Thank you Greek Travel Pages for highlighting...      1  \n",
       "2   Œó¬†Eurobank¬†ŒµŒΩŒ∑ŒºŒµœÅœéŒΩŒµŒπ œåœÑŒπ œÑŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ¨¬†œÑŒ∑œÇ,¬†Œ∫...      0  \n",
       "3   Sea side holidays in Afytos, Halikidiki White...      1  \n",
       "4   The future's bright.\\nhttps://t.co/XLpskBYi4u...      0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-training-set.csv')\n",
    "training_set = training_set.replace(np.nan, '')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "located-moment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 457\n",
      "Rate limit reached. Sleeping for: 604\n",
      "Rate limit reached. Sleeping for: 674\n",
      "Rate limit reached. Sleeping for: 730\n",
      "Rate limit reached. Sleeping for: 645\n",
      "Rate limit reached. Sleeping for: 759\n",
      "Rate limit reached. Sleeping for: 693\n",
      "Rate limit reached. Sleeping for: 756\n",
      "Rate limit reached. Sleeping for: 795\n",
      "Rate limit reached. Sleeping for: 743\n",
      "Rate limit reached. Sleeping for: 733\n",
      "Rate limit reached. Sleeping for: 711\n",
      "Rate limit reached. Sleeping for: 673\n"
     ]
    }
   ],
   "source": [
    "hotel_friends = calculate_hotel_friends(training_set.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "great-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['friends_hotel_count_1000'] = hotel_friends\n",
    "training_set.to_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv', index=False)\n",
    "del training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-spectrum",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sexual-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "      <th>friends_hotel_count_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>AlexanderHaus</td>\n",
       "      <td>Alexander Haus</td>\n",
       "      <td>#Studio #Rooms to Let, in #Halkidiki, #Sithoni...</td>\n",
       "      <td>492</td>\n",
       "      <td>232</td>\n",
       "      <td>277</td>\n",
       "      <td>Though is winter, summer is coming! https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>artsoundgr</td>\n",
       "      <td>ArtSound &amp; Lights</td>\n",
       "      <td>Art Sound &amp; Lights Professional Audio/Video Se...</td>\n",
       "      <td>443</td>\n",
       "      <td>497</td>\n",
       "      <td>191</td>\n",
       "      <td>Œ†œÅŒøœÉœÜŒøœÅŒ¨ STROBE 1500W DMX ARTLIGHT ST1500W Œºœå...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>JOHNMARYRHODES</td>\n",
       "      <td>JOHNMARY FALIRAKI</td>\n",
       "      <td>John Mary is a famly hotel and is located at F...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>http://t.co/DNz6I3s0Sh http://t.co/9eqsMrL4MK</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>THEMETHOTEL</td>\n",
       "      <td>THE MET HOTEL</td>\n",
       "      <td>https://t.co/fi814NlnxK\\r\\nhttp://t.co/AlYUMI5...</td>\n",
       "      <td>2816</td>\n",
       "      <td>181</td>\n",
       "      <td>1136</td>\n",
       "      <td>Let the LOVE sparkle at The MET Hotel!!\\n\\n#T...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>EvitaResort</td>\n",
       "      <td>SunConnect Evita</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>Zumba time @evitaresort @evitaresort #sunconn...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        screen_name               name  \\\n",
       "195   AlexanderHaus     Alexander Haus   \n",
       "196      artsoundgr  ArtSound & Lights   \n",
       "197  JOHNMARYRHODES  JOHNMARY FALIRAKI   \n",
       "198     THEMETHOTEL      THE MET HOTEL   \n",
       "199     EvitaResort   SunConnect Evita   \n",
       "\n",
       "                                           description  statuses_count  \\\n",
       "195  #Studio #Rooms to Let, in #Halkidiki, #Sithoni...             492   \n",
       "196  Art Sound & Lights Professional Audio/Video Se...             443   \n",
       "197  John Mary is a famly hotel and is located at F...               2   \n",
       "198  https://t.co/fi814NlnxK\\r\\nhttp://t.co/AlYUMI5...            2816   \n",
       "199                                                                 25   \n",
       "\n",
       "     friends_count  followers_count  \\\n",
       "195            232              277   \n",
       "196            497              191   \n",
       "197             15                7   \n",
       "198            181             1136   \n",
       "199             24               37   \n",
       "\n",
       "                                   recent_100_statuses  hotel  \\\n",
       "195   Though is winter, summer is coming! https://t...      1   \n",
       "196   Œ†œÅŒøœÉœÜŒøœÅŒ¨ STROBE 1500W DMX ARTLIGHT ST1500W Œºœå...      0   \n",
       "197      http://t.co/DNz6I3s0Sh http://t.co/9eqsMrL4MK      1   \n",
       "198   Let the LOVE sparkle at The MET Hotel!!\\n\\n#T...      1   \n",
       "199   Zumba time @evitaresort @evitaresort #sunconn...      1   \n",
       "\n",
       "     friends_hotel_count_1000  \n",
       "195                        14  \n",
       "196                        13  \n",
       "197                         2  \n",
       "198                         9  \n",
       "199                         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Training Set\n",
    "training_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv')\n",
    "training_set = training_set.replace(np.nan, '')\n",
    "training_set.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-mountain",
   "metadata": {},
   "source": [
    "# Only Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "transsexual-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advisory-genome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      " Best Params: {'svm__C': 0.1, 'svm__kernel': 'sigmoid'}.\n",
      " Score: 0.6799999999999999\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan   nan 0.62  0.62  0.59  0.635 0.57  0.585 0.585 0.61  0.635 0.62\n",
      " 0.635 0.62  0.64  0.62  0.655 0.63  0.62  0.62  0.615 0.63  0.62  0.625\n",
      " 0.61  0.625 0.625 0.625 0.625 0.635 0.615 0.625 0.63  0.625 0.635 0.625\n",
      " 0.63  0.625 0.655 0.625]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'knn__n_neighbors': 8, 'knn__weights': 'uniform'}.\n",
      " Score: 0.655\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      " Best Params: {'lr__C': 0.1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [ nan 0.65  nan  nan 0.65  nan  nan 0.65  nan  nan 0.65  nan  nan 0.65\n",
      "  nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to select the data\n",
    "def get_data_hotel_friends_(df):\n",
    "    data = df['friends_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_hotel_friends_)\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'knn__n_neighbors': [i for i in range(20)],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-pierce",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "pediatric-judge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 FunctionTransformer(func=<function get_data_hotel_fr at 0x7fb269cd8d30>)),\n",
       "                ('svm', SVC(C=0.1, kernel='sigmoid'))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel\n",
    "\n",
    "# Function to select the data\n",
    "def get_data_hotel_fr(df):\n",
    "    data = df['friends_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_hotel_fr)\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC(C=0.1, kernel='sigmoid'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dental-battlefield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_hotel_fr.sav']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'classifier_hotel_fr.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-organization",
   "metadata": {},
   "source": [
    "# Name Description Tweets and Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-entrance",
   "metadata": {},
   "source": [
    "## Without NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rural-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quality-orange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.77  0.565 0.645 0.665 0.875 0.56  0.65  0.565 0.88  0.56  0.655 0.535\n",
      " 0.825 0.56  0.645 0.535 0.8   0.56  0.64  0.535 0.83  0.565 0.645 0.665\n",
      " 0.895 0.56  0.65  0.565 0.88  0.56  0.66  0.535 0.855 0.565 0.645 0.535\n",
      " 0.82  0.565 0.64  0.535 0.81  0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.885 0.56  0.66  0.535 0.845 0.56  0.645 0.535 0.815 0.565 0.64  0.535\n",
      " 0.725 0.565 0.645 0.665 0.87  0.56  0.65  0.565 0.88  0.56  0.655 0.535\n",
      " 0.835 0.56  0.645 0.535 0.79  0.56  0.64  0.535 0.81  0.565 0.645 0.665\n",
      " 0.88  0.56  0.65  0.565 0.88  0.56  0.66  0.535 0.845 0.565 0.645 0.535\n",
      " 0.825 0.56  0.64  0.535 0.81  0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.885 0.56  0.66  0.535 0.845 0.56  0.645 0.535 0.815 0.565 0.64  0.535\n",
      " 0.69  0.565 0.645 0.665 0.88  0.56  0.65  0.565 0.885 0.56  0.655 0.535\n",
      " 0.86  0.555 0.645 0.535 0.86  0.555 0.635 0.535 0.805 0.565 0.645 0.665\n",
      " 0.885 0.56  0.65  0.565 0.88  0.56  0.66  0.535 0.84  0.565 0.645 0.535\n",
      " 0.825 0.56  0.64  0.535 0.81  0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.885 0.56  0.66  0.535 0.845 0.56  0.645 0.535 0.815 0.565 0.64  0.535\n",
      " 0.78  0.565 0.645 0.665 0.86  0.56  0.65  0.565 0.875 0.56  0.66  0.535\n",
      " 0.84  0.57  0.645 0.535 0.78  0.57  0.64  0.535 0.8   0.565 0.645 0.665\n",
      " 0.875 0.56  0.65  0.565 0.87  0.56  0.66  0.535 0.86  0.57  0.645 0.535\n",
      " 0.8   0.575 0.64  0.535 0.795 0.565 0.645 0.665 0.865 0.56  0.65  0.565\n",
      " 0.875 0.56  0.66  0.535 0.865 0.565 0.645 0.535 0.805 0.575 0.64  0.535\n",
      " 0.76  0.565 0.645 0.665 0.85  0.56  0.65  0.565 0.86  0.56  0.655 0.535\n",
      " 0.82  0.57  0.645 0.535 0.775 0.565 0.64  0.535 0.805 0.565 0.645 0.665\n",
      " 0.86  0.56  0.65  0.565 0.865 0.56  0.66  0.535 0.845 0.57  0.645 0.535\n",
      " 0.81  0.575 0.64  0.535 0.795 0.565 0.645 0.665 0.865 0.56  0.65  0.565\n",
      " 0.875 0.56  0.66  0.535 0.865 0.565 0.645 0.535 0.805 0.575 0.64  0.535\n",
      " 0.71  0.565 0.645 0.665 0.85  0.56  0.65  0.565 0.865 0.56  0.655 0.535\n",
      " 0.855 0.56  0.645 0.535 0.86  0.565 0.64  0.535 0.805 0.565 0.645 0.665\n",
      " 0.86  0.56  0.65  0.565 0.87  0.56  0.66  0.535 0.845 0.57  0.645 0.535\n",
      " 0.8   0.575 0.64  0.535 0.795 0.565 0.645 0.665 0.865 0.56  0.65  0.565\n",
      " 0.875 0.56  0.66  0.535 0.865 0.565 0.645 0.535 0.805 0.575 0.64  0.535\n",
      " 0.65  0.565 0.645 0.665 0.665 0.56  0.65  0.58  0.705 0.56  0.655 0.57\n",
      " 0.675 0.555 0.645 0.565 0.665 0.555 0.64  0.555   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.65  0.565 0.645 0.665 0.665 0.56  0.65  0.58  0.69  0.56  0.655 0.57\n",
      " 0.675 0.555 0.645 0.565 0.645 0.555 0.64  0.555   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.64  0.565 0.645 0.665 0.665 0.56  0.65  0.555 0.7   0.56  0.655 0.535\n",
      " 0.705 0.555 0.645 0.535 0.705 0.555 0.64  0.535   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 0.5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.8949999999999999\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.635 0.635 0.58  0.635 0.645 0.64  0.67  0.65  0.685 0.685 0.675 0.67\n",
      " 0.7   0.705 0.685 0.69  0.675 0.7   0.685 0.69  0.66  0.66  0.595 0.66\n",
      " 0.66  0.66  0.665 0.67  0.69  0.695 0.68  0.68  0.705 0.71  0.7   0.7\n",
      " 0.675 0.7   0.68  0.7   0.67  0.67  0.61  0.67  0.675 0.67  0.675 0.69\n",
      " 0.685 0.69  0.665 0.675 0.695 0.705 0.7   0.695 0.68  0.7   0.67  0.7\n",
      " 0.62  0.62  0.555 0.62  0.615 0.615 0.66  0.62  0.675 0.685 0.67  0.67\n",
      " 0.695 0.705 0.69  0.7   0.675 0.705 0.675 0.695 0.65  0.65  0.605 0.665\n",
      " 0.665 0.665 0.665 0.665 0.685 0.69  0.685 0.685 0.705 0.71  0.695 0.705\n",
      " 0.68  0.705 0.685 0.7   0.67  0.67  0.61  0.67  0.675 0.67  0.675 0.69\n",
      " 0.685 0.69  0.665 0.675 0.695 0.705 0.7   0.695 0.68  0.7   0.67  0.7\n",
      " 0.715 0.715 0.6   0.715 0.675 0.67  0.665 0.665 0.685 0.685 0.665 0.665\n",
      " 0.7   0.7   0.675 0.665 0.67  0.675 0.67  0.675 0.65  0.65  0.605 0.665\n",
      " 0.66  0.66  0.67  0.665 0.685 0.69  0.685 0.685 0.705 0.71  0.695 0.705\n",
      " 0.68  0.705 0.685 0.7   0.67  0.67  0.61  0.67  0.675 0.67  0.675 0.69\n",
      " 0.685 0.69  0.665 0.675 0.695 0.705 0.7   0.695 0.68  0.7   0.67  0.7\n",
      " 0.645 0.645 0.565 0.645 0.615 0.63  0.675 0.645 0.685 0.695 0.68  0.685\n",
      " 0.705 0.72  0.695 0.71  0.68  0.71  0.69  0.705 0.655 0.655 0.595 0.655\n",
      " 0.655 0.66  0.675 0.66  0.69  0.7   0.7   0.7   0.725 0.74  0.705 0.7\n",
      " 0.685 0.715 0.695 0.705 0.665 0.665 0.605 0.665 0.66  0.67  0.68  0.675\n",
      " 0.685 0.7   0.69  0.69  0.725 0.745 0.695 0.715 0.68  0.71  0.675 0.705\n",
      " 0.6   0.6   0.545 0.6   0.62  0.63  0.68  0.615 0.69  0.7   0.69  0.685\n",
      " 0.705 0.72  0.705 0.715 0.69  0.715 0.695 0.705 0.64  0.64  0.595 0.64\n",
      " 0.66  0.665 0.675 0.675 0.7   0.71  0.7   0.705 0.725 0.74  0.7   0.72\n",
      " 0.69  0.71  0.695 0.705 0.665 0.665 0.605 0.665 0.66  0.67  0.68  0.675\n",
      " 0.685 0.7   0.69  0.69  0.725 0.745 0.695 0.715 0.68  0.71  0.675 0.705\n",
      " 0.695 0.695 0.59  0.695 0.685 0.68  0.66  0.69  0.69  0.685 0.67  0.68\n",
      " 0.71  0.71  0.685 0.695 0.665 0.67  0.67  0.67  0.64  0.64  0.6   0.64\n",
      " 0.66  0.665 0.68  0.675 0.705 0.71  0.7   0.7   0.73  0.74  0.7   0.72\n",
      " 0.69  0.71  0.69  0.705 0.665 0.665 0.605 0.665 0.66  0.67  0.68  0.675\n",
      " 0.685 0.7   0.69  0.69  0.725 0.745 0.695 0.715 0.68  0.71  0.675 0.705\n",
      " 0.575 0.575 0.54  0.575 0.505 0.505 0.58  0.545 0.57  0.58  0.62  0.595\n",
      " 0.575 0.575 0.615 0.585 0.575 0.59  0.59  0.57    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.53  0.53  0.505 0.53  0.54  0.535 0.6   0.565 0.59  0.595 0.615 0.575\n",
      " 0.625 0.64  0.625 0.64  0.63  0.65  0.615 0.635   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.625 0.625 0.58  0.625 0.625 0.62  0.61  0.63  0.645 0.645 0.64  0.645\n",
      " 0.66  0.665 0.66  0.65  0.64  0.645 0.65  0.65    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 10, 'knn__n_neighbors': 7, 'knn__weights': 'distance'}.\n",
      " Score: 0.7449999999999999\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.745   nan 0.845   nan 0.845   nan 0.845   nan 0.845   nan 0.845\n",
      "   nan 0.845   nan 0.845   nan 0.865   nan 0.845   nan 0.76    nan 0.86\n",
      "   nan 0.84    nan 0.86    nan 0.86    nan 0.86    nan 0.865   nan 0.86\n",
      "   nan 0.875   nan 0.86    nan 0.77    nan 0.845   nan 0.835   nan 0.845\n",
      "   nan 0.855   nan 0.845   nan 0.86    nan 0.845   nan 0.865   nan 0.845\n",
      "   nan 0.74    nan 0.83    nan 0.835   nan 0.83    nan 0.835   nan 0.83\n",
      "   nan 0.85    nan 0.83    nan 0.855   nan 0.83    nan 0.765   nan 0.86\n",
      "   nan 0.835   nan 0.86    nan 0.86    nan 0.86    nan 0.86    nan 0.86\n",
      "   nan 0.865   nan 0.86    nan 0.77    nan 0.845   nan 0.835   nan 0.845\n",
      "   nan 0.855   nan 0.845   nan 0.86    nan 0.845   nan 0.865   nan 0.845\n",
      "   nan 0.72    nan 0.825   nan 0.825   nan 0.825   nan 0.865   nan 0.825\n",
      "   nan 0.865   nan 0.825   nan 0.87    nan 0.825   nan 0.765   nan 0.85\n",
      "   nan 0.835   nan 0.85    nan 0.86    nan 0.85    nan 0.86    nan 0.85\n",
      "   nan 0.865   nan 0.85    nan 0.77    nan 0.845   nan 0.835   nan 0.845\n",
      "   nan 0.855   nan 0.845   nan 0.86    nan 0.845   nan 0.865   nan 0.845\n",
      "   nan 0.755   nan 0.82    nan 0.82    nan 0.82    nan 0.835   nan 0.82\n",
      "   nan 0.85    nan 0.82    nan 0.855   nan 0.82    nan 0.765   nan 0.835\n",
      "   nan 0.815   nan 0.835   nan 0.84    nan 0.835   nan 0.855   nan 0.835\n",
      "   nan 0.86    nan 0.835   nan 0.765   nan 0.825   nan 0.825   nan 0.825\n",
      "   nan 0.83    nan 0.825   nan 0.84    nan 0.825   nan 0.855   nan 0.825\n",
      "   nan 0.74    nan 0.805   nan 0.82    nan 0.805   nan 0.815   nan 0.805\n",
      "   nan 0.85    nan 0.805   nan 0.855   nan 0.805   nan 0.765   nan 0.84\n",
      "   nan 0.815   nan 0.84    nan 0.835   nan 0.84    nan 0.855   nan 0.84\n",
      "   nan 0.865   nan 0.84    nan 0.765   nan 0.825   nan 0.825   nan 0.825\n",
      "   nan 0.83    nan 0.825   nan 0.84    nan 0.825   nan 0.855   nan 0.825\n",
      "   nan 0.725   nan 0.81    nan 0.8     nan 0.81    nan 0.825   nan 0.81\n",
      "   nan 0.86    nan 0.81    nan 0.87    nan 0.81    nan 0.765   nan 0.845\n",
      "   nan 0.815   nan 0.845   nan 0.835   nan 0.845   nan 0.855   nan 0.845\n",
      "   nan 0.865   nan 0.845   nan 0.765   nan 0.825   nan 0.825   nan 0.825\n",
      "   nan 0.83    nan 0.825   nan 0.84    nan 0.825   nan 0.855   nan 0.825\n",
      "   nan 0.655   nan 0.64    nan 0.65    nan 0.64    nan 0.67    nan 0.64\n",
      "   nan 0.7     nan 0.64    nan 0.71    nan 0.64    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.65    nan 0.72    nan 0.65    nan 0.72    nan 0.665   nan 0.72\n",
      "   nan 0.7     nan 0.72    nan 0.69    nan 0.72    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.65    nan 0.68    nan 0.65    nan 0.68    nan 0.65    nan 0.68\n",
      "   nan 0.685   nan 0.68    nan 0.69    nan 0.68    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.875\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['friends_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-stable",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liable-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "received-transcription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.71  0.565 0.645 0.665 0.885 0.56  0.65  0.565 0.895 0.56  0.655 0.57\n",
      " 0.86  0.56  0.645 0.565 0.86  0.56  0.64  0.555 0.745 0.565 0.645 0.665\n",
      " 0.89  0.56  0.65  0.565 0.89  0.56  0.655 0.535 0.875 0.565 0.645 0.535\n",
      " 0.83  0.56  0.645 0.535 0.785 0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.875 0.56  0.655 0.535 0.85  0.56  0.645 0.535 0.84  0.56  0.645 0.535\n",
      " 0.705 0.565 0.645 0.665 0.875 0.56  0.65  0.565 0.875 0.56  0.655 0.535\n",
      " 0.865 0.56  0.645 0.535 0.865 0.56  0.64  0.535 0.735 0.565 0.645 0.665\n",
      " 0.89  0.56  0.65  0.565 0.885 0.56  0.655 0.535 0.88  0.56  0.645 0.535\n",
      " 0.85  0.56  0.645 0.535 0.785 0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.875 0.56  0.655 0.535 0.85  0.56  0.645 0.535 0.84  0.56  0.645 0.535\n",
      " 0.68  0.565 0.645 0.665 0.865 0.56  0.65  0.575 0.87  0.56  0.655 0.57\n",
      " 0.865 0.555 0.645 0.565 0.865 0.555 0.64  0.555 0.735 0.565 0.645 0.665\n",
      " 0.89  0.56  0.65  0.565 0.885 0.56  0.655 0.535 0.88  0.56  0.645 0.535\n",
      " 0.85  0.56  0.645 0.535 0.785 0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.875 0.56  0.655 0.535 0.85  0.56  0.645 0.535 0.84  0.56  0.645 0.535\n",
      " 0.71  0.565 0.645 0.665 0.875 0.56  0.65  0.565 0.89  0.56  0.655 0.57\n",
      " 0.85  0.56  0.645 0.565 0.845 0.56  0.64  0.555 0.74  0.565 0.645 0.665\n",
      " 0.89  0.56  0.65  0.565 0.89  0.56  0.655 0.535 0.855 0.565 0.645 0.535\n",
      " 0.835 0.56  0.645 0.535 0.785 0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.87  0.56  0.655 0.535 0.845 0.56  0.645 0.535 0.825 0.56  0.645 0.535\n",
      " 0.705 0.565 0.645 0.665 0.865 0.56  0.65  0.565 0.875 0.56  0.655 0.535\n",
      " 0.855 0.56  0.645 0.535 0.85  0.56  0.64  0.535 0.735 0.565 0.645 0.665\n",
      " 0.89  0.56  0.65  0.565 0.89  0.56  0.655 0.535 0.87  0.56  0.645 0.535\n",
      " 0.85  0.56  0.645 0.535 0.785 0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.87  0.56  0.655 0.535 0.845 0.56  0.645 0.535 0.825 0.56  0.645 0.535\n",
      " 0.68  0.565 0.645 0.665 0.865 0.56  0.65  0.56  0.87  0.56  0.655 0.57\n",
      " 0.855 0.555 0.645 0.565 0.855 0.555 0.64  0.555 0.735 0.565 0.645 0.665\n",
      " 0.89  0.56  0.65  0.565 0.89  0.56  0.655 0.535 0.87  0.56  0.645 0.535\n",
      " 0.85  0.56  0.645 0.535 0.785 0.565 0.645 0.665 0.885 0.56  0.65  0.565\n",
      " 0.87  0.56  0.655 0.535 0.845 0.56  0.645 0.535 0.825 0.56  0.645 0.535\n",
      " 0.65  0.565 0.645 0.665 0.665 0.56  0.65  0.58  0.68  0.56  0.655 0.57\n",
      " 0.65  0.555 0.645 0.565 0.645 0.555 0.64  0.555   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.65  0.565 0.645 0.665 0.665 0.56  0.65  0.58  0.69  0.56  0.655 0.57\n",
      " 0.675 0.555 0.645 0.565 0.66  0.555 0.64  0.555   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.64  0.565 0.645 0.665 0.665 0.56  0.65  0.555 0.7   0.56  0.655 0.535\n",
      " 0.705 0.555 0.645 0.535 0.705 0.555 0.64  0.535   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.8949999999999999\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.58  0.58  0.545 0.6   0.595 0.595 0.68  0.61  0.68  0.685 0.68  0.655\n",
      " 0.705 0.715 0.695 0.69  0.675 0.685 0.69  0.69  0.58  0.58  0.555 0.6\n",
      " 0.605 0.6   0.67  0.62  0.685 0.685 0.68  0.655 0.71  0.72  0.705 0.7\n",
      " 0.685 0.69  0.69  0.69  0.585 0.585 0.545 0.605 0.62  0.615 0.665 0.615\n",
      " 0.68  0.68  0.665 0.655 0.715 0.715 0.705 0.695 0.69  0.695 0.685 0.695\n",
      " 0.575 0.575 0.55  0.595 0.59  0.59  0.665 0.62  0.675 0.685 0.68  0.66\n",
      " 0.72  0.735 0.7   0.695 0.68  0.69  0.7   0.69  0.615 0.615 0.565 0.635\n",
      " 0.625 0.625 0.675 0.635 0.68  0.68  0.68  0.67  0.72  0.725 0.705 0.695\n",
      " 0.69  0.695 0.695 0.7   0.585 0.585 0.545 0.605 0.62  0.615 0.665 0.615\n",
      " 0.68  0.68  0.665 0.655 0.715 0.715 0.705 0.695 0.69  0.695 0.685 0.695\n",
      " 0.71  0.71  0.595 0.705 0.665 0.66  0.675 0.675 0.705 0.705 0.695 0.68\n",
      " 0.72  0.72  0.7   0.68  0.685 0.685 0.69  0.7   0.615 0.615 0.565 0.635\n",
      " 0.625 0.625 0.675 0.635 0.68  0.68  0.68  0.67  0.72  0.725 0.705 0.695\n",
      " 0.69  0.695 0.695 0.7   0.585 0.585 0.545 0.605 0.62  0.615 0.665 0.615\n",
      " 0.68  0.68  0.665 0.655 0.715 0.715 0.705 0.695 0.69  0.695 0.685 0.695\n",
      " 0.575 0.575 0.535 0.575 0.605 0.615 0.675 0.63  0.67  0.69  0.67  0.665\n",
      " 0.705 0.73  0.7   0.705 0.685 0.705 0.7   0.705 0.585 0.585 0.55  0.585\n",
      " 0.61  0.62  0.68  0.63  0.675 0.69  0.685 0.67  0.705 0.725 0.705 0.71\n",
      " 0.69  0.7   0.69  0.71  0.58  0.58  0.545 0.58  0.62  0.625 0.67  0.63\n",
      " 0.675 0.685 0.685 0.675 0.715 0.725 0.715 0.705 0.7   0.71  0.685 0.71\n",
      " 0.57  0.57  0.545 0.57  0.595 0.605 0.67  0.625 0.675 0.69  0.675 0.65\n",
      " 0.69  0.72  0.7   0.695 0.68  0.7   0.685 0.705 0.615 0.615 0.57  0.615\n",
      " 0.625 0.625 0.67  0.64  0.67  0.68  0.69  0.675 0.705 0.725 0.705 0.695\n",
      " 0.685 0.695 0.685 0.705 0.58  0.58  0.545 0.58  0.62  0.625 0.67  0.63\n",
      " 0.675 0.685 0.685 0.675 0.715 0.725 0.715 0.705 0.7   0.71  0.685 0.71\n",
      " 0.715 0.715 0.595 0.715 0.66  0.655 0.68  0.675 0.705 0.705 0.69  0.68\n",
      " 0.715 0.72  0.7   0.685 0.685 0.685 0.69  0.7   0.615 0.615 0.57  0.615\n",
      " 0.625 0.625 0.67  0.64  0.67  0.68  0.69  0.675 0.705 0.725 0.705 0.695\n",
      " 0.685 0.695 0.685 0.705 0.58  0.58  0.545 0.58  0.62  0.625 0.67  0.63\n",
      " 0.675 0.685 0.685 0.675 0.715 0.725 0.715 0.705 0.7   0.71  0.685 0.71\n",
      " 0.57  0.57  0.545 0.585 0.515 0.515 0.58  0.555 0.56  0.575 0.58  0.555\n",
      " 0.59  0.585 0.605 0.59  0.55  0.56  0.575 0.555   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.545 0.545 0.525 0.555 0.54  0.54  0.585 0.585 0.56  0.57  0.615 0.59\n",
      " 0.595 0.615 0.63  0.64  0.61  0.64  0.6   0.635   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.62  0.62  0.57  0.62  0.625 0.62  0.62  0.625 0.625 0.625 0.65  0.625\n",
      " 0.67  0.68  0.665 0.65  0.635 0.64  0.65  0.645   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 2000, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 7, 'knn__weights': 'distance'}.\n",
      " Score: 0.735\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.715   nan 0.88    nan 0.84    nan 0.88    nan 0.87    nan 0.88\n",
      "   nan 0.89    nan 0.88    nan 0.88    nan 0.88    nan 0.73    nan 0.89\n",
      "   nan 0.855   nan 0.89    nan 0.895   nan 0.89    nan 0.89    nan 0.89\n",
      "   nan 0.89    nan 0.89    nan 0.76    nan 0.88    nan 0.86    nan 0.88\n",
      "   nan 0.885   nan 0.88    nan 0.89    nan 0.88    nan 0.88    nan 0.88\n",
      "   nan 0.71    nan 0.845   nan 0.83    nan 0.845   nan 0.855   nan 0.845\n",
      "   nan 0.865   nan 0.845   nan 0.865   nan 0.845   nan 0.725   nan 0.88\n",
      "   nan 0.835   nan 0.88    nan 0.88    nan 0.88    nan 0.885   nan 0.88\n",
      "   nan 0.89    nan 0.88    nan 0.76    nan 0.88    nan 0.86    nan 0.88\n",
      "   nan 0.885   nan 0.88    nan 0.89    nan 0.88    nan 0.88    nan 0.88\n",
      "   nan 0.69    nan 0.855   nan 0.815   nan 0.855   nan 0.835   nan 0.855\n",
      "   nan 0.87    nan 0.855   nan 0.87    nan 0.855   nan 0.725   nan 0.88\n",
      "   nan 0.835   nan 0.88    nan 0.88    nan 0.88    nan 0.885   nan 0.88\n",
      "   nan 0.89    nan 0.88    nan 0.76    nan 0.88    nan 0.86    nan 0.88\n",
      "   nan 0.885   nan 0.88    nan 0.89    nan 0.88    nan 0.88    nan 0.88\n",
      "   nan 0.72    nan 0.825   nan 0.85    nan 0.825   nan 0.86    nan 0.825\n",
      "   nan 0.88    nan 0.825   nan 0.875   nan 0.825   nan 0.73    nan 0.86\n",
      "   nan 0.855   nan 0.86    nan 0.885   nan 0.86    nan 0.89    nan 0.86\n",
      "   nan 0.885   nan 0.86    nan 0.755   nan 0.875   nan 0.86    nan 0.875\n",
      "   nan 0.88    nan 0.875   nan 0.885   nan 0.875   nan 0.875   nan 0.875\n",
      "   nan 0.71    nan 0.815   nan 0.83    nan 0.815   nan 0.845   nan 0.815\n",
      "   nan 0.865   nan 0.815   nan 0.87    nan 0.815   nan 0.725   nan 0.86\n",
      "   nan 0.835   nan 0.86    nan 0.875   nan 0.86    nan 0.885   nan 0.86\n",
      "   nan 0.885   nan 0.86    nan 0.755   nan 0.875   nan 0.86    nan 0.875\n",
      "   nan 0.88    nan 0.875   nan 0.885   nan 0.875   nan 0.875   nan 0.875\n",
      "   nan 0.69    nan 0.855   nan 0.81    nan 0.855   nan 0.84    nan 0.855\n",
      "   nan 0.87    nan 0.855   nan 0.87    nan 0.855   nan 0.725   nan 0.86\n",
      "   nan 0.835   nan 0.86    nan 0.875   nan 0.86    nan 0.885   nan 0.86\n",
      "   nan 0.885   nan 0.86    nan 0.755   nan 0.875   nan 0.86    nan 0.875\n",
      "   nan 0.88    nan 0.875   nan 0.885   nan 0.875   nan 0.875   nan 0.875\n",
      "   nan 0.65    nan 0.61    nan 0.65    nan 0.61    nan 0.675   nan 0.61\n",
      "   nan 0.68    nan 0.61    nan 0.68    nan 0.61    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.66    nan 0.72    nan 0.655   nan 0.72    nan 0.675   nan 0.72\n",
      "   nan 0.7     nan 0.72    nan 0.685   nan 0.72    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.655   nan 0.68    nan 0.65    nan 0.68    nan 0.65    nan 0.68\n",
      "   nan 0.685   nan 0.68    nan 0.685   nan 0.68    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.8950000000000001\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['friends_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-irrigation",
   "metadata": {},
   "source": [
    "## Lemmatization and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scheduled-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "advance-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.725 0.565 0.645 0.665 0.89  0.56  0.65  0.565 0.895 0.56  0.655 0.535\n",
      " 0.86  0.56  0.645 0.535 0.86  0.56  0.64  0.535 0.755 0.565 0.645 0.665\n",
      " 0.885 0.56  0.65  0.565 0.885 0.56  0.655 0.535 0.87  0.555 0.645 0.535\n",
      " 0.855 0.56  0.645 0.535 0.8   0.565 0.645 0.665 0.89  0.56  0.65  0.565\n",
      " 0.885 0.56  0.655 0.535 0.88  0.56  0.645 0.535 0.855 0.56  0.645 0.535\n",
      " 0.71  0.565 0.645 0.665 0.885 0.56  0.65  0.565 0.865 0.56  0.655 0.535\n",
      " 0.865 0.56  0.645 0.535 0.87  0.56  0.64  0.535 0.74  0.565 0.645 0.665\n",
      " 0.88  0.56  0.65  0.565 0.895 0.56  0.655 0.535 0.87  0.555 0.645 0.535\n",
      " 0.855 0.56  0.645 0.535 0.8   0.565 0.645 0.665 0.89  0.56  0.65  0.565\n",
      " 0.885 0.56  0.655 0.535 0.88  0.56  0.645 0.535 0.855 0.56  0.645 0.535\n",
      " 0.685 0.565 0.645 0.665 0.865 0.56  0.65  0.56  0.88  0.56  0.655 0.535\n",
      " 0.88  0.555 0.645 0.535 0.885 0.555 0.64  0.535 0.74  0.565 0.645 0.665\n",
      " 0.88  0.56  0.65  0.565 0.895 0.56  0.655 0.535 0.87  0.555 0.645 0.535\n",
      " 0.855 0.56  0.645 0.535 0.8   0.565 0.645 0.665 0.89  0.56  0.65  0.565\n",
      " 0.885 0.56  0.655 0.535 0.88  0.56  0.645 0.535 0.855 0.56  0.645 0.535\n",
      " 0.73  0.565 0.645 0.665 0.885 0.56  0.65  0.565 0.89  0.56  0.655 0.535\n",
      " 0.855 0.56  0.645 0.535 0.87  0.56  0.64  0.535 0.755 0.565 0.645 0.665\n",
      " 0.89  0.56  0.65  0.565 0.88  0.56  0.655 0.535 0.865 0.555 0.645 0.535\n",
      " 0.86  0.56  0.645 0.535 0.8   0.565 0.645 0.665 0.89  0.56  0.65  0.565\n",
      " 0.88  0.56  0.655 0.535 0.855 0.56  0.645 0.535 0.835 0.56  0.645 0.535\n",
      " 0.71  0.565 0.645 0.665 0.875 0.56  0.65  0.565 0.865 0.56  0.655 0.535\n",
      " 0.87  0.56  0.645 0.535 0.9   0.56  0.64  0.535 0.74  0.565 0.645 0.665\n",
      " 0.885 0.56  0.65  0.565 0.885 0.56  0.655 0.535 0.875 0.555 0.645 0.535\n",
      " 0.865 0.56  0.645 0.535 0.8   0.565 0.645 0.665 0.89  0.56  0.65  0.565\n",
      " 0.88  0.56  0.655 0.535 0.855 0.56  0.645 0.535 0.835 0.56  0.645 0.535\n",
      " 0.695 0.565 0.645 0.665 0.865 0.56  0.65  0.56  0.875 0.56  0.655 0.535\n",
      " 0.88  0.555 0.645 0.535 0.89  0.555 0.64  0.535 0.74  0.565 0.645 0.665\n",
      " 0.885 0.56  0.65  0.565 0.885 0.56  0.655 0.535 0.875 0.555 0.645 0.535\n",
      " 0.865 0.56  0.645 0.535 0.8   0.565 0.645 0.665 0.89  0.56  0.65  0.565\n",
      " 0.88  0.56  0.655 0.535 0.855 0.56  0.645 0.535 0.835 0.56  0.645 0.535\n",
      " 0.645 0.565 0.645 0.665 0.66  0.56  0.65  0.58  0.685 0.56  0.655 0.57\n",
      " 0.685 0.555 0.645 0.565 0.665 0.555 0.64  0.555   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.65  0.565 0.645 0.665 0.665 0.56  0.65  0.58  0.67  0.56  0.655 0.57\n",
      " 0.685 0.555 0.645 0.565 0.62  0.555 0.64  0.555   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.64  0.565 0.645 0.665 0.665 0.56  0.65  0.555 0.69  0.56  0.655 0.535\n",
      " 0.69  0.555 0.645 0.535 0.695 0.555 0.64  0.535   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 2000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 10, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.605 0.605 0.58  0.655 0.655 0.655 0.685 0.665 0.68  0.685 0.69  0.665\n",
      " 0.7   0.715 0.7   0.715 0.68  0.695 0.685 0.7   0.59  0.59  0.6   0.64\n",
      " 0.665 0.655 0.68  0.675 0.685 0.68  0.685 0.66  0.705 0.71  0.71  0.705\n",
      " 0.69  0.69  0.685 0.695 0.59  0.59  0.595 0.64  0.665 0.655 0.67  0.675\n",
      " 0.685 0.685 0.68  0.66  0.715 0.71  0.705 0.7   0.695 0.685 0.695 0.69\n",
      " 0.605 0.605 0.59  0.65  0.665 0.67  0.68  0.675 0.675 0.68  0.685 0.65\n",
      " 0.7   0.71  0.715 0.695 0.685 0.69  0.685 0.69  0.605 0.605 0.61  0.655\n",
      " 0.67  0.66  0.67  0.69  0.685 0.685 0.695 0.665 0.715 0.725 0.72  0.695\n",
      " 0.69  0.69  0.685 0.69  0.59  0.59  0.595 0.64  0.665 0.655 0.67  0.675\n",
      " 0.685 0.685 0.68  0.66  0.715 0.71  0.705 0.7   0.695 0.685 0.695 0.69\n",
      " 0.71  0.71  0.595 0.71  0.67  0.665 0.68  0.675 0.695 0.695 0.7   0.69\n",
      " 0.72  0.725 0.7   0.685 0.68  0.68  0.685 0.69  0.605 0.605 0.61  0.655\n",
      " 0.67  0.66  0.67  0.69  0.685 0.685 0.695 0.665 0.715 0.725 0.72  0.695\n",
      " 0.69  0.69  0.685 0.69  0.59  0.59  0.595 0.64  0.665 0.655 0.67  0.675\n",
      " 0.685 0.685 0.68  0.66  0.715 0.71  0.705 0.7   0.695 0.685 0.695 0.69\n",
      " 0.605 0.605 0.59  0.6   0.66  0.66  0.685 0.665 0.695 0.7   0.7   0.675\n",
      " 0.705 0.72  0.71  0.715 0.69  0.7   0.685 0.705 0.595 0.595 0.59  0.595\n",
      " 0.66  0.66  0.675 0.69  0.68  0.685 0.685 0.655 0.705 0.715 0.705 0.705\n",
      " 0.69  0.695 0.685 0.705 0.59  0.59  0.595 0.59  0.66  0.655 0.675 0.66\n",
      " 0.68  0.68  0.69  0.665 0.71  0.715 0.7   0.7   0.69  0.695 0.69  0.695\n",
      " 0.6   0.6   0.6   0.6   0.655 0.655 0.665 0.665 0.67  0.68  0.69  0.66\n",
      " 0.695 0.715 0.71  0.7   0.685 0.695 0.685 0.7   0.605 0.605 0.615 0.605\n",
      " 0.68  0.675 0.675 0.695 0.675 0.685 0.685 0.67  0.7   0.72  0.71  0.705\n",
      " 0.685 0.695 0.69  0.7   0.59  0.59  0.595 0.59  0.66  0.655 0.675 0.66\n",
      " 0.68  0.68  0.69  0.665 0.71  0.715 0.7   0.7   0.69  0.695 0.69  0.695\n",
      " 0.71  0.71  0.585 0.71  0.675 0.67  0.68  0.68  0.7   0.7   0.7   0.69\n",
      " 0.7   0.71  0.7   0.685 0.7   0.7   0.69  0.7   0.605 0.605 0.615 0.605\n",
      " 0.68  0.675 0.675 0.695 0.675 0.685 0.685 0.67  0.7   0.72  0.71  0.705\n",
      " 0.685 0.695 0.69  0.7   0.59  0.59  0.595 0.59  0.66  0.655 0.675 0.66\n",
      " 0.68  0.68  0.69  0.665 0.71  0.715 0.7   0.7   0.69  0.695 0.69  0.695\n",
      " 0.595 0.595 0.56  0.6   0.54  0.535 0.595 0.585 0.565 0.58  0.6   0.59\n",
      " 0.575 0.58  0.585 0.565 0.56  0.57  0.605 0.565   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.6   0.6   0.535 0.6   0.525 0.525 0.59  0.58  0.595 0.605 0.62  0.6\n",
      " 0.605 0.62  0.625 0.625 0.6   0.635 0.57  0.63    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.655 0.655 0.605 0.655 0.615 0.61  0.615 0.62  0.66  0.66  0.655 0.635\n",
      " 0.665 0.675 0.67  0.65  0.655 0.66  0.65  0.65    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 2000, 'features__text_features__vectorizer__min_df': 5, 'knn__n_neighbors': 7, 'knn__weights': 'distance'}.\n",
      " Score: 0.725\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.725   nan 0.875   nan 0.845   nan 0.875   nan 0.885   nan 0.875\n",
      "   nan 0.885   nan 0.875   nan 0.885   nan 0.875   nan 0.73    nan 0.87\n",
      "   nan 0.86    nan 0.87    nan 0.885   nan 0.87    nan 0.89    nan 0.87\n",
      "   nan 0.9     nan 0.87    nan 0.77    nan 0.845   nan 0.865   nan 0.845\n",
      "   nan 0.89    nan 0.845   nan 0.885   nan 0.845   nan 0.89    nan 0.845\n",
      "   nan 0.72    nan 0.84    nan 0.835   nan 0.84    nan 0.855   nan 0.84\n",
      "   nan 0.87    nan 0.84    nan 0.87    nan 0.84    nan 0.725   nan 0.875\n",
      "   nan 0.85    nan 0.875   nan 0.88    nan 0.875   nan 0.885   nan 0.875\n",
      "   nan 0.89    nan 0.875   nan 0.77    nan 0.845   nan 0.865   nan 0.845\n",
      "   nan 0.89    nan 0.845   nan 0.885   nan 0.845   nan 0.89    nan 0.845\n",
      "   nan 0.695   nan 0.86    nan 0.825   nan 0.86    nan 0.845   nan 0.86\n",
      "   nan 0.865   nan 0.86    nan 0.87    nan 0.86    nan 0.725   nan 0.875\n",
      "   nan 0.85    nan 0.875   nan 0.88    nan 0.875   nan 0.885   nan 0.875\n",
      "   nan 0.89    nan 0.875   nan 0.77    nan 0.845   nan 0.865   nan 0.845\n",
      "   nan 0.89    nan 0.845   nan 0.885   nan 0.845   nan 0.89    nan 0.845\n",
      "   nan 0.73    nan 0.82    nan 0.85    nan 0.82    nan 0.88    nan 0.82\n",
      "   nan 0.88    nan 0.82    nan 0.88    nan 0.82    nan 0.73    nan 0.84\n",
      "   nan 0.865   nan 0.84    nan 0.885   nan 0.84    nan 0.885   nan 0.84\n",
      "   nan 0.89    nan 0.84    nan 0.78    nan 0.85    nan 0.865   nan 0.85\n",
      "   nan 0.89    nan 0.85    nan 0.88    nan 0.85    nan 0.875   nan 0.85\n",
      "   nan 0.72    nan 0.825   nan 0.835   nan 0.825   nan 0.855   nan 0.825\n",
      "   nan 0.87    nan 0.825   nan 0.87    nan 0.825   nan 0.725   nan 0.865\n",
      "   nan 0.855   nan 0.865   nan 0.88    nan 0.865   nan 0.88    nan 0.865\n",
      "   nan 0.885   nan 0.865   nan 0.78    nan 0.85    nan 0.865   nan 0.85\n",
      "   nan 0.89    nan 0.85    nan 0.88    nan 0.85    nan 0.875   nan 0.85\n",
      "   nan 0.695   nan 0.865   nan 0.825   nan 0.865   nan 0.845   nan 0.865\n",
      "   nan 0.865   nan 0.865   nan 0.87    nan 0.865   nan 0.725   nan 0.865\n",
      "   nan 0.855   nan 0.865   nan 0.88    nan 0.865   nan 0.88    nan 0.865\n",
      "   nan 0.885   nan 0.865   nan 0.78    nan 0.85    nan 0.865   nan 0.85\n",
      "   nan 0.89    nan 0.85    nan 0.88    nan 0.85    nan 0.875   nan 0.85\n",
      "   nan 0.65    nan 0.655   nan 0.64    nan 0.655   nan 0.66    nan 0.655\n",
      "   nan 0.7     nan 0.655   nan 0.71    nan 0.655   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.645   nan 0.665   nan 0.64    nan 0.665   nan 0.65    nan 0.665\n",
      "   nan 0.68    nan 0.665   nan 0.68    nan 0.665   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.65    nan 0.67    nan 0.65    nan 0.67    nan 0.645   nan 0.67\n",
      "   nan 0.68    nan 0.67    nan 0.68    nan 0.67    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['friends_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-narrative",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "short-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "valid-holly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('numeric_features',\n",
       "                                                 Pipeline(steps=[('selector_num',\n",
       "                                                                  FunctionTransformer(func=<function get_hotel_numeric_data_ at 0x7ff070c15dc0>))])),\n",
       "                                                ('text_features',\n",
       "                                                 Pipeline(steps=[('selector_text',\n",
       "                                                                  FunctionTransformer(func=<function get_hotel_text_data_ at 0x7ff070c15940>)),\n",
       "                                                                 ('vectorizer',\n",
       "                                                                  TfidfVectorizer(max_df=0.75,\n",
       "                                                                                  max_features=2000))]))])),\n",
       "                ('svm', SVC(C=10, kernel='linear'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hotel_text_data_(df):\n",
    "    df = df.copy()\n",
    "    df['textdata'] = clean_text(df['name']+ ' ' + df['description'] + ' ' + df['recent_100_statuses'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_hotel_text_data_)\n",
    "\n",
    "\n",
    "def get_hotel_numeric_data_(df):\n",
    "    data = df['friends_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_hotel_numeric_data_)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer(max_df=0.75, max_features=2000, min_df=1)),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC(kernel='linear', C=10))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "russian-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../classifiers/classifier_hotel_ndtfr.sav']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../classifiers/classifier_hotel_ndtfr.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-iceland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
