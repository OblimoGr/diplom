{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unlikely-matrix",
   "metadata": {},
   "source": [
    "# Classification Experiment: Followers\n",
    "---\n",
    "This Notebook, includes a series of experiments, on using a node's Friends for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brown-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "humanitarian-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_credentials = []\n",
    "with open('../../../../twitter_credentials.json', 'r') as f:\n",
    "    twitter_credentials = json.load(f)\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_credentials['consumer_key'], twitter_credentials['consumer_secret'])\n",
    "auth.set_access_token(twitter_credentials['access_token_key'],twitter_credentials['access_token_secret'])\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, timeout=60*5, \n",
    "                 retry_count=10, retry_delay=60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "celtic-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function For Text Normalization\n",
    "def clean_text(data):\n",
    "    urls = r'http\\S+'\n",
    "    non_unicode_char = r'\\W'\n",
    "    numbers = r'[0-9_]'\n",
    "    fix_whitespace = r'\\s+'\n",
    "    single_whitespace = ' '\n",
    "    \n",
    "    data = (data.replace([urls], single_whitespace, regex=True)\n",
    "                    .replace([non_unicode_char, numbers], single_whitespace, regex=True)\n",
    "                    .replace(fix_whitespace, single_whitespace, regex=True))\n",
    "    data = data.apply(lambda s: s.lower() if type(s) == str else s)\n",
    "    return data\n",
    "\n",
    "# NLP Functions\n",
    "nlp_el = spacy.load('el_core_news_md')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "STOPWORDS = set(list(spacy.lang.en.STOP_WORDS) + list(spacy.lang.el.STOP_WORDS))\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    row = [str(token) for token in nlp_el(row)]\n",
    "    return [w for w in row if w not in STOPWORDS]\n",
    "\n",
    "def tokenize_lemmatize(row):\n",
    "    return [str(token.lemma_) for token in nlp_el(row)]\n",
    "\n",
    "def tokenize_lemmatize_en(row):\n",
    "    return [str(token.lemma_) for token in nlp_en(row)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chinese-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data_hotel_nd(df):\n",
    "    df['textdata'] = clean_text(df['name'] + ' ' + df['description'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "def fetch_followers(node, count=1000):\n",
    "    # Fetch followers IDs\n",
    "    friend_ids = []\n",
    "    try:\n",
    "        for ids in tweepy.Cursor(API.followers_ids, node).items(count):\n",
    "            friend_ids.append(ids)\n",
    "\n",
    "    except tweepy.error.TweepError as err:\n",
    "        return pd.DataFrame(columns=['name', 'description'])\n",
    "\n",
    "    except Exception as err:\n",
    "        raise Exception(f'An unknown Error has occurred.\\n{err}')\n",
    "\n",
    "    # If node has zero friends\n",
    "    if not friend_ids:\n",
    "        return pd.DataFrame(columns=['name', 'description'])\n",
    "\n",
    "    # Calculate Iteration Required, to iterate per 100 ids\n",
    "    if (int(len(friend_ids)) % 100) == 0:\n",
    "        it_num = int(len(friend_ids) / 100)\n",
    "    else:\n",
    "        it_num = (int(len(friend_ids) / 100) + 1)\n",
    "\n",
    "    # Transform IDs to User Objects\n",
    "    users = list()\n",
    "    try:\n",
    "        for i in range(it_num):\n",
    "            users.append(API.lookup_users(friend_ids[100 * i: 100 * (1 + i)]))\n",
    "    except Exception as err:\n",
    "        print(f'An unknown Error has occurred.\\n{err}')\n",
    "        time.sleep(60*5)\n",
    "        for i in range(it_num):\n",
    "            users.append(API.lookup_users(friend_ids[100 * i: 100 * (1 + i)]))\n",
    "\n",
    "    # Extract Profile Name and Description for each friends and save it to a DataFrame\n",
    "    results = pd.DataFrame()\n",
    "    for items in users:\n",
    "        for user in items:\n",
    "            results = results.append(pd.DataFrame([user.name, user.description]).T)\n",
    "    results.columns = ['name', 'description']\n",
    "    results = results.reset_index().drop('index', axis=1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_hotel_followers(nodes):\n",
    "\n",
    "    counts = []\n",
    "    for node in tqdm(nodes, leave=False):\n",
    "        # Get Required Data\n",
    "        data = fetch_followers(node=node, count=1000)\n",
    "\n",
    "        # Get Labels\n",
    "        if not data.empty:\n",
    "            model_nd = joblib.load('../classifiers/classifier_hotel_nd.sav')\n",
    "            data['label'] = model_nd.predict(data)\n",
    "\n",
    "            count = len(data[data['label'] == 1])\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        counts.append(count)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-penguin",
   "metadata": {},
   "source": [
    "## Calculate Hotel Followers Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-accreditation",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "irish-gossip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "      <th>friends_hotel_count_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SophiaSuites</td>\n",
       "      <td>Sophia Collection Santorini</td>\n",
       "      <td>Luxury Suites, hotels and villas Santorini com...</td>\n",
       "      <td>513</td>\n",
       "      <td>41</td>\n",
       "      <td>127</td>\n",
       "      <td>\"The tans will fade but the memories will las...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnthiMariaApart</td>\n",
       "      <td>AnthiMariaApartments</td>\n",
       "      <td>Anthi Maria Beach Apartments is a self-caterin...</td>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "      <td>110</td>\n",
       "      <td>Our fantastic New and Improved abc online web...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wEndowproject</td>\n",
       "      <td>wEndow project</td>\n",
       "      <td>WEndow Escape Resort &amp; Villas | Tailor-made Ad...</td>\n",
       "      <td>350</td>\n",
       "      <td>344</td>\n",
       "      <td>103</td>\n",
       "      <td>https://t.co/DHuXrG8G6o For those who still d...</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paphotels</td>\n",
       "      <td>paphotels</td>\n",
       "      <td>The best of Greek hospitality! Follow us, visi...</td>\n",
       "      <td>975</td>\n",
       "      <td>1182</td>\n",
       "      <td>475</td>\n",
       "      <td>@AlbertBourla üíØüíØüíØüíØüíØ Happy Easter !!!üê£ @paphot...</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medpalace</td>\n",
       "      <td>Mediterranean Palace</td>\n",
       "      <td>A cozy 5 star hotel in the city center with an...</td>\n",
       "      <td>269</td>\n",
       "      <td>543</td>\n",
       "      <td>381</td>\n",
       "      <td>https://t.co/WPCR6KSnw2 New era!\\nNew Brand! ...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                         name  \\\n",
       "0     SophiaSuites  Sophia Collection Santorini   \n",
       "1  AnthiMariaApart         AnthiMariaApartments   \n",
       "2    wEndowproject               wEndow project   \n",
       "3        paphotels                    paphotels   \n",
       "4        medpalace         Mediterranean Palace   \n",
       "\n",
       "                                         description  statuses_count  \\\n",
       "0  Luxury Suites, hotels and villas Santorini com...             513   \n",
       "1  Anthi Maria Beach Apartments is a self-caterin...             102   \n",
       "2  WEndow Escape Resort & Villas | Tailor-made Ad...             350   \n",
       "3  The best of Greek hospitality! Follow us, visi...             975   \n",
       "4  A cozy 5 star hotel in the city center with an...             269   \n",
       "\n",
       "   friends_count  followers_count  \\\n",
       "0             41              127   \n",
       "1             25              110   \n",
       "2            344              103   \n",
       "3           1182              475   \n",
       "4            543              381   \n",
       "\n",
       "                                 recent_100_statuses  hotel  \\\n",
       "0   \"The tans will fade but the memories will las...      1   \n",
       "1   Our fantastic New and Improved abc online web...      1   \n",
       "2   https://t.co/DHuXrG8G6o For those who still d...      1   \n",
       "3   @AlbertBourla üíØüíØüíØüíØüíØ Happy Easter !!!üê£ @paphot...      1   \n",
       "4   https://t.co/WPCR6KSnw2 New era!\\nNew Brand! ...      1   \n",
       "\n",
       "   friends_hotel_count_1000  \n",
       "0                         4  \n",
       "1                         5  \n",
       "2                        33  \n",
       "3                        67  \n",
       "4                        49  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-validation-set-enhanced.csv')\n",
    "validation_set = validation_set.replace(np.nan, '')\n",
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mediterranean-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 580\n",
      "Rate limit reached. Sleeping for: 487\n",
      "Rate limit reached. Sleeping for: 699\n"
     ]
    }
   ],
   "source": [
    "hotel_followers = calculate_hotel_followers(validation_set.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "attended-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set['followers_hotel_count_1000'] = hotel_followers\n",
    "validation_set.to_csv('../../../../datasets/Hotels/classification/hotels-validation-set-enhanced.csv', index=False)\n",
    "del validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-union",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjacent-payment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "      <th>friends_hotel_count_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aldemar_resorts</td>\n",
       "      <td>Aldemar Resorts</td>\n",
       "      <td>Guest satisfaction is our top priority! *Luxur...</td>\n",
       "      <td>1832</td>\n",
       "      <td>1569</td>\n",
       "      <td>2229</td>\n",
       "      <td>Summer vacation is meant to make you feel ‚õ± r...</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AquaVistaHotels</td>\n",
       "      <td>Aqua Vista Hotels</td>\n",
       "      <td>A compilation of extraordinary hotels catering...</td>\n",
       "      <td>5924</td>\n",
       "      <td>1650</td>\n",
       "      <td>2116</td>\n",
       "      <td>Thank you Greek Travel Pages for highlighting...</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eurobank_Group</td>\n",
       "      <td>Eurobank</td>\n",
       "      <td>ŒöŒ±ŒªœâœÉŒÆœÅŒ∏Œ±œÑŒµ œÉœÑŒ∑ŒΩ ŒµœÄŒØœÉŒ∑ŒºŒ∑ œÉŒµŒªŒØŒ¥Œ± œÑŒ∑œÇ Eurobank œÉ...</td>\n",
       "      <td>3284</td>\n",
       "      <td>0</td>\n",
       "      <td>2691</td>\n",
       "      <td>Œó¬†Eurobank¬†ŒµŒΩŒ∑ŒºŒµœÅœéŒΩŒµŒπ œåœÑŒπ œÑŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ¨¬†œÑŒ∑œÇ,¬†Œ∫...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white_suites</td>\n",
       "      <td>White¬†Suites Resort</td>\n",
       "      <td>White Suites Resort is a luxury beach hotel in...</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>18</td>\n",
       "      <td>Sea side holidays in Afytos, Halikidiki White...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KarenMillen</td>\n",
       "      <td>Karen Millen</td>\n",
       "      <td>Timeless, elevated ready-to-wear style for women.</td>\n",
       "      <td>10908</td>\n",
       "      <td>1409</td>\n",
       "      <td>35679</td>\n",
       "      <td>The future's bright.\\nhttps://t.co/XLpskBYi4u...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                 name  \\\n",
       "0  aldemar_resorts      Aldemar Resorts   \n",
       "1  AquaVistaHotels    Aqua Vista Hotels   \n",
       "2   Eurobank_Group             Eurobank   \n",
       "3     white_suites  White¬†Suites Resort   \n",
       "4      KarenMillen         Karen Millen   \n",
       "\n",
       "                                         description  statuses_count  \\\n",
       "0  Guest satisfaction is our top priority! *Luxur...            1832   \n",
       "1  A compilation of extraordinary hotels catering...            5924   \n",
       "2  ŒöŒ±ŒªœâœÉŒÆœÅŒ∏Œ±œÑŒµ œÉœÑŒ∑ŒΩ ŒµœÄŒØœÉŒ∑ŒºŒ∑ œÉŒµŒªŒØŒ¥Œ± œÑŒ∑œÇ Eurobank œÉ...            3284   \n",
       "3  White Suites Resort is a luxury beach hotel in...               2   \n",
       "4  Timeless, elevated ready-to-wear style for women.           10908   \n",
       "\n",
       "   friends_count  followers_count  \\\n",
       "0           1569             2229   \n",
       "1           1650             2116   \n",
       "2              0             2691   \n",
       "3             93               18   \n",
       "4           1409            35679   \n",
       "\n",
       "                                 recent_100_statuses  hotel  \\\n",
       "0   Summer vacation is meant to make you feel ‚õ± r...      1   \n",
       "1   Thank you Greek Travel Pages for highlighting...      1   \n",
       "2   Œó¬†Eurobank¬†ŒµŒΩŒ∑ŒºŒµœÅœéŒΩŒµŒπ œåœÑŒπ œÑŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ¨¬†œÑŒ∑œÇ,¬†Œ∫...      0   \n",
       "3   Sea side holidays in Afytos, Halikidiki White...      1   \n",
       "4   The future's bright.\\nhttps://t.co/XLpskBYi4u...      0   \n",
       "\n",
       "   friends_hotel_count_1000  \n",
       "0                        83  \n",
       "1                       118  \n",
       "2                         0  \n",
       "3                         6  \n",
       "4                        30  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv')\n",
    "training_set = training_set.replace(np.nan, '')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "indian-clothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 728\n",
      "Rate limit reached. Sleeping for: 695\n",
      "Rate limit reached. Sleeping for: 683\n",
      "Rate limit reached. Sleeping for: 699\n",
      "Rate limit reached. Sleeping for: 771\n",
      "Rate limit reached. Sleeping for: 714\n",
      "Rate limit reached. Sleeping for: 597\n",
      "Rate limit reached. Sleeping for: 666\n",
      "Rate limit reached. Sleeping for: 759\n",
      "Rate limit reached. Sleeping for: 685\n",
      "Rate limit reached. Sleeping for: 610\n",
      "Rate limit reached. Sleeping for: 674\n",
      "Rate limit reached. Sleeping for: 631\n"
     ]
    }
   ],
   "source": [
    "hotel_friends = calculate_hotel_followers(training_set.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "clean-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['followers_hotel_count_1000'] = hotel_friends\n",
    "training_set.to_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv', index=False)\n",
    "del training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-reminder",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sunset-twenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>recent_100_statuses</th>\n",
       "      <th>hotel</th>\n",
       "      <th>friends_hotel_count_1000</th>\n",
       "      <th>followers_hotel_count_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>AlexanderHaus</td>\n",
       "      <td>Alexander Haus</td>\n",
       "      <td>#Studio #Rooms to Let, in #Halkidiki, #Sithoni...</td>\n",
       "      <td>492</td>\n",
       "      <td>232</td>\n",
       "      <td>277</td>\n",
       "      <td>Though is winter, summer is coming! https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>artsoundgr</td>\n",
       "      <td>ArtSound &amp; Lights</td>\n",
       "      <td>Art Sound &amp; Lights Professional Audio/Video Se...</td>\n",
       "      <td>443</td>\n",
       "      <td>497</td>\n",
       "      <td>191</td>\n",
       "      <td>Œ†œÅŒøœÉœÜŒøœÅŒ¨ STROBE 1500W DMX ARTLIGHT ST1500W Œºœå...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>JOHNMARYRHODES</td>\n",
       "      <td>JOHNMARY FALIRAKI</td>\n",
       "      <td>John Mary is a famly hotel and is located at F...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>http://t.co/DNz6I3s0Sh http://t.co/9eqsMrL4MK</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>THEMETHOTEL</td>\n",
       "      <td>THE MET HOTEL</td>\n",
       "      <td>https://t.co/fi814NlnxK\\r\\nhttp://t.co/AlYUMI5...</td>\n",
       "      <td>2816</td>\n",
       "      <td>181</td>\n",
       "      <td>1136</td>\n",
       "      <td>Let the LOVE sparkle at The MET Hotel!!\\n\\n#T...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>EvitaResort</td>\n",
       "      <td>SunConnect Evita</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>Zumba time @evitaresort @evitaresort #sunconn...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        screen_name               name  \\\n",
       "195   AlexanderHaus     Alexander Haus   \n",
       "196      artsoundgr  ArtSound & Lights   \n",
       "197  JOHNMARYRHODES  JOHNMARY FALIRAKI   \n",
       "198     THEMETHOTEL      THE MET HOTEL   \n",
       "199     EvitaResort   SunConnect Evita   \n",
       "\n",
       "                                           description  statuses_count  \\\n",
       "195  #Studio #Rooms to Let, in #Halkidiki, #Sithoni...             492   \n",
       "196  Art Sound & Lights Professional Audio/Video Se...             443   \n",
       "197  John Mary is a famly hotel and is located at F...               2   \n",
       "198  https://t.co/fi814NlnxK\\r\\nhttp://t.co/AlYUMI5...            2816   \n",
       "199                                                                 25   \n",
       "\n",
       "     friends_count  followers_count  \\\n",
       "195            232              277   \n",
       "196            497              191   \n",
       "197             15                7   \n",
       "198            181             1136   \n",
       "199             24               37   \n",
       "\n",
       "                                   recent_100_statuses  hotel  \\\n",
       "195   Though is winter, summer is coming! https://t...      1   \n",
       "196   Œ†œÅŒøœÉœÜŒøœÅŒ¨ STROBE 1500W DMX ARTLIGHT ST1500W Œºœå...      0   \n",
       "197      http://t.co/DNz6I3s0Sh http://t.co/9eqsMrL4MK      1   \n",
       "198   Let the LOVE sparkle at The MET Hotel!!\\n\\n#T...      1   \n",
       "199   Zumba time @evitaresort @evitaresort #sunconn...      1   \n",
       "\n",
       "     friends_hotel_count_1000  followers_hotel_count_1000  \n",
       "195                        14                          24  \n",
       "196                        13                           5  \n",
       "197                         2                           3  \n",
       "198                         9                          68  \n",
       "199                         1                           3  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Training Set\n",
    "training_set = pd.read_csv('../../../../datasets/Hotels/classification/hotels-training-set-enhanced.csv')\n",
    "training_set = training_set.replace(np.nan, '')\n",
    "training_set.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-maple",
   "metadata": {},
   "source": [
    "# Only Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "helpful-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "greatest-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      " Best Params: {'svm__C': 0.1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.6849999999999999\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan   nan 0.655 0.655 0.645 0.675 0.655 0.66  0.67  0.67  0.655 0.66\n",
      " 0.655 0.675 0.665 0.665 0.64  0.665 0.635 0.66  0.625 0.66  0.645 0.655\n",
      " 0.61  0.655 0.63  0.66  0.635 0.66  0.63  0.66  0.61  0.66  0.62  0.66\n",
      " 0.635 0.66  0.63  0.66 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'knn__n_neighbors': 2, 'knn__weights': 'distance'}.\n",
      " Score: 0.675\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      " Best Params: {'lr__C': 0.1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.6799999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [ nan 0.68  nan  nan 0.68  nan  nan 0.68  nan  nan 0.68  nan  nan 0.68\n",
      "  nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df['followers_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'knn__n_neighbors': [i for i in range(20)],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-daily",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "theoretical-background",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 FunctionTransformer(func=<function get_data_hotel_fo at 0x7f906ebce5e0>)),\n",
       "                ('svm', SVC(C=0.1, kernel='linear'))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel\n",
    "\n",
    "# Function to select the data\n",
    "def get_data_hotel_fo(df):\n",
    "    data = df['followers_hotel_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_hotel_fo)\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC(C=0.1, kernel='linear'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bacterial-implementation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_hotel_fo.sav']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'classifier_hotel_fo.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-module",
   "metadata": {},
   "source": [
    "## Friends and Followers Politician COunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fatty-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "increased-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      " Best Params: {'svm__C': 0.1, 'svm__kernel': 'rbf'}.\n",
      " Score: 0.6900000000000001\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan   nan 0.66  0.66  0.69  0.695 0.68  0.7   0.7   0.715 0.71  0.71\n",
      " 0.725 0.715 0.73  0.71  0.735 0.725 0.745 0.71  0.755 0.715 0.745 0.71\n",
      " 0.735 0.72  0.725 0.715 0.74  0.72  0.725 0.71  0.715 0.72  0.71  0.715\n",
      " 0.725 0.72  0.715 0.715]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'knn__n_neighbors': 10, 'knn__weights': 'uniform'}.\n",
      " Score: 0.7550000000000001\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      " Best Params: {'lr__C': 0.1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.6849999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.685   nan   nan 0.685   nan   nan 0.685   nan   nan 0.685   nan\n",
      "   nan 0.685   nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'knn__n_neighbors': [i for i in range(20)],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-lucas",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cardiac-establishment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 FunctionTransformer(func=<function get_data_ at 0x7f906ebce940>)),\n",
       "                ('knn', KNeighborsClassifier(n_neighbors=10))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel\n",
    "\n",
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10, weights='uniform'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "mounted-guard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../classifiers/classifier_hotel_frfo.sav']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../classifiers/classifier_hotel_frfo.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-commission",
   "metadata": {},
   "source": [
    "# Name Description Tweets and Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-position",
   "metadata": {},
   "source": [
    "## Without NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "southwest-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "central-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.735 0.65  0.69  0.635 0.875 0.67  0.69  0.505 0.87  0.67  0.69  0.5\n",
      " 0.82  0.675 0.69  0.57  0.79  0.675 0.685 0.575 0.765 0.65  0.69  0.635\n",
      " 0.875 0.67  0.69  0.505 0.88  0.67  0.69  0.5   0.855 0.675 0.69  0.57\n",
      " 0.81  0.675 0.685 0.575 0.775 0.65  0.69  0.635 0.875 0.67  0.69  0.505\n",
      " 0.88  0.67  0.69  0.5   0.825 0.675 0.69  0.57  0.81  0.675 0.685 0.575\n",
      " 0.73  0.65  0.69  0.635 0.87  0.67  0.69  0.505 0.865 0.67  0.69  0.5\n",
      " 0.83  0.675 0.69  0.57  0.785 0.675 0.685 0.575 0.76  0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.505 0.875 0.67  0.69  0.5   0.845 0.675 0.69  0.57\n",
      " 0.835 0.675 0.685 0.575 0.775 0.65  0.69  0.635 0.875 0.67  0.69  0.505\n",
      " 0.88  0.67  0.69  0.5   0.825 0.675 0.69  0.57  0.81  0.675 0.685 0.575\n",
      " 0.715 0.65  0.69  0.635 0.865 0.67  0.69  0.505 0.87  0.67  0.69  0.5\n",
      " 0.865 0.675 0.69  0.57  0.865 0.675 0.685 0.575 0.76  0.65  0.69  0.635\n",
      " 0.87  0.67  0.69  0.505 0.875 0.67  0.69  0.5   0.845 0.675 0.69  0.57\n",
      " 0.835 0.675 0.685 0.575 0.775 0.65  0.69  0.635 0.875 0.67  0.69  0.505\n",
      " 0.88  0.67  0.69  0.5   0.825 0.675 0.69  0.57  0.81  0.675 0.685 0.575\n",
      " 0.75  0.65  0.69  0.635 0.865 0.67  0.69  0.505 0.87  0.67  0.69  0.5\n",
      " 0.855 0.675 0.69  0.57  0.77  0.675 0.685 0.575 0.78  0.65  0.69  0.635\n",
      " 0.875 0.67  0.69  0.505 0.87  0.67  0.69  0.5   0.875 0.675 0.69  0.57\n",
      " 0.81  0.675 0.685 0.575 0.795 0.65  0.69  0.635 0.865 0.67  0.69  0.505\n",
      " 0.87  0.67  0.69  0.5   0.86  0.675 0.69  0.57  0.83  0.675 0.685 0.575\n",
      " 0.735 0.65  0.69  0.635 0.855 0.67  0.69  0.505 0.86  0.67  0.69  0.5\n",
      " 0.835 0.675 0.69  0.57  0.775 0.675 0.685 0.575 0.765 0.65  0.69  0.635\n",
      " 0.865 0.67  0.69  0.505 0.865 0.67  0.69  0.5   0.85  0.675 0.69  0.57\n",
      " 0.83  0.675 0.685 0.575 0.795 0.65  0.69  0.635 0.865 0.67  0.69  0.505\n",
      " 0.87  0.67  0.69  0.5   0.86  0.675 0.69  0.57  0.83  0.675 0.685 0.575\n",
      " 0.715 0.65  0.69  0.635 0.85  0.67  0.69  0.505 0.86  0.67  0.69  0.5\n",
      " 0.855 0.675 0.69  0.495 0.855 0.675 0.685 0.495 0.76  0.65  0.69  0.635\n",
      " 0.86  0.67  0.69  0.505 0.865 0.67  0.69  0.5   0.85  0.675 0.69  0.57\n",
      " 0.825 0.675 0.685 0.575 0.795 0.65  0.69  0.635 0.865 0.67  0.69  0.505\n",
      " 0.87  0.67  0.69  0.5   0.86  0.675 0.69  0.57  0.83  0.675 0.685 0.575\n",
      " 0.675 0.65  0.69  0.635 0.675 0.67  0.69  0.505 0.695 0.67  0.69  0.5\n",
      " 0.73  0.675 0.69  0.495 0.67  0.675 0.685 0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.675 0.65  0.69  0.635 0.68  0.67  0.69  0.505 0.685 0.67  0.69  0.5\n",
      " 0.715 0.675 0.69  0.495 0.65  0.675 0.685 0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.675 0.65  0.69  0.635 0.685 0.67  0.69  0.505 0.695 0.67  0.69  0.5\n",
      " 0.715 0.675 0.69  0.495 0.715 0.675 0.685 0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.8799999999999999\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.665 0.665 0.695 0.7   0.715 0.71  0.71  0.725 0.705 0.705 0.72  0.72\n",
      " 0.72  0.715 0.745 0.725 0.75  0.73  0.755 0.72  0.66  0.66  0.69  0.695\n",
      " 0.725 0.72  0.71  0.735 0.72  0.72  0.72  0.72  0.73  0.725 0.74  0.73\n",
      " 0.75  0.73  0.76  0.72  0.66  0.66  0.69  0.7   0.72  0.715 0.715 0.735\n",
      " 0.73  0.73  0.73  0.725 0.74  0.735 0.75  0.745 0.755 0.735 0.755 0.725\n",
      " 0.65  0.65  0.69  0.685 0.71  0.705 0.71  0.725 0.71  0.71  0.73  0.715\n",
      " 0.74  0.735 0.745 0.73  0.75  0.73  0.755 0.72  0.665 0.665 0.695 0.7\n",
      " 0.715 0.71  0.71  0.735 0.715 0.715 0.73  0.715 0.745 0.74  0.745 0.74\n",
      " 0.745 0.725 0.755 0.72  0.66  0.66  0.69  0.7   0.72  0.715 0.715 0.735\n",
      " 0.73  0.73  0.73  0.725 0.74  0.735 0.75  0.745 0.755 0.735 0.755 0.725\n",
      " 0.685 0.685 0.68  0.69  0.705 0.705 0.71  0.72  0.73  0.735 0.725 0.725\n",
      " 0.74  0.745 0.755 0.745 0.76  0.745 0.76  0.73  0.665 0.665 0.695 0.7\n",
      " 0.71  0.705 0.71  0.735 0.715 0.715 0.73  0.715 0.74  0.735 0.75  0.74\n",
      " 0.745 0.725 0.755 0.72  0.66  0.66  0.69  0.7   0.72  0.715 0.715 0.735\n",
      " 0.73  0.73  0.73  0.725 0.74  0.735 0.75  0.745 0.755 0.735 0.755 0.725\n",
      " 0.665 0.665 0.69  0.665 0.715 0.715 0.72  0.73  0.72  0.725 0.72  0.72\n",
      " 0.72  0.715 0.745 0.73  0.755 0.735 0.765 0.725 0.665 0.665 0.695 0.67\n",
      " 0.73  0.73  0.715 0.73  0.725 0.73  0.72  0.72  0.73  0.725 0.745 0.735\n",
      " 0.755 0.735 0.76  0.725 0.665 0.665 0.68  0.665 0.72  0.72  0.705 0.715\n",
      " 0.715 0.715 0.72  0.715 0.73  0.725 0.74  0.735 0.75  0.73  0.755 0.725\n",
      " 0.665 0.665 0.69  0.67  0.72  0.72  0.715 0.725 0.715 0.72  0.725 0.715\n",
      " 0.725 0.725 0.74  0.725 0.755 0.74  0.765 0.73  0.67  0.67  0.69  0.67\n",
      " 0.725 0.725 0.71  0.73  0.715 0.72  0.725 0.71  0.73  0.725 0.74  0.735\n",
      " 0.745 0.725 0.76  0.73  0.665 0.665 0.68  0.665 0.72  0.72  0.705 0.715\n",
      " 0.715 0.715 0.72  0.715 0.73  0.725 0.74  0.735 0.75  0.73  0.755 0.725\n",
      " 0.7   0.7   0.685 0.7   0.705 0.705 0.715 0.72  0.72  0.725 0.72  0.725\n",
      " 0.735 0.735 0.74  0.735 0.755 0.74  0.77  0.73  0.675 0.675 0.69  0.675\n",
      " 0.725 0.725 0.71  0.73  0.715 0.72  0.725 0.715 0.73  0.725 0.74  0.74\n",
      " 0.745 0.725 0.76  0.73  0.665 0.665 0.68  0.665 0.72  0.72  0.705 0.715\n",
      " 0.715 0.715 0.72  0.715 0.73  0.725 0.74  0.735 0.75  0.73  0.755 0.725\n",
      " 0.615 0.615 0.68  0.655 0.645 0.645 0.68  0.66  0.695 0.695 0.73  0.665\n",
      " 0.705 0.7   0.735 0.695 0.735 0.705 0.74  0.705   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.595 0.595 0.675 0.64  0.655 0.67  0.695 0.68  0.685 0.7   0.705 0.7\n",
      " 0.69  0.725 0.74  0.715 0.74  0.715 0.75  0.715   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.685 0.69  0.685 0.705 0.705 0.7   0.715 0.715 0.72  0.72  0.72\n",
      " 0.735 0.745 0.745 0.735 0.745 0.72  0.755 0.72    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 10, 'knn__weights': 'uniform'}.\n",
      " Score: 0.77\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.735   nan 0.845   nan 0.82    nan 0.845   nan 0.87    nan 0.845\n",
      "   nan 0.86    nan 0.845   nan 0.855   nan 0.845   nan 0.74    nan 0.88\n",
      "   nan 0.845   nan 0.88    nan 0.87    nan 0.88    nan 0.865   nan 0.88\n",
      "   nan 0.86    nan 0.88    nan 0.74    nan 0.875   nan 0.855   nan 0.875\n",
      "   nan 0.865   nan 0.875   nan 0.86    nan 0.875   nan 0.865   nan 0.875\n",
      "   nan 0.72    nan 0.85    nan 0.815   nan 0.85    nan 0.85    nan 0.85\n",
      "   nan 0.855   nan 0.85    nan 0.86    nan 0.85    nan 0.72    nan 0.865\n",
      "   nan 0.845   nan 0.865   nan 0.865   nan 0.865   nan 0.865   nan 0.865\n",
      "   nan 0.865   nan 0.865   nan 0.74    nan 0.875   nan 0.855   nan 0.875\n",
      "   nan 0.865   nan 0.875   nan 0.86    nan 0.875   nan 0.865   nan 0.875\n",
      "   nan 0.72    nan 0.84    nan 0.775   nan 0.84    nan 0.82    nan 0.84\n",
      "   nan 0.865   nan 0.84    nan 0.855   nan 0.84    nan 0.72    nan 0.87\n",
      "   nan 0.845   nan 0.87    nan 0.865   nan 0.87    nan 0.865   nan 0.87\n",
      "   nan 0.86    nan 0.87    nan 0.74    nan 0.875   nan 0.855   nan 0.875\n",
      "   nan 0.865   nan 0.875   nan 0.86    nan 0.875   nan 0.865   nan 0.875\n",
      "   nan 0.715   nan 0.825   nan 0.825   nan 0.825   nan 0.845   nan 0.825\n",
      "   nan 0.845   nan 0.825   nan 0.85    nan 0.825   nan 0.73    nan 0.865\n",
      "   nan 0.83    nan 0.865   nan 0.855   nan 0.865   nan 0.85    nan 0.865\n",
      "   nan 0.855   nan 0.865   nan 0.735   nan 0.83    nan 0.83    nan 0.83\n",
      "   nan 0.855   nan 0.83    nan 0.855   nan 0.83    nan 0.855   nan 0.83\n",
      "   nan 0.705   nan 0.825   nan 0.815   nan 0.825   nan 0.84    nan 0.825\n",
      "   nan 0.84    nan 0.825   nan 0.855   nan 0.825   nan 0.72    nan 0.865\n",
      "   nan 0.83    nan 0.865   nan 0.855   nan 0.865   nan 0.85    nan 0.865\n",
      "   nan 0.85    nan 0.865   nan 0.735   nan 0.83    nan 0.83    nan 0.83\n",
      "   nan 0.855   nan 0.83    nan 0.855   nan 0.83    nan 0.855   nan 0.83\n",
      "   nan 0.7     nan 0.83    nan 0.77    nan 0.83    nan 0.83    nan 0.83\n",
      "   nan 0.845   nan 0.83    nan 0.85    nan 0.83    nan 0.715   nan 0.86\n",
      "   nan 0.83    nan 0.86    nan 0.855   nan 0.86    nan 0.85    nan 0.86\n",
      "   nan 0.85    nan 0.86    nan 0.735   nan 0.83    nan 0.83    nan 0.83\n",
      "   nan 0.855   nan 0.83    nan 0.855   nan 0.83    nan 0.855   nan 0.83\n",
      "   nan 0.685   nan 0.66    nan 0.685   nan 0.66    nan 0.695   nan 0.66\n",
      "   nan 0.74    nan 0.66    nan 0.71    nan 0.66    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.685   nan 0.725   nan 0.68    nan 0.725   nan 0.695   nan 0.725\n",
      "   nan 0.725   nan 0.725   nan 0.725   nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.685   nan 0.72    nan 0.695   nan 0.72    nan 0.705   nan 0.72\n",
      "   nan 0.72    nan 0.72    nan 0.72    nan 0.72    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 0.1, 'lr__penalty': 'none'}.\n",
      " Score: 0.8800000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-minneapolis",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "failing-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "outdoor-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.725 0.65  0.69  0.635 0.865 0.67  0.69  0.505 0.89  0.67  0.69  0.5\n",
      " 0.85  0.675 0.69  0.57  0.845 0.675 0.685 0.575 0.735 0.65  0.69  0.635\n",
      " 0.885 0.67  0.69  0.505 0.89  0.67  0.69  0.5   0.855 0.675 0.69  0.57\n",
      " 0.835 0.675 0.685 0.575 0.755 0.65  0.69  0.635 0.875 0.67  0.69  0.505\n",
      " 0.885 0.67  0.69  0.5   0.86  0.675 0.69  0.57  0.835 0.675 0.685 0.575\n",
      " 0.7   0.65  0.69  0.635 0.85  0.67  0.69  0.505 0.875 0.67  0.69  0.5\n",
      " 0.85  0.675 0.69  0.57  0.845 0.675 0.685 0.575 0.735 0.65  0.69  0.635\n",
      " 0.885 0.67  0.69  0.505 0.885 0.67  0.69  0.5   0.87  0.675 0.69  0.495\n",
      " 0.85  0.675 0.685 0.495 0.755 0.65  0.69  0.635 0.875 0.67  0.69  0.505\n",
      " 0.885 0.67  0.69  0.5   0.86  0.675 0.69  0.57  0.835 0.675 0.685 0.575\n",
      " 0.69  0.65  0.69  0.635 0.85  0.67  0.69  0.505 0.875 0.67  0.69  0.5\n",
      " 0.87  0.675 0.69  0.495 0.875 0.675 0.685 0.495 0.735 0.65  0.69  0.635\n",
      " 0.885 0.67  0.69  0.505 0.885 0.67  0.69  0.5   0.87  0.675 0.69  0.495\n",
      " 0.85  0.675 0.685 0.495 0.755 0.65  0.69  0.635 0.875 0.67  0.69  0.505\n",
      " 0.885 0.67  0.69  0.5   0.86  0.675 0.69  0.57  0.835 0.675 0.685 0.575\n",
      " 0.725 0.65  0.69  0.635 0.865 0.67  0.69  0.505 0.89  0.67  0.69  0.5\n",
      " 0.85  0.675 0.69  0.57  0.84  0.675 0.685 0.575 0.735 0.65  0.69  0.635\n",
      " 0.885 0.67  0.69  0.505 0.89  0.67  0.69  0.5   0.86  0.675 0.69  0.57\n",
      " 0.835 0.675 0.685 0.575 0.75  0.65  0.69  0.635 0.885 0.67  0.69  0.505\n",
      " 0.885 0.67  0.69  0.5   0.84  0.675 0.69  0.57  0.825 0.675 0.685 0.575\n",
      " 0.705 0.65  0.69  0.635 0.86  0.67  0.69  0.505 0.87  0.67  0.69  0.5\n",
      " 0.845 0.675 0.69  0.57  0.86  0.675 0.685 0.575 0.735 0.65  0.69  0.635\n",
      " 0.88  0.67  0.69  0.505 0.885 0.67  0.69  0.5   0.86  0.675 0.69  0.495\n",
      " 0.85  0.675 0.685 0.495 0.75  0.65  0.69  0.635 0.885 0.67  0.69  0.505\n",
      " 0.885 0.67  0.69  0.5   0.84  0.675 0.69  0.57  0.825 0.675 0.685 0.575\n",
      " 0.69  0.65  0.69  0.635 0.85  0.67  0.69  0.505 0.87  0.67  0.69  0.5\n",
      " 0.87  0.675 0.69  0.495 0.87  0.675 0.685 0.495 0.735 0.65  0.69  0.635\n",
      " 0.88  0.67  0.69  0.505 0.885 0.67  0.69  0.5   0.86  0.675 0.69  0.495\n",
      " 0.85  0.675 0.685 0.495 0.75  0.65  0.69  0.635 0.885 0.67  0.69  0.505\n",
      " 0.885 0.67  0.69  0.5   0.84  0.675 0.69  0.57  0.825 0.675 0.685 0.575\n",
      " 0.675 0.65  0.69  0.635 0.675 0.67  0.69  0.505 0.68  0.67  0.69  0.5\n",
      " 0.685 0.675 0.69  0.495 0.66  0.675 0.685 0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.675 0.65  0.69  0.635 0.685 0.67  0.69  0.505 0.685 0.67  0.69  0.5\n",
      " 0.71  0.675 0.69  0.495 0.665 0.675 0.685 0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.675 0.65  0.69  0.635 0.685 0.67  0.69  0.505 0.695 0.67  0.69  0.5\n",
      " 0.715 0.675 0.69  0.495 0.715 0.675 0.685 0.495   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.89\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.655 0.655 0.685 0.695 0.71  0.705 0.71  0.725 0.72  0.72  0.73  0.72\n",
      " 0.73  0.725 0.745 0.735 0.745 0.73  0.755 0.72  0.655 0.655 0.685 0.69\n",
      " 0.695 0.69  0.705 0.72  0.72  0.72  0.725 0.715 0.73  0.725 0.74  0.73\n",
      " 0.75  0.73  0.75  0.715 0.665 0.665 0.685 0.7   0.7   0.695 0.71  0.725\n",
      " 0.71  0.71  0.725 0.71  0.73  0.725 0.745 0.73  0.74  0.725 0.75  0.72\n",
      " 0.655 0.655 0.685 0.695 0.685 0.68  0.7   0.715 0.715 0.715 0.73  0.715\n",
      " 0.735 0.73  0.74  0.74  0.745 0.725 0.75  0.72  0.65  0.65  0.69  0.68\n",
      " 0.705 0.7   0.715 0.73  0.725 0.725 0.73  0.715 0.74  0.735 0.74  0.74\n",
      " 0.75  0.735 0.75  0.715 0.665 0.665 0.685 0.7   0.7   0.695 0.71  0.725\n",
      " 0.71  0.71  0.725 0.71  0.73  0.725 0.745 0.73  0.74  0.725 0.75  0.72\n",
      " 0.69  0.69  0.695 0.7   0.7   0.7   0.705 0.71  0.725 0.73  0.725 0.72\n",
      " 0.73  0.735 0.745 0.735 0.745 0.73  0.755 0.72  0.65  0.65  0.69  0.68\n",
      " 0.705 0.7   0.715 0.73  0.725 0.725 0.73  0.715 0.74  0.735 0.74  0.74\n",
      " 0.75  0.735 0.75  0.715 0.665 0.665 0.685 0.7   0.7   0.695 0.71  0.725\n",
      " 0.71  0.71  0.725 0.71  0.73  0.725 0.745 0.73  0.74  0.725 0.75  0.72\n",
      " 0.66  0.66  0.69  0.665 0.71  0.71  0.71  0.72  0.715 0.72  0.725 0.72\n",
      " 0.725 0.725 0.73  0.735 0.74  0.73  0.745 0.725 0.66  0.66  0.685 0.66\n",
      " 0.7   0.7   0.705 0.72  0.715 0.72  0.725 0.715 0.73  0.73  0.73  0.735\n",
      " 0.745 0.73  0.75  0.72  0.67  0.67  0.685 0.665 0.705 0.705 0.72  0.72\n",
      " 0.72  0.725 0.73  0.715 0.73  0.73  0.74  0.74  0.745 0.735 0.755 0.725\n",
      " 0.66  0.66  0.69  0.665 0.7   0.7   0.71  0.71  0.715 0.72  0.73  0.71\n",
      " 0.735 0.735 0.73  0.74  0.745 0.73  0.75  0.72  0.645 0.645 0.675 0.64\n",
      " 0.7   0.7   0.71  0.715 0.715 0.72  0.73  0.715 0.74  0.74  0.74  0.745\n",
      " 0.745 0.735 0.75  0.72  0.67  0.67  0.685 0.665 0.705 0.705 0.72  0.72\n",
      " 0.72  0.725 0.73  0.715 0.73  0.73  0.74  0.74  0.745 0.735 0.755 0.725\n",
      " 0.7   0.7   0.69  0.705 0.705 0.705 0.71  0.715 0.725 0.73  0.72  0.72\n",
      " 0.725 0.73  0.74  0.735 0.745 0.73  0.755 0.72  0.645 0.645 0.675 0.64\n",
      " 0.7   0.7   0.71  0.715 0.715 0.72  0.73  0.715 0.74  0.74  0.74  0.745\n",
      " 0.745 0.735 0.75  0.72  0.67  0.67  0.685 0.665 0.705 0.705 0.72  0.72\n",
      " 0.72  0.725 0.73  0.715 0.73  0.73  0.74  0.74  0.745 0.735 0.755 0.725\n",
      " 0.59  0.59  0.675 0.635 0.645 0.635 0.685 0.65  0.69  0.68  0.72  0.665\n",
      " 0.715 0.69  0.745 0.69  0.745 0.7   0.75  0.7     nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.59  0.59  0.68  0.64  0.675 0.67  0.68  0.68  0.705 0.72  0.7   0.695\n",
      " 0.7   0.71  0.725 0.71  0.735 0.71  0.735 0.715   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.685 0.685 0.69  0.685 0.7   0.7   0.69  0.705 0.715 0.72  0.725 0.72\n",
      " 0.735 0.74  0.74  0.73  0.735 0.715 0.755 0.72    nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 10, 'knn__weights': 'uniform'}.\n",
      " Score: 0.7550000000000001\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [  nan 0.725   nan 0.835   nan 0.815   nan 0.835   nan 0.84    nan 0.835\n",
      "   nan 0.88    nan 0.835   nan 0.88    nan 0.835   nan 0.725   nan 0.875\n",
      "   nan 0.835   nan 0.875   nan 0.86    nan 0.875   nan 0.885   nan 0.875\n",
      "   nan 0.88    nan 0.875   nan 0.735   nan 0.875   nan 0.835   nan 0.875\n",
      "   nan 0.86    nan 0.875   nan 0.895   nan 0.875   nan 0.895   nan 0.875\n",
      "   nan 0.715   nan 0.84    nan 0.795   nan 0.84    nan 0.825   nan 0.84\n",
      "   nan 0.87    nan 0.84    nan 0.865   nan 0.84    nan 0.725   nan 0.875\n",
      "   nan 0.825   nan 0.875   nan 0.84    nan 0.875   nan 0.88    nan 0.875\n",
      "   nan 0.875   nan 0.875   nan 0.735   nan 0.875   nan 0.835   nan 0.875\n",
      "   nan 0.86    nan 0.875   nan 0.895   nan 0.875   nan 0.895   nan 0.875\n",
      "   nan 0.705   nan 0.865   nan 0.785   nan 0.865   nan 0.815   nan 0.865\n",
      "   nan 0.87    nan 0.865   nan 0.875   nan 0.865   nan 0.725   nan 0.875\n",
      "   nan 0.825   nan 0.875   nan 0.84    nan 0.875   nan 0.88    nan 0.875\n",
      "   nan 0.875   nan 0.875   nan 0.735   nan 0.875   nan 0.835   nan 0.875\n",
      "   nan 0.86    nan 0.875   nan 0.895   nan 0.875   nan 0.895   nan 0.875\n",
      "   nan 0.725   nan 0.8     nan 0.81    nan 0.8     nan 0.84    nan 0.8\n",
      "   nan 0.88    nan 0.8     nan 0.88    nan 0.8     nan 0.73    nan 0.83\n",
      "   nan 0.835   nan 0.83    nan 0.875   nan 0.83    nan 0.88    nan 0.83\n",
      "   nan 0.88    nan 0.83    nan 0.74    nan 0.85    nan 0.835   nan 0.85\n",
      "   nan 0.87    nan 0.85    nan 0.885   nan 0.85    nan 0.88    nan 0.85\n",
      "   nan 0.72    nan 0.83    nan 0.805   nan 0.83    nan 0.825   nan 0.83\n",
      "   nan 0.87    nan 0.83    nan 0.865   nan 0.83    nan 0.725   nan 0.86\n",
      "   nan 0.815   nan 0.86    nan 0.855   nan 0.86    nan 0.875   nan 0.86\n",
      "   nan 0.875   nan 0.86    nan 0.74    nan 0.85    nan 0.835   nan 0.85\n",
      "   nan 0.87    nan 0.85    nan 0.885   nan 0.85    nan 0.88    nan 0.85\n",
      "   nan 0.705   nan 0.87    nan 0.785   nan 0.87    nan 0.81    nan 0.87\n",
      "   nan 0.865   nan 0.87    nan 0.87    nan 0.87    nan 0.725   nan 0.86\n",
      "   nan 0.815   nan 0.86    nan 0.855   nan 0.86    nan 0.875   nan 0.86\n",
      "   nan 0.875   nan 0.86    nan 0.74    nan 0.85    nan 0.835   nan 0.85\n",
      "   nan 0.87    nan 0.85    nan 0.885   nan 0.85    nan 0.88    nan 0.85\n",
      "   nan 0.685   nan 0.6     nan 0.69    nan 0.6     nan 0.69    nan 0.6\n",
      "   nan 0.725   nan 0.6     nan 0.7     nan 0.6     nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.685   nan 0.735   nan 0.685   nan 0.735   nan 0.685   nan 0.735\n",
      "   nan 0.71    nan 0.735   nan 0.715   nan 0.735   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan 0.685   nan 0.725   nan 0.69    nan 0.725   nan 0.7     nan 0.725\n",
      "   nan 0.72    nan 0.725   nan 0.72    nan 0.725   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 10, 'lr__C': 5, 'lr__penalty': 'l2'}.\n",
      " Score: 0.8949999999999999\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-soldier",
   "metadata": {},
   "source": [
    "## Lemmatization and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "necessary-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_statuses'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "X = train\n",
    "y = train.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "thrown-insulin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.88       0.75       0.85       0.85333333 0.91333333 0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.91       0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.90333333 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.91333333 0.76333333 0.87666667 0.83333333 0.89666667 0.77666667\n",
      " 0.88       0.83333333 0.89333333 0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90333333 0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.9        0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91333333 0.75666667 0.86666667 0.83333333 0.90333333 0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90333333 0.76333333 0.87666667 0.83333333 0.90333333 0.77666667\n",
      " 0.88       0.83333333 0.90333333 0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.9        0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90333333 0.76333333 0.87666667 0.83333333 0.90333333 0.77666667\n",
      " 0.88       0.83333333 0.90333333 0.77666667 0.89333333 0.83\n",
      " 0.88666667 0.75       0.85       0.85333333 0.91       0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.90333333 0.77666667\n",
      " 0.89333333 0.83       0.88333333 0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.89333333 0.77666667 0.88       0.83333333\n",
      " 0.89666667 0.77666667 0.89333333 0.83       0.88333333 0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.9        0.77666667\n",
      " 0.88       0.83333333 0.9        0.77666667 0.89333333 0.83\n",
      " 0.88333333 0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.90666667 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.91       0.76333333\n",
      " 0.87666667 0.83333333 0.90333333 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.91       0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90333333 0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.90666667 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.91333333 0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.91       0.77666667 0.89333333 0.83\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87666667 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.85666667 0.77666667 0.88       0.83       0.85333333 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87333333 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.85666667 0.77666667 0.88       0.83       0.84333333 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87666667 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.86       0.77666667 0.88       0.83       0.85666667 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 0.5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9133333333333333\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.82333333 0.82333333 0.78       0.82333333 0.88666667 0.87666667\n",
      " 0.87333333 0.87333333 0.89333333 0.89       0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78       0.81666667\n",
      " 0.88666667 0.87666667 0.87333333 0.87333333 0.89333333 0.89\n",
      " 0.89       0.9        0.88666667 0.89333333 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.81666667 0.81666667\n",
      " 0.78       0.81666667 0.88666667 0.87666667 0.87333333 0.87333333\n",
      " 0.89333333 0.89       0.89       0.9        0.88666667 0.89333333\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.9\n",
      " 0.82       0.82       0.78       0.82       0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89       0.89       0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78333333 0.81666667\n",
      " 0.88666667 0.87666667 0.87       0.87333333 0.89333333 0.89\n",
      " 0.89       0.89666667 0.88666667 0.89333333 0.88666667 0.89333333\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.82       0.82\n",
      " 0.78       0.82       0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89       0.89       0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.88666667 0.89333333 0.89666667 0.90333333 0.89       0.9\n",
      " 0.81333333 0.81333333 0.78       0.81333333 0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89333333 0.89333333 0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.9        0.82       0.82       0.77666667 0.82\n",
      " 0.88666667 0.87666667 0.87       0.87333333 0.89333333 0.89333333\n",
      " 0.89       0.89666667 0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.89666667 0.90333333 0.89       0.9        0.82       0.82\n",
      " 0.78       0.82       0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89       0.89       0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.88666667 0.89333333 0.89666667 0.90333333 0.89       0.9\n",
      " 0.81333333 0.81333333 0.78       0.81333333 0.88666667 0.87666667\n",
      " 0.87       0.87666667 0.89       0.89       0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89666667 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78       0.81666667\n",
      " 0.88666667 0.87666667 0.87       0.87666667 0.89       0.89\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.82333333 0.82333333\n",
      " 0.78       0.82333333 0.89       0.88       0.87333333 0.88\n",
      " 0.89333333 0.89       0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.90333333\n",
      " 0.81333333 0.81333333 0.77666667 0.81333333 0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89333333 0.89       0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89666667 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81333333 0.81333333 0.78       0.81333333\n",
      " 0.88666667 0.87666667 0.87       0.87333333 0.89333333 0.89\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.81333333 0.81333333\n",
      " 0.77666667 0.81333333 0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89333333 0.89       0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.90333333\n",
      " 0.81666667 0.81666667 0.77333333 0.81666667 0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89333333 0.89       0.89       0.89666667\n",
      " 0.89       0.89666667 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78       0.81666667\n",
      " 0.88666667 0.87666667 0.87333333 0.87333333 0.89333333 0.89\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.81333333 0.81333333\n",
      " 0.77666667 0.81333333 0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89333333 0.89       0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.90333333\n",
      " 0.79666667 0.79666667 0.77333333 0.79666667 0.89       0.87333333\n",
      " 0.87666667 0.86666667 0.89333333 0.88       0.88666667 0.88666667\n",
      " 0.88666667 0.88333333 0.88333333 0.88666667 0.89666667 0.89666667\n",
      " 0.89       0.89333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8        0.8        0.78       0.8        0.89333333 0.88\n",
      " 0.87666667 0.87       0.89333333 0.88666667 0.88666667 0.89333333\n",
      " 0.89       0.89333333 0.88333333 0.89       0.89666667 0.9\n",
      " 0.89       0.89666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82       0.82       0.77666667 0.82       0.88666667 0.87333333\n",
      " 0.87       0.87       0.9        0.89333333 0.89       0.89333333\n",
      " 0.89       0.89333333 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}.\n",
      " Score: 0.9033333333333333\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.87333333        nan 0.89333333        nan 0.88333333\n",
      "        nan 0.89333333        nan 0.90666667        nan 0.89333333\n",
      "        nan 0.91              nan 0.89333333        nan 0.90333333\n",
      "        nan 0.89333333        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.88333333        nan 0.90333333        nan 0.90333333\n",
      "        nan 0.90333333        nan 0.91              nan 0.90333333\n",
      "        nan 0.90666667        nan 0.90333333        nan 0.87333333\n",
      "        nan 0.9               nan 0.88333333        nan 0.9\n",
      "        nan 0.90333333        nan 0.9               nan 0.91\n",
      "        nan 0.9               nan 0.90333333        nan 0.9\n",
      "        nan 0.87              nan 0.88666667        nan 0.88333333\n",
      "        nan 0.88666667        nan 0.9               nan 0.88666667\n",
      "        nan 0.91              nan 0.88666667        nan 0.90666667\n",
      "        nan 0.88666667        nan 0.87              nan 0.89\n",
      "        nan 0.88333333        nan 0.89              nan 0.9\n",
      "        nan 0.89              nan 0.91              nan 0.89\n",
      "        nan 0.90666667        nan 0.89              nan 0.87\n",
      "        nan 0.89              nan 0.88333333        nan 0.89\n",
      "        nan 0.90333333        nan 0.89              nan 0.91333333\n",
      "        nan 0.89              nan 0.90333333        nan 0.89\n",
      "        nan 0.87              nan 0.90333333        nan 0.88333333\n",
      "        nan 0.90333333        nan 0.88666667        nan 0.90333333\n",
      "        nan 0.91              nan 0.90333333        nan 0.91333333\n",
      "        nan 0.90333333        nan 0.87              nan 0.91\n",
      "        nan 0.88333333        nan 0.91              nan 0.89333333\n",
      "        nan 0.91              nan 0.91666667        nan 0.91\n",
      "        nan 0.91              nan 0.91              nan 0.87\n",
      "        nan 0.89              nan 0.88333333        nan 0.89\n",
      "        nan 0.90333333        nan 0.89              nan 0.91333333\n",
      "        nan 0.89              nan 0.90333333        nan 0.89\n",
      "        nan 0.87333333        nan 0.9               nan 0.9\n",
      "        nan 0.9               nan 0.90666667        nan 0.9\n",
      "        nan 0.91              nan 0.9               nan 0.91\n",
      "        nan 0.9               nan 0.87333333        nan 0.87666667\n",
      "        nan 0.89666667        nan 0.87666667        nan 0.90666667\n",
      "        nan 0.87666667        nan 0.91              nan 0.87666667\n",
      "        nan 0.90666667        nan 0.87666667        nan 0.87333333\n",
      "        nan 0.89              nan 0.89333333        nan 0.89\n",
      "        nan 0.90666667        nan 0.89              nan 0.91333333\n",
      "        nan 0.89              nan 0.91333333        nan 0.89\n",
      "        nan 0.87333333        nan 0.89              nan 0.89666667\n",
      "        nan 0.89              nan 0.90333333        nan 0.89\n",
      "        nan 0.91              nan 0.89              nan 0.91\n",
      "        nan 0.89              nan 0.87333333        nan 0.87333333\n",
      "        nan 0.89333333        nan 0.87333333        nan 0.90666667\n",
      "        nan 0.87333333        nan 0.91333333        nan 0.87333333\n",
      "        nan 0.90666667        nan 0.87333333        nan 0.87333333\n",
      "        nan 0.90666667        nan 0.89              nan 0.90666667\n",
      "        nan 0.90666667        nan 0.90666667        nan 0.91\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.87333333        nan 0.88333333        nan 0.88333333\n",
      "        nan 0.88333333        nan 0.89666667        nan 0.88333333\n",
      "        nan 0.90666667        nan 0.88333333        nan 0.91333333\n",
      "        nan 0.88333333        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.88666667        nan 0.90333333        nan 0.90666667\n",
      "        nan 0.90333333        nan 0.91666667        nan 0.90333333\n",
      "        nan 0.91333333        nan 0.90333333        nan 0.87333333\n",
      "        nan 0.90666667        nan 0.89              nan 0.90666667\n",
      "        nan 0.90666667        nan 0.90666667        nan 0.91\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.86666667        nan 0.78666667        nan 0.86666667\n",
      "        nan 0.78666667        nan 0.86666667        nan 0.78666667\n",
      "        nan 0.87              nan 0.78666667        nan 0.87\n",
      "        nan 0.78666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.77333333        nan 0.86666667\n",
      "        nan 0.77333333        nan 0.87              nan 0.77333333\n",
      "        nan 0.86666667        nan 0.77333333        nan 0.86666667\n",
      "        nan 0.77333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.79666667        nan 0.86666667\n",
      "        nan 0.79666667        nan 0.86666667        nan 0.79666667\n",
      "        nan 0.87              nan 0.79666667        nan 0.87\n",
      "        nan 0.79666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 5, 'lr__penalty': 'l2'}.\n",
      " Score: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_politician_count_1000', 'followers_politician_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-marble",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "southwest-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "transsexual-generation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('numeric_features',\n",
       "                                                 Pipeline(steps=[('selector_num',\n",
       "                                                                  FunctionTransformer(func=<function get_numeric_data_ at 0x7f906c32caf0>))])),\n",
       "                                                ('text_features',\n",
       "                                                 Pipeline(steps=[('selector_text',\n",
       "                                                                  FunctionTransformer(func=<function get_text_data_ at 0x7f906c32c280>)),\n",
       "                                                                 ('vectorizer',\n",
       "                                                                  TfidfVectorizer(max_df=0.5,\n",
       "                                                                                  min_df=5))]))])),\n",
       "                ('lr', LogisticRegression(C=5, max_iter=1000))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    df = df.copy()\n",
    "    df['textdata'] = clean_text(df['name']+ ' ' + df['description'] + ' ' + df['recent_100_statuses'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: tokenize_lemmatize_en(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_hotel_count_1000', 'followers_hotel_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=None, min_df=5)),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000, penalty='l2',  C=5))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "driven-respondent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../classifiers/classifier_hotel_ndtfrfo.sav']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../classifiers/classifier_hotel_ndtfrfo.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-cleaners",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
