{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "german-marsh",
   "metadata": {},
   "source": [
    "# Classification Experiment: Friends\n",
    "---\n",
    "This Notebook, includes a series of experiments, on using a node's Friends for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-archives",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cubic-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "import time \n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-replica",
   "metadata": {},
   "source": [
    "Twitter API Authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simple-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_credentials = []\n",
    "with open('../../../../twitter_credentials.json', 'r') as f:\n",
    "    twitter_credentials = json.load(f)\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_credentials['consumer_key'], twitter_credentials['consumer_secret'])\n",
    "auth.set_access_token(twitter_credentials['access_token_key'],twitter_credentials['access_token_secret'])\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, timeout=60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mediterranean-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function For Text Normalization\n",
    "def clean_text(data):\n",
    "    urls = r'http\\S+'\n",
    "    non_unicode_char = r'\\W'\n",
    "    numbers = r'[0-9_]'\n",
    "    fix_whitespace = r'\\s+'\n",
    "    single_whitespace = ' '\n",
    "    \n",
    "    data = (data.replace([urls], single_whitespace, regex=True)\n",
    "                    .replace([non_unicode_char, numbers], single_whitespace, regex=True)\n",
    "                    .replace(fix_whitespace, single_whitespace, regex=True))\n",
    "    data = data.apply(lambda s: s.lower() if type(s) == str else s)\n",
    "    return data\n",
    "\n",
    "nlp_el = spacy.load('el_core_news_md')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "STOPWORDS = set(list(spacy.lang.en.STOP_WORDS) + list(spacy.lang.el.STOP_WORDS))\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    row = [str(token) for token in nlp_el(row)]\n",
    "    return [w for w in row if w not in STOPWORDS]\n",
    "\n",
    "def tokenize_lemmatize(row):\n",
    "    return [str(token.lemma_) for token in nlp_el(row)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-alexandria",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "positive-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training Set\n",
    "training_set = pd.read_csv('../../../../datasets/Greek Politicians/classification/parliament_members_training_set.csv')\n",
    "training_set = training_set.replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "growing-sailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>recent_100_tweets</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>recent_100_friends_nd</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>parliament_member</th>\n",
       "      <th>friends_politician_count_1000</th>\n",
       "      <th>followers_politician_count_1000</th>\n",
       "      <th>mentions_politician_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>manoskonsolas</td>\n",
       "      <td>Manos Konsolas</td>\n",
       "      <td>Βουλευτής Δωδεκανήσου - Καθηγητής Πανεπιστημίο...</td>\n",
       "      <td>5467</td>\n",
       "      <td>Λανθασμένη επιλογή η αναστολή λειτουργίας των...</td>\n",
       "      <td>1534</td>\n",
       "      <td>Kostas Tsiaras Υπουργός Δικαιοσύνης &amp; Βουλευτ...</td>\n",
       "      <td>3757</td>\n",
       "      <td>False</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>vnaypsilantis</td>\n",
       "      <td>Vassilis Ypsilantis</td>\n",
       "      <td>Α’ Κοσμήτορας της Βουλής των Ελλήνων | Βουλευτ...</td>\n",
       "      <td>1240</td>\n",
       "      <td>RT @grigoris_d: Corriere della Sera: Το Καστε...</td>\n",
       "      <td>582</td>\n",
       "      <td>Ιωάννης Μπούγας ΙΩΑΝΝΗΣ Δ ΜΠΟΥΓΑΣ \\nΔικηγόρος...</td>\n",
       "      <td>956</td>\n",
       "      <td>False</td>\n",
       "      <td>676</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>FofiGennimata</td>\n",
       "      <td>Fofi Gennimata</td>\n",
       "      <td>Πρόεδρος του Κινήματος Αλλαγής | Πρόεδρος του ...</td>\n",
       "      <td>9248</td>\n",
       "      <td>Η Μελίνα των οραμάτων, των μαρμάρων, του πολι...</td>\n",
       "      <td>2380</td>\n",
       "      <td>Τομέας Απόδημου Ελληνισμού - Κίνημα Αλλαγής  ...</td>\n",
       "      <td>45893</td>\n",
       "      <td>False</td>\n",
       "      <td>421</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>ChitasKostas</td>\n",
       "      <td>Chitas Constantinos</td>\n",
       "      <td>Βουλευτής Β´ Θεσσαλονίκης       Ελληνική Λύση ...</td>\n",
       "      <td>5632</td>\n",
       "      <td>Σήμερα στις 12:00 στον «αέρα» του @FOCUSFM103...</td>\n",
       "      <td>599</td>\n",
       "      <td>Ελληνες Ενωμένοι επιτέλους Μακεδόνισσα λατρεύ...</td>\n",
       "      <td>997</td>\n",
       "      <td>False</td>\n",
       "      <td>12328</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>KastanidisHaris</td>\n",
       "      <td>Kastanidis Haris</td>\n",
       "      <td>Όλη η ζωή είναι μια ξένη χώρα.\\r\\nJack Kerouac</td>\n",
       "      <td>4117</td>\n",
       "      <td>Οι μικρές και μεσαίες επιχειρήσεις αδυνατούν ...</td>\n",
       "      <td>700</td>\n",
       "      <td>Andreas Spiropoulos Μηχανικός, Μέλος Πολιτικο...</td>\n",
       "      <td>3841</td>\n",
       "      <td>False</td>\n",
       "      <td>1281</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         screen_name                 name  \\\n",
       "295    manoskonsolas       Manos Konsolas   \n",
       "296    vnaypsilantis  Vassilis Ypsilantis   \n",
       "297    FofiGennimata       Fofi Gennimata   \n",
       "298     ChitasKostas  Chitas Constantinos   \n",
       "299  KastanidisHaris     Kastanidis Haris   \n",
       "\n",
       "                                           description  statuses_count  \\\n",
       "295  Βουλευτής Δωδεκανήσου - Καθηγητής Πανεπιστημίο...            5467   \n",
       "296  Α’ Κοσμήτορας της Βουλής των Ελλήνων | Βουλευτ...            1240   \n",
       "297  Πρόεδρος του Κινήματος Αλλαγής | Πρόεδρος του ...            9248   \n",
       "298  Βουλευτής Β´ Θεσσαλονίκης       Ελληνική Λύση ...            5632   \n",
       "299     Όλη η ζωή είναι μια ξένη χώρα.\\r\\nJack Kerouac            4117   \n",
       "\n",
       "                                     recent_100_tweets  friends_count  \\\n",
       "295   Λανθασμένη επιλογή η αναστολή λειτουργίας των...           1534   \n",
       "296   RT @grigoris_d: Corriere della Sera: Το Καστε...            582   \n",
       "297   Η Μελίνα των οραμάτων, των μαρμάρων, του πολι...           2380   \n",
       "298   Σήμερα στις 12:00 στον «αέρα» του @FOCUSFM103...            599   \n",
       "299   Οι μικρές και μεσαίες επιχειρήσεις αδυνατούν ...            700   \n",
       "\n",
       "                                 recent_100_friends_nd  followers_count  \\\n",
       "295   Kostas Tsiaras Υπουργός Δικαιοσύνης & Βουλευτ...             3757   \n",
       "296   Ιωάννης Μπούγας ΙΩΑΝΝΗΣ Δ ΜΠΟΥΓΑΣ \\nΔικηγόρος...              956   \n",
       "297   Τομέας Απόδημου Ελληνισμού - Κίνημα Αλλαγής  ...            45893   \n",
       "298   Ελληνες Ενωμένοι επιτέλους Μακεδόνισσα λατρεύ...              997   \n",
       "299   Andreas Spiropoulos Μηχανικός, Μέλος Πολιτικο...             3841   \n",
       "\n",
       "     default_profile_image  favourites_count  parliament_member  \\\n",
       "295                  False               151                  1   \n",
       "296                  False               676                  1   \n",
       "297                  False               421                  1   \n",
       "298                  False             12328                  1   \n",
       "299                  False              1281                  1   \n",
       "\n",
       "     friends_politician_count_1000  followers_politician_count_1000  \\\n",
       "295                            101                             10.0   \n",
       "296                            122                             11.0   \n",
       "297                             91                              2.0   \n",
       "298                             51                              2.0   \n",
       "299                             74                              6.0   \n",
       "\n",
       "     mentions_politician_count  \n",
       "295                          1  \n",
       "296                         16  \n",
       "297                          0  \n",
       "298                         66  \n",
       "299                          3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-porter",
   "metadata": {},
   "source": [
    "# Only Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "auburn-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exclusive-commerce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      " Best Params: {'svm__C': 0.5, 'svm__kernel': 'rbf'}.\n",
      " Score: 0.8666666666666668\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.79333333 0.79333333 0.76333333 0.80666667\n",
      " 0.86333333 0.84       0.86       0.85       0.87333333 0.84666667\n",
      " 0.86666667 0.85       0.87666667 0.85       0.87       0.85\n",
      " 0.87333333 0.85333333 0.87       0.85333333 0.87       0.85333333\n",
      " 0.87       0.85333333 0.87       0.85333333 0.86666667 0.85333333\n",
      " 0.87       0.85333333 0.86666667 0.85333333 0.87       0.85333333\n",
      " 0.86666667 0.85333333 0.87       0.85333333]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'knn__n_neighbors': 7, 'knn__weights': 'uniform'}.\n",
      " Score: 0.8766666666666667\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      " Best Params: {'lr__C': 0.1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.8433333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.84333333        nan        nan 0.84333333        nan\n",
      "        nan 0.84333333        nan        nan 0.84333333        nan\n",
      "        nan 0.84333333        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df['friends_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'knn__n_neighbors': [i for i in range(20)],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-return",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "given-preliminary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 FunctionTransformer(func=<function get_data_parl_fr at 0x7fab5967fb80>)),\n",
       "                ('knn', KNeighborsClassifier(n_neighbors=7))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member\n",
    "\n",
    "# Function to select the data\n",
    "def get_data_parl_fr(df):\n",
    "    data = df['friends_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_parl_fr)\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=7, weights='uniform'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alike-office",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_parl_fr.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'classifier_parl_fr.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-wesley",
   "metadata": {},
   "source": [
    "# Name Description Tweets and Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-elevation",
   "metadata": {},
   "source": [
    "## Without NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "heavy-virginia",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_tweets'])\n",
    "X = train\n",
    "y = train.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "respective-discrimination",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.87       0.75       0.84666667 0.85       0.88666667 0.76\n",
      " 0.86       0.83666667 0.89333333 0.76       0.87       0.83333333\n",
      " 0.89666667 0.77       0.86666667 0.83333333 0.89666667 0.77\n",
      " 0.87       0.83       0.87       0.75       0.84666667 0.85\n",
      " 0.89333333 0.76       0.86       0.83666667 0.90333333 0.76\n",
      " 0.87       0.83333333 0.89333333 0.77       0.86666667 0.83333333\n",
      " 0.89       0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88666667 0.76       0.86       0.83666667\n",
      " 0.89333333 0.76       0.87       0.83333333 0.88       0.77\n",
      " 0.86666667 0.83333333 0.87666667 0.77       0.87       0.83\n",
      " 0.86666667 0.75       0.84666667 0.85       0.88666667 0.76\n",
      " 0.86       0.83666667 0.89666667 0.76       0.87       0.83333333\n",
      " 0.89       0.77       0.86666667 0.83333333 0.89       0.77\n",
      " 0.87       0.83       0.87       0.75       0.84666667 0.85\n",
      " 0.88333333 0.76       0.86       0.83666667 0.89666667 0.76\n",
      " 0.87       0.83333333 0.89       0.77       0.86666667 0.83333333\n",
      " 0.89       0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.87666667 0.76       0.86       0.83666667\n",
      " 0.89333333 0.76       0.87       0.83333333 0.88333333 0.77\n",
      " 0.86666667 0.83333333 0.88666667 0.77       0.87       0.83\n",
      " 0.85666667 0.75       0.84666667 0.85       0.87666667 0.76\n",
      " 0.86       0.83666667 0.89333333 0.76       0.87       0.83333333\n",
      " 0.89333333 0.77       0.86666667 0.83333333 0.89333333 0.77\n",
      " 0.87       0.83       0.86333333 0.75       0.84666667 0.85\n",
      " 0.88666667 0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.88333333 0.77       0.86666667 0.83333333\n",
      " 0.88       0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.87666667 0.76       0.86       0.83666667\n",
      " 0.89       0.76       0.87       0.83333333 0.88333333 0.77\n",
      " 0.86666667 0.83333333 0.88666667 0.77       0.87       0.83\n",
      " 0.87333333 0.75       0.84666667 0.85       0.89666667 0.76\n",
      " 0.86       0.83666667 0.91333333 0.76       0.87       0.83333333\n",
      " 0.89666667 0.77       0.86666667 0.83333333 0.89333333 0.77\n",
      " 0.87       0.83       0.87333333 0.75       0.84666667 0.85\n",
      " 0.89666667 0.76       0.86       0.83666667 0.91       0.76\n",
      " 0.87       0.83333333 0.89       0.77       0.86666667 0.83333333\n",
      " 0.89       0.77       0.87       0.83       0.87333333 0.75\n",
      " 0.84666667 0.85       0.89       0.76       0.86       0.83666667\n",
      " 0.9        0.76       0.87       0.83333333 0.89666667 0.77\n",
      " 0.86666667 0.83333333 0.88666667 0.77       0.87       0.83\n",
      " 0.87333333 0.75       0.84666667 0.85       0.89       0.76\n",
      " 0.86       0.83666667 0.90333333 0.76       0.87       0.83333333\n",
      " 0.9        0.77       0.86666667 0.83333333 0.90333333 0.77\n",
      " 0.87       0.83       0.87333333 0.75       0.84666667 0.85\n",
      " 0.89666667 0.76       0.86       0.83666667 0.90333333 0.76\n",
      " 0.87       0.83333333 0.89333333 0.77       0.86666667 0.83333333\n",
      " 0.89333333 0.77       0.87       0.83       0.87333333 0.75\n",
      " 0.84666667 0.85       0.89       0.76       0.86       0.83666667\n",
      " 0.9        0.76       0.87       0.83333333 0.88333333 0.77\n",
      " 0.86666667 0.83333333 0.88333333 0.77       0.87       0.83\n",
      " 0.86       0.75       0.84666667 0.85       0.88       0.76\n",
      " 0.86       0.83666667 0.9        0.76       0.87       0.83333333\n",
      " 0.89666667 0.77       0.86666667 0.83333333 0.89666667 0.77\n",
      " 0.87       0.83       0.87       0.75       0.84666667 0.85\n",
      " 0.89666667 0.76       0.86       0.83666667 0.90333333 0.76\n",
      " 0.87       0.83333333 0.89       0.77       0.86666667 0.83333333\n",
      " 0.89       0.77       0.87       0.83       0.87333333 0.75\n",
      " 0.84666667 0.85       0.89       0.76       0.86       0.83666667\n",
      " 0.9        0.76       0.87       0.83333333 0.88333333 0.77\n",
      " 0.86666667 0.83333333 0.88333333 0.77       0.87       0.83\n",
      " 0.84333333 0.75       0.84666667 0.85       0.85       0.76\n",
      " 0.86       0.83333333 0.85333333 0.76       0.87       0.83333333\n",
      " 0.85       0.77       0.86666667 0.83333333 0.85333333 0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84333333 0.75       0.84666667 0.85       0.84666667 0.76\n",
      " 0.86       0.83333333 0.85333333 0.76       0.87       0.83333333\n",
      " 0.85333333 0.77       0.86666667 0.83333333 0.86       0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84333333 0.75       0.84666667 0.85       0.85       0.76\n",
      " 0.86       0.83333333 0.86       0.76       0.87       0.83333333\n",
      " 0.84666667 0.77       0.86666667 0.83333333 0.84333333 0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9133333333333333\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.83       0.83       0.79666667 0.83       0.87       0.86666667\n",
      " 0.86333333 0.86666667 0.87333333 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667 0.83333333 0.83333333 0.79333333 0.83333333\n",
      " 0.86666667 0.86333333 0.86333333 0.86666667 0.87333333 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.87666667 0.83333333 0.83333333\n",
      " 0.79333333 0.83333333 0.87       0.86666667 0.86333333 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83       0.83       0.79333333 0.83       0.87333333 0.87\n",
      " 0.86333333 0.86666667 0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83       0.83       0.79333333 0.83\n",
      " 0.87       0.86666667 0.86333333 0.86666667 0.87333333 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83333333 0.83333333\n",
      " 0.79       0.83333333 0.87       0.86666667 0.86333333 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83666667 0.83666667 0.79333333 0.83666667 0.87666667 0.87333333\n",
      " 0.86666667 0.86666667 0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.88       0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667 0.83333333 0.83333333 0.79333333 0.83333333\n",
      " 0.87       0.86666667 0.86333333 0.86666667 0.87666667 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83333333 0.83333333\n",
      " 0.79       0.83333333 0.87       0.86666667 0.86333333 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.82666667 0.82666667 0.80666667 0.82666667 0.87333333 0.87\n",
      " 0.86       0.86666667 0.87       0.87666667 0.86666667 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83       0.83       0.80666667 0.83\n",
      " 0.87       0.86666667 0.86       0.86666667 0.87       0.87666667\n",
      " 0.86666667 0.87333333 0.87666667 0.87333333 0.87       0.87666667\n",
      " 0.87666667 0.88       0.87       0.88       0.84       0.84\n",
      " 0.79333333 0.84       0.87       0.86666667 0.86       0.86666667\n",
      " 0.87       0.87666667 0.86666667 0.87333333 0.87666667 0.87333333\n",
      " 0.87       0.87666667 0.87666667 0.88       0.87       0.88\n",
      " 0.83       0.83       0.8        0.83       0.87333333 0.87\n",
      " 0.86       0.86666667 0.87333333 0.87666667 0.86333333 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83333333 0.83333333 0.80666667 0.83333333\n",
      " 0.87       0.86666667 0.86       0.86666667 0.87       0.87666667\n",
      " 0.86333333 0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83666667 0.83666667\n",
      " 0.79333333 0.83666667 0.87       0.86666667 0.86       0.86666667\n",
      " 0.87       0.87666667 0.86666667 0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83666667 0.83666667 0.79333333 0.83666667 0.87666667 0.87333333\n",
      " 0.86       0.86666667 0.87666667 0.87666667 0.86333333 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83       0.83       0.8        0.83\n",
      " 0.87       0.86666667 0.86       0.86666667 0.87       0.87666667\n",
      " 0.86333333 0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83666667 0.83666667\n",
      " 0.79333333 0.83666667 0.87       0.86666667 0.86       0.86666667\n",
      " 0.87       0.87666667 0.86666667 0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.78       0.78       0.78666667 0.78       0.85333333 0.84333333\n",
      " 0.86333333 0.84333333 0.87333333 0.86666667 0.87       0.86666667\n",
      " 0.87333333 0.86666667 0.87       0.87       0.87333333 0.87333333\n",
      " 0.87       0.87333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.79       0.79       0.77666667 0.79       0.85666667 0.85666667\n",
      " 0.85666667 0.85666667 0.87333333 0.87333333 0.87333333 0.87\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81333333 0.81333333 0.77666667 0.81333333 0.87       0.87\n",
      " 0.86666667 0.86666667 0.87333333 0.87333333 0.87333333 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}.\n",
      " Score: 0.8800000000000001\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.85666667        nan 0.9               nan 0.87666667\n",
      "        nan 0.9               nan 0.87666667        nan 0.9\n",
      "        nan 0.91              nan 0.9               nan 0.91333333\n",
      "        nan 0.9               nan 0.85666667        nan 0.90333333\n",
      "        nan 0.87666667        nan 0.90333333        nan 0.87666667\n",
      "        nan 0.90333333        nan 0.90666667        nan 0.90333333\n",
      "        nan 0.90333333        nan 0.90333333        nan 0.85666667\n",
      "        nan 0.91              nan 0.87666667        nan 0.91\n",
      "        nan 0.87666667        nan 0.91              nan 0.90333333\n",
      "        nan 0.91              nan 0.90666667        nan 0.91\n",
      "        nan 0.85333333        nan 0.91333333        nan 0.87333333\n",
      "        nan 0.91333333        nan 0.87666667        nan 0.91333333\n",
      "        nan 0.90666667        nan 0.91333333        nan 0.9\n",
      "        nan 0.91333333        nan 0.85666667        nan 0.9\n",
      "        nan 0.87333333        nan 0.9               nan 0.87666667\n",
      "        nan 0.9               nan 0.90666667        nan 0.9\n",
      "        nan 0.90666667        nan 0.9               nan 0.85666667\n",
      "        nan 0.89333333        nan 0.87666667        nan 0.89333333\n",
      "        nan 0.87666667        nan 0.89333333        nan 0.90333333\n",
      "        nan 0.89333333        nan 0.89333333        nan 0.89333333\n",
      "        nan 0.85              nan 0.91              nan 0.86333333\n",
      "        nan 0.91              nan 0.87              nan 0.91\n",
      "        nan 0.88666667        nan 0.91              nan 0.89\n",
      "        nan 0.91              nan 0.85333333        nan 0.91333333\n",
      "        nan 0.87333333        nan 0.91333333        nan 0.87666667\n",
      "        nan 0.91333333        nan 0.89666667        nan 0.91333333\n",
      "        nan 0.90333333        nan 0.91333333        nan 0.85666667\n",
      "        nan 0.89              nan 0.87666667        nan 0.89\n",
      "        nan 0.87666667        nan 0.89              nan 0.90333333\n",
      "        nan 0.89              nan 0.89666667        nan 0.89\n",
      "        nan 0.86              nan 0.90333333        nan 0.87333333\n",
      "        nan 0.90333333        nan 0.89333333        nan 0.90333333\n",
      "        nan 0.90666667        nan 0.90333333        nan 0.91\n",
      "        nan 0.90333333        nan 0.86              nan 0.91\n",
      "        nan 0.87333333        nan 0.91              nan 0.89333333\n",
      "        nan 0.91              nan 0.90333333        nan 0.91\n",
      "        nan 0.90666667        nan 0.91              nan 0.86\n",
      "        nan 0.90333333        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.89              nan 0.90333333        nan 0.89333333\n",
      "        nan 0.90333333        nan 0.89666667        nan 0.90333333\n",
      "        nan 0.85666667        nan 0.91              nan 0.87333333\n",
      "        nan 0.91              nan 0.88666667        nan 0.91\n",
      "        nan 0.89666667        nan 0.91              nan 0.9\n",
      "        nan 0.91              nan 0.85666667        nan 0.9\n",
      "        nan 0.87333333        nan 0.9               nan 0.89333333\n",
      "        nan 0.9               nan 0.90333333        nan 0.9\n",
      "        nan 0.90333333        nan 0.9               nan 0.85666667\n",
      "        nan 0.91333333        nan 0.87333333        nan 0.91333333\n",
      "        nan 0.88666667        nan 0.91333333        nan 0.89333333\n",
      "        nan 0.91333333        nan 0.89666667        nan 0.91333333\n",
      "        nan 0.85              nan 0.90666667        nan 0.87\n",
      "        nan 0.90666667        nan 0.87333333        nan 0.90666667\n",
      "        nan 0.89666667        nan 0.90666667        nan 0.89666667\n",
      "        nan 0.90666667        nan 0.85666667        nan 0.91\n",
      "        nan 0.87333333        nan 0.91              nan 0.88\n",
      "        nan 0.91              nan 0.90333333        nan 0.91\n",
      "        nan 0.90333333        nan 0.91              nan 0.85666667\n",
      "        nan 0.90333333        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.88666667        nan 0.90333333        nan 0.89333333\n",
      "        nan 0.90333333        nan 0.89666667        nan 0.90333333\n",
      "        nan 0.84666667        nan 0.76666667        nan 0.84333333\n",
      "        nan 0.76666667        nan 0.84666667        nan 0.76666667\n",
      "        nan 0.86333333        nan 0.76666667        nan 0.86333333\n",
      "        nan 0.76666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84666667        nan 0.78333333        nan 0.84666667\n",
      "        nan 0.78333333        nan 0.85              nan 0.78333333\n",
      "        nan 0.85333333        nan 0.78333333        nan 0.87\n",
      "        nan 0.78333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84666667        nan 0.79666667        nan 0.85\n",
      "        nan 0.79666667        nan 0.85333333        nan 0.79666667\n",
      "        nan 0.85666667        nan 0.79666667        nan 0.86\n",
      "        nan 0.79666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.9133333333333334\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['friends_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-prince",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "successful-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_tweets'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "\n",
    "X = train\n",
    "y = train.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noble-shakespeare",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.86666667 0.75       0.84666667 0.85       0.88333333 0.76\n",
      " 0.86       0.83666667 0.90333333 0.76       0.87       0.83333333\n",
      " 0.91666667 0.77       0.86666667 0.83333333 0.90666667 0.77\n",
      " 0.87       0.83       0.87       0.75       0.84666667 0.85\n",
      " 0.89       0.76       0.86       0.83666667 0.90666667 0.76\n",
      " 0.87       0.83333333 0.9        0.77       0.86666667 0.83333333\n",
      " 0.9        0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88333333 0.76       0.86       0.83666667\n",
      " 0.90333333 0.76       0.87       0.83333333 0.9        0.77\n",
      " 0.86666667 0.83333333 0.90333333 0.77       0.87       0.83\n",
      " 0.86333333 0.75       0.84666667 0.85       0.88666667 0.76\n",
      " 0.86       0.83666667 0.89666667 0.76       0.87       0.83333333\n",
      " 0.90333333 0.77       0.86666667 0.83333333 0.9        0.77\n",
      " 0.87       0.83       0.86666667 0.75       0.84666667 0.85\n",
      " 0.88333333 0.76       0.86       0.83666667 0.89666667 0.76\n",
      " 0.87       0.83333333 0.9        0.77       0.86666667 0.83333333\n",
      " 0.9        0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88       0.76       0.86       0.83666667\n",
      " 0.89666667 0.76       0.87       0.83333333 0.88666667 0.77\n",
      " 0.86666667 0.83333333 0.89       0.77       0.87       0.83\n",
      " 0.85333333 0.75       0.84666667 0.85       0.87333333 0.76\n",
      " 0.86       0.83666667 0.89333333 0.76       0.87       0.83333333\n",
      " 0.89       0.77       0.86666667 0.83333333 0.89       0.77\n",
      " 0.87       0.83       0.86       0.75       0.84666667 0.85\n",
      " 0.89       0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.89       0.77       0.86666667 0.83333333\n",
      " 0.89       0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88       0.76       0.86       0.83666667\n",
      " 0.89666667 0.76       0.87       0.83333333 0.88666667 0.77\n",
      " 0.86666667 0.83333333 0.89       0.77       0.87       0.83\n",
      " 0.86666667 0.75       0.84666667 0.85       0.88666667 0.76\n",
      " 0.86       0.83666667 0.91       0.76       0.87       0.83333333\n",
      " 0.91666667 0.77       0.86666667 0.83333333 0.90666667 0.77\n",
      " 0.87       0.83       0.87       0.75       0.84666667 0.85\n",
      " 0.89       0.76       0.86       0.83666667 0.90666667 0.76\n",
      " 0.87       0.83333333 0.90333333 0.77       0.86666667 0.83333333\n",
      " 0.90333333 0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88333333 0.76       0.86       0.83666667\n",
      " 0.90666667 0.76       0.87       0.83333333 0.90333333 0.77\n",
      " 0.86666667 0.83333333 0.90666667 0.77       0.87       0.83\n",
      " 0.86666667 0.75       0.84666667 0.85       0.88666667 0.76\n",
      " 0.86       0.83666667 0.9        0.76       0.87       0.83333333\n",
      " 0.90333333 0.77       0.86666667 0.83333333 0.9        0.77\n",
      " 0.87       0.83       0.86666667 0.75       0.84666667 0.85\n",
      " 0.88333333 0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.89666667 0.77       0.86666667 0.83333333\n",
      " 0.9        0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88       0.76       0.86       0.83666667\n",
      " 0.89666667 0.76       0.87       0.83333333 0.89333333 0.77\n",
      " 0.86666667 0.83333333 0.9        0.77       0.87       0.83\n",
      " 0.86       0.75       0.84666667 0.85       0.87666667 0.76\n",
      " 0.86       0.83666667 0.89       0.76       0.87       0.83333333\n",
      " 0.89       0.77       0.86666667 0.83333333 0.89       0.77\n",
      " 0.87       0.83       0.86333333 0.75       0.84666667 0.85\n",
      " 0.89       0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.9        0.77       0.86666667 0.83333333\n",
      " 0.9        0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88       0.76       0.86       0.83666667\n",
      " 0.89666667 0.76       0.87       0.83333333 0.89333333 0.77\n",
      " 0.86666667 0.83333333 0.9        0.77       0.87       0.83\n",
      " 0.84333333 0.75       0.84666667 0.85       0.85666667 0.76\n",
      " 0.86       0.83333333 0.85666667 0.76       0.87       0.83333333\n",
      " 0.85333333 0.77       0.86666667 0.83333333 0.85666667 0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84333333 0.75       0.84666667 0.85       0.84666667 0.76\n",
      " 0.86       0.83333333 0.85333333 0.76       0.87       0.83333333\n",
      " 0.85333333 0.77       0.86666667 0.83333333 0.86       0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84333333 0.75       0.84666667 0.85       0.85       0.76\n",
      " 0.86       0.83333333 0.86       0.76       0.87       0.83333333\n",
      " 0.84666667 0.77       0.86666667 0.83333333 0.84333333 0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9166666666666666\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.83333333 0.83333333 0.79666667 0.83333333 0.87333333 0.87\n",
      " 0.86333333 0.86666667 0.87333333 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667 0.83       0.83       0.8        0.83\n",
      " 0.87       0.86666667 0.86333333 0.86666667 0.87333333 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.87666667 0.83       0.83\n",
      " 0.8        0.83       0.87       0.86666667 0.86666667 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83333333 0.83333333 0.79       0.83333333 0.87666667 0.87333333\n",
      " 0.86333333 0.86666667 0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83333333 0.83333333 0.8        0.83333333\n",
      " 0.87       0.86666667 0.86666667 0.86666667 0.87666667 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.82666667 0.82666667\n",
      " 0.79333333 0.82666667 0.87       0.86666667 0.86666667 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83333333 0.83333333 0.79666667 0.83333333 0.87666667 0.87333333\n",
      " 0.87       0.87       0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667 0.83333333 0.83333333 0.79333333 0.83333333\n",
      " 0.87       0.86666667 0.86666667 0.86666667 0.87666667 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.82666667 0.82666667\n",
      " 0.79333333 0.82666667 0.87       0.86666667 0.86666667 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83333333 0.83333333 0.8        0.83333333 0.87333333 0.87\n",
      " 0.86333333 0.86666667 0.87333333 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667 0.83       0.83       0.80333333 0.83\n",
      " 0.87       0.86666667 0.86333333 0.86666667 0.87333333 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.87666667 0.83666667 0.83666667\n",
      " 0.79666667 0.83666667 0.87       0.86666667 0.86666667 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83333333 0.83333333 0.79       0.83333333 0.87666667 0.87333333\n",
      " 0.86333333 0.86666667 0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83333333 0.83333333 0.79666667 0.83333333\n",
      " 0.87       0.86666667 0.86333333 0.86666667 0.87666667 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83333333 0.83333333\n",
      " 0.79666667 0.83333333 0.87       0.86666667 0.86666667 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.82666667 0.82666667 0.8        0.82666667 0.87666667 0.87333333\n",
      " 0.87       0.87       0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667 0.83       0.83       0.79666667 0.83\n",
      " 0.87       0.86666667 0.86666667 0.86666667 0.87666667 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83333333 0.83333333\n",
      " 0.79666667 0.83333333 0.87       0.86666667 0.86666667 0.86666667\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.78666667 0.78666667 0.78333333 0.78666667 0.85666667 0.85\n",
      " 0.86666667 0.85       0.87666667 0.87333333 0.87       0.87\n",
      " 0.87666667 0.87       0.87       0.87333333 0.87333333 0.87666667\n",
      " 0.87       0.87666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.78333333 0.78333333 0.77333333 0.78333333 0.85666667 0.85666667\n",
      " 0.86       0.86       0.87666667 0.87666667 0.87       0.87\n",
      " 0.87666667 0.87666667 0.86666667 0.87666667 0.87333333 0.88\n",
      " 0.87       0.88              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82       0.82       0.79       0.82       0.87333333 0.87333333\n",
      " 0.86666667 0.87       0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}.\n",
      " Score: 0.8800000000000001\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.85666667        nan 0.90333333        nan 0.87333333\n",
      "        nan 0.90333333        nan 0.88333333        nan 0.90333333\n",
      "        nan 0.90666667        nan 0.90333333        nan 0.91333333\n",
      "        nan 0.90333333        nan 0.85666667        nan 0.91333333\n",
      "        nan 0.87333333        nan 0.91333333        nan 0.88333333\n",
      "        nan 0.91333333        nan 0.90666667        nan 0.91333333\n",
      "        nan 0.91333333        nan 0.91333333        nan 0.85666667\n",
      "        nan 0.90666667        nan 0.87666667        nan 0.90666667\n",
      "        nan 0.88333333        nan 0.90666667        nan 0.90333333\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.85666667        nan 0.90666667        nan 0.87333333\n",
      "        nan 0.90666667        nan 0.88333333        nan 0.90666667\n",
      "        nan 0.90666667        nan 0.90666667        nan 0.90333333\n",
      "        nan 0.90666667        nan 0.85666667        nan 0.91666667\n",
      "        nan 0.87333333        nan 0.91666667        nan 0.88333333\n",
      "        nan 0.91666667        nan 0.9               nan 0.91666667\n",
      "        nan 0.91              nan 0.91666667        nan 0.85666667\n",
      "        nan 0.91666667        nan 0.87666667        nan 0.91666667\n",
      "        nan 0.88              nan 0.91666667        nan 0.90333333\n",
      "        nan 0.91666667        nan 0.90333333        nan 0.91666667\n",
      "        nan 0.85              nan 0.9               nan 0.86\n",
      "        nan 0.9               nan 0.87              nan 0.9\n",
      "        nan 0.89              nan 0.9               nan 0.89\n",
      "        nan 0.9               nan 0.85666667        nan 0.91333333\n",
      "        nan 0.87333333        nan 0.91333333        nan 0.88\n",
      "        nan 0.91333333        nan 0.89666667        nan 0.91333333\n",
      "        nan 0.90333333        nan 0.91333333        nan 0.85666667\n",
      "        nan 0.91666667        nan 0.87666667        nan 0.91666667\n",
      "        nan 0.88              nan 0.91666667        nan 0.90333333\n",
      "        nan 0.91666667        nan 0.90333333        nan 0.91666667\n",
      "        nan 0.85666667        nan 0.91333333        nan 0.88\n",
      "        nan 0.91333333        nan 0.88333333        nan 0.91333333\n",
      "        nan 0.91              nan 0.91333333        nan 0.91333333\n",
      "        nan 0.91333333        nan 0.86              nan 0.90666667\n",
      "        nan 0.88              nan 0.90666667        nan 0.88333333\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.91333333        nan 0.90666667        nan 0.86\n",
      "        nan 0.90333333        nan 0.87666667        nan 0.90333333\n",
      "        nan 0.88666667        nan 0.90333333        nan 0.91\n",
      "        nan 0.90333333        nan 0.91              nan 0.90333333\n",
      "        nan 0.85666667        nan 0.91              nan 0.87333333\n",
      "        nan 0.91              nan 0.88333333        nan 0.91\n",
      "        nan 0.90333333        nan 0.91              nan 0.90333333\n",
      "        nan 0.91              nan 0.85666667        nan 0.91666667\n",
      "        nan 0.88              nan 0.91666667        nan 0.88333333\n",
      "        nan 0.91666667        nan 0.91333333        nan 0.91666667\n",
      "        nan 0.90333333        nan 0.91666667        nan 0.86\n",
      "        nan 0.91              nan 0.87666667        nan 0.91\n",
      "        nan 0.88333333        nan 0.91              nan 0.90666667\n",
      "        nan 0.91              nan 0.90666667        nan 0.91\n",
      "        nan 0.85              nan 0.9               nan 0.86\n",
      "        nan 0.9               nan 0.87333333        nan 0.9\n",
      "        nan 0.89              nan 0.9               nan 0.89333333\n",
      "        nan 0.9               nan 0.85666667        nan 0.91666667\n",
      "        nan 0.87333333        nan 0.91666667        nan 0.88\n",
      "        nan 0.91666667        nan 0.89333333        nan 0.91666667\n",
      "        nan 0.90666667        nan 0.91666667        nan 0.86\n",
      "        nan 0.91              nan 0.87666667        nan 0.91\n",
      "        nan 0.88333333        nan 0.91              nan 0.90666667\n",
      "        nan 0.91              nan 0.90666667        nan 0.91\n",
      "        nan 0.84666667        nan 0.77666667        nan 0.84333333\n",
      "        nan 0.77666667        nan 0.84666667        nan 0.77666667\n",
      "        nan 0.86333333        nan 0.77666667        nan 0.87\n",
      "        nan 0.77666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84666667        nan 0.76666667        nan 0.84666667\n",
      "        nan 0.76666667        nan 0.85              nan 0.76666667\n",
      "        nan 0.85333333        nan 0.76666667        nan 0.87\n",
      "        nan 0.76666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84666667        nan 0.79666667        nan 0.85\n",
      "        nan 0.79666667        nan 0.85333333        nan 0.79666667\n",
      "        nan 0.85666667        nan 0.79666667        nan 0.86\n",
      "        nan 0.79666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 2000, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 0.1, 'lr__penalty': 'none'}.\n",
      " Score: 0.9166666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['friends_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-yahoo",
   "metadata": {},
   "source": [
    "## Lemmatization and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "official-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_tweets'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "X = train\n",
    "y = train.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "martial-credit",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.87       0.75       0.84666667 0.85       0.88666667 0.76\n",
      " 0.86       0.83666667 0.89333333 0.76       0.87       0.83333333\n",
      " 0.9        0.77       0.86666667 0.83333333 0.9        0.77\n",
      " 0.87       0.83       0.87       0.75       0.84666667 0.85\n",
      " 0.89666667 0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.9        0.77       0.86666667 0.83333333\n",
      " 0.9        0.77       0.87       0.83       0.87333333 0.75\n",
      " 0.84666667 0.85       0.88       0.76       0.86       0.83666667\n",
      " 0.89       0.76       0.87       0.83333333 0.9        0.77\n",
      " 0.86666667 0.83333333 0.89666667 0.77       0.87       0.83\n",
      " 0.86333333 0.75       0.84666667 0.85       0.88333333 0.76\n",
      " 0.86       0.83666667 0.89333333 0.76       0.87       0.83333333\n",
      " 0.88666667 0.77       0.86666667 0.83333333 0.88666667 0.77\n",
      " 0.87       0.83       0.86666667 0.75       0.84666667 0.85\n",
      " 0.89       0.76       0.86       0.83666667 0.89333333 0.76\n",
      " 0.87       0.83333333 0.89333333 0.77       0.86666667 0.83333333\n",
      " 0.89666667 0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88333333 0.76       0.86       0.83666667\n",
      " 0.88666667 0.76       0.87       0.83333333 0.89666667 0.77\n",
      " 0.86666667 0.83333333 0.90333333 0.77       0.87       0.83\n",
      " 0.85666667 0.75       0.84666667 0.85       0.87333333 0.76\n",
      " 0.86       0.83666667 0.88666667 0.76       0.87       0.83333333\n",
      " 0.88666667 0.77       0.86666667 0.83333333 0.88333333 0.77\n",
      " 0.87       0.83       0.86333333 0.75       0.84666667 0.85\n",
      " 0.88       0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.89       0.77       0.86666667 0.83333333\n",
      " 0.88666667 0.77       0.87       0.83       0.87       0.75\n",
      " 0.84666667 0.85       0.88333333 0.76       0.86       0.83666667\n",
      " 0.88666667 0.76       0.87       0.83333333 0.89666667 0.77\n",
      " 0.86666667 0.83333333 0.90333333 0.77       0.87       0.83\n",
      " 0.87333333 0.75       0.84666667 0.85       0.89333333 0.76\n",
      " 0.86       0.83666667 0.90666667 0.76       0.87       0.83333333\n",
      " 0.90666667 0.77       0.86666667 0.83333333 0.90666667 0.77\n",
      " 0.87       0.83       0.87333333 0.75       0.84666667 0.85\n",
      " 0.89666667 0.76       0.86       0.83666667 0.90666667 0.76\n",
      " 0.87       0.83333333 0.9        0.77       0.86666667 0.83333333\n",
      " 0.90333333 0.77       0.87       0.83       0.87333333 0.75\n",
      " 0.84666667 0.85       0.89333333 0.76       0.86       0.83666667\n",
      " 0.90333333 0.76       0.87       0.83333333 0.89666667 0.77\n",
      " 0.86666667 0.83333333 0.9        0.77       0.87       0.83\n",
      " 0.87333333 0.75       0.84666667 0.85       0.89666667 0.76\n",
      " 0.86       0.83666667 0.89666667 0.76       0.87       0.83333333\n",
      " 0.89666667 0.77       0.86666667 0.83333333 0.89666667 0.77\n",
      " 0.87       0.83       0.87333333 0.75       0.84666667 0.85\n",
      " 0.89666667 0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.89666667 0.77       0.86666667 0.83333333\n",
      " 0.89666667 0.77       0.87       0.83       0.87333333 0.75\n",
      " 0.84666667 0.85       0.89333333 0.76       0.86       0.83666667\n",
      " 0.9        0.76       0.87       0.83333333 0.89666667 0.77\n",
      " 0.86666667 0.83333333 0.89333333 0.77       0.87       0.83\n",
      " 0.86333333 0.75       0.84666667 0.85       0.88666667 0.76\n",
      " 0.86       0.83666667 0.89333333 0.76       0.87       0.83333333\n",
      " 0.89       0.77       0.86666667 0.83333333 0.89333333 0.77\n",
      " 0.87       0.83       0.86666667 0.75       0.84666667 0.85\n",
      " 0.89666667 0.76       0.86       0.83666667 0.9        0.76\n",
      " 0.87       0.83333333 0.9        0.77       0.86666667 0.83333333\n",
      " 0.90333333 0.77       0.87       0.83       0.87333333 0.75\n",
      " 0.84666667 0.85       0.89333333 0.76       0.86       0.83666667\n",
      " 0.9        0.76       0.87       0.83333333 0.89666667 0.77\n",
      " 0.86666667 0.83333333 0.89333333 0.77       0.87       0.83\n",
      " 0.84666667 0.75       0.84666667 0.85       0.85       0.76\n",
      " 0.86       0.83333333 0.86       0.76       0.87       0.83333333\n",
      " 0.85       0.77       0.86666667 0.83333333 0.85333333 0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84333333 0.75       0.84666667 0.85       0.85       0.76\n",
      " 0.86       0.83333333 0.85666667 0.76       0.87       0.83333333\n",
      " 0.84       0.77       0.86666667 0.83333333 0.85       0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84333333 0.75       0.84666667 0.85       0.85       0.76\n",
      " 0.86       0.83333333 0.85666667 0.76       0.87       0.83333333\n",
      " 0.84666667 0.77       0.86666667 0.83333333 0.85       0.77\n",
      " 0.87       0.83              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9066666666666666\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.82666667 0.82666667 0.79666667 0.82666667 0.87333333 0.87\n",
      " 0.86333333 0.87       0.87333333 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83       0.83       0.8        0.83\n",
      " 0.87       0.86666667 0.86333333 0.87       0.87333333 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83       0.83\n",
      " 0.79666667 0.83       0.87       0.86666667 0.86333333 0.87\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.82333333 0.82333333 0.79666667 0.82333333 0.87333333 0.87\n",
      " 0.86333333 0.86666667 0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83       0.83       0.79666667 0.83\n",
      " 0.87       0.86666667 0.86       0.86666667 0.87333333 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83333333 0.83333333\n",
      " 0.79333333 0.83333333 0.87       0.86666667 0.86333333 0.87\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83333333 0.83333333 0.79666667 0.83333333 0.87333333 0.87\n",
      " 0.86666667 0.86666667 0.87666667 0.87666667 0.87       0.87333333\n",
      " 0.88       0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.87666667 0.82666667 0.82666667 0.79333333 0.82666667\n",
      " 0.87       0.86666667 0.86333333 0.86666667 0.87666667 0.87666667\n",
      " 0.87       0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83333333 0.83333333\n",
      " 0.79333333 0.83333333 0.87       0.86666667 0.86333333 0.87\n",
      " 0.87666667 0.87666667 0.87       0.87333333 0.87666667 0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83333333 0.83333333 0.81       0.83333333 0.87333333 0.87\n",
      " 0.86       0.87       0.87       0.87666667 0.86333333 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83333333 0.83333333 0.81       0.83333333\n",
      " 0.87       0.86666667 0.86       0.87       0.87       0.87666667\n",
      " 0.86666667 0.87333333 0.87666667 0.87333333 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.83666667 0.83666667\n",
      " 0.79666667 0.83666667 0.87       0.86666667 0.86       0.87\n",
      " 0.87       0.87666667 0.86666667 0.87333333 0.88       0.87333333\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83       0.83       0.8        0.83       0.87333333 0.87\n",
      " 0.86       0.86666667 0.87       0.87666667 0.86666667 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83       0.83       0.81       0.83\n",
      " 0.87       0.86666667 0.86       0.86666667 0.87       0.87666667\n",
      " 0.86333333 0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.84       0.84\n",
      " 0.79666667 0.84       0.87       0.86666667 0.86       0.86666667\n",
      " 0.87       0.87666667 0.86333333 0.87333333 0.88       0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.83333333 0.83333333 0.79666667 0.83333333 0.87666667 0.87333333\n",
      " 0.86       0.86666667 0.87666667 0.87666667 0.86333333 0.87333333\n",
      " 0.88       0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88       0.83       0.83       0.80333333 0.83\n",
      " 0.87       0.86666667 0.86       0.86333333 0.87       0.87666667\n",
      " 0.86333333 0.87333333 0.87666667 0.87666667 0.87       0.87666667\n",
      " 0.87333333 0.88       0.87       0.88       0.84       0.84\n",
      " 0.79666667 0.84       0.87       0.86666667 0.86       0.86666667\n",
      " 0.87       0.87666667 0.86333333 0.87333333 0.88       0.87666667\n",
      " 0.87       0.87666667 0.87333333 0.88       0.87       0.88\n",
      " 0.79666667 0.79666667 0.79333333 0.79666667 0.86666667 0.85666667\n",
      " 0.86333333 0.85666667 0.87333333 0.86666667 0.87       0.86666667\n",
      " 0.87666667 0.87       0.87       0.87       0.87333333 0.87333333\n",
      " 0.87       0.87333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.79       0.79       0.79       0.79       0.86666667 0.86666667\n",
      " 0.86333333 0.86333333 0.87333333 0.87333333 0.87333333 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82333333 0.82333333 0.79333333 0.82333333 0.87       0.87\n",
      " 0.87       0.87       0.87666667 0.87666667 0.87333333 0.87333333\n",
      " 0.87666667 0.87666667 0.87       0.87666667 0.87333333 0.88\n",
      " 0.87       0.88              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}.\n",
      " Score: 0.8800000000000001\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.85333333        nan 0.89666667        nan 0.87333333\n",
      "        nan 0.89666667        nan 0.88              nan 0.89666667\n",
      "        nan 0.90333333        nan 0.89666667        nan 0.9\n",
      "        nan 0.89666667        nan 0.85333333        nan 0.88666667\n",
      "        nan 0.87333333        nan 0.88666667        nan 0.88\n",
      "        nan 0.88666667        nan 0.9               nan 0.88666667\n",
      "        nan 0.9               nan 0.88666667        nan 0.85333333\n",
      "        nan 0.89666667        nan 0.87666667        nan 0.89666667\n",
      "        nan 0.87666667        nan 0.89666667        nan 0.89333333\n",
      "        nan 0.89666667        nan 0.89666667        nan 0.89666667\n",
      "        nan 0.85333333        nan 0.88333333        nan 0.87333333\n",
      "        nan 0.88333333        nan 0.87666667        nan 0.88333333\n",
      "        nan 0.89333333        nan 0.88333333        nan 0.89333333\n",
      "        nan 0.88333333        nan 0.85333333        nan 0.90333333\n",
      "        nan 0.87333333        nan 0.90333333        nan 0.88\n",
      "        nan 0.90333333        nan 0.9               nan 0.90333333\n",
      "        nan 0.89333333        nan 0.90333333        nan 0.85333333\n",
      "        nan 0.90333333        nan 0.87666667        nan 0.90333333\n",
      "        nan 0.87333333        nan 0.90333333        nan 0.9\n",
      "        nan 0.90333333        nan 0.89              nan 0.90333333\n",
      "        nan 0.85              nan 0.89              nan 0.86333333\n",
      "        nan 0.89              nan 0.87              nan 0.89\n",
      "        nan 0.88666667        nan 0.89              nan 0.89333333\n",
      "        nan 0.89              nan 0.85333333        nan 0.89666667\n",
      "        nan 0.87333333        nan 0.89666667        nan 0.88\n",
      "        nan 0.89666667        nan 0.89666667        nan 0.89666667\n",
      "        nan 0.89333333        nan 0.89666667        nan 0.85333333\n",
      "        nan 0.91333333        nan 0.87666667        nan 0.91333333\n",
      "        nan 0.87333333        nan 0.91333333        nan 0.89666667\n",
      "        nan 0.91333333        nan 0.89              nan 0.91333333\n",
      "        nan 0.86              nan 0.90666667        nan 0.87666667\n",
      "        nan 0.90666667        nan 0.88666667        nan 0.90666667\n",
      "        nan 0.91              nan 0.90666667        nan 0.9\n",
      "        nan 0.90666667        nan 0.86              nan 0.89\n",
      "        nan 0.88              nan 0.89              nan 0.89333333\n",
      "        nan 0.89              nan 0.91              nan 0.89\n",
      "        nan 0.9               nan 0.89              nan 0.86\n",
      "        nan 0.89333333        nan 0.87666667        nan 0.89333333\n",
      "        nan 0.89              nan 0.89333333        nan 0.9\n",
      "        nan 0.89333333        nan 0.90333333        nan 0.89333333\n",
      "        nan 0.85333333        nan 0.89666667        nan 0.87333333\n",
      "        nan 0.89666667        nan 0.88666667        nan 0.89666667\n",
      "        nan 0.9               nan 0.89666667        nan 0.89666667\n",
      "        nan 0.89666667        nan 0.85666667        nan 0.88333333\n",
      "        nan 0.87666667        nan 0.88333333        nan 0.88666667\n",
      "        nan 0.88333333        nan 0.90333333        nan 0.88333333\n",
      "        nan 0.89666667        nan 0.88333333        nan 0.86\n",
      "        nan 0.89666667        nan 0.87333333        nan 0.89666667\n",
      "        nan 0.88666667        nan 0.89666667        nan 0.9\n",
      "        nan 0.89666667        nan 0.89666667        nan 0.89666667\n",
      "        nan 0.85333333        nan 0.86666667        nan 0.87\n",
      "        nan 0.86666667        nan 0.87666667        nan 0.86666667\n",
      "        nan 0.9               nan 0.86666667        nan 0.90333333\n",
      "        nan 0.86666667        nan 0.85333333        nan 0.89666667\n",
      "        nan 0.87333333        nan 0.89666667        nan 0.88666667\n",
      "        nan 0.89666667        nan 0.90333333        nan 0.89666667\n",
      "        nan 0.9               nan 0.89666667        nan 0.86\n",
      "        nan 0.90333333        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.88666667        nan 0.90333333        nan 0.9\n",
      "        nan 0.90333333        nan 0.89666667        nan 0.90333333\n",
      "        nan 0.84666667        nan 0.78              nan 0.85\n",
      "        nan 0.78              nan 0.85              nan 0.78\n",
      "        nan 0.86333333        nan 0.78              nan 0.86\n",
      "        nan 0.78              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84666667        nan 0.77              nan 0.84666667\n",
      "        nan 0.77              nan 0.84666667        nan 0.77\n",
      "        nan 0.85666667        nan 0.77              nan 0.86\n",
      "        nan 0.77              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84666667        nan 0.82666667        nan 0.85\n",
      "        nan 0.82666667        nan 0.85333333        nan 0.82666667\n",
      "        nan 0.85666667        nan 0.82666667        nan 0.85666667\n",
      "        nan 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 10, 'lr__C': 0.1, 'lr__penalty': 'none'}.\n",
      " Score: 0.9133333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['friends_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-management",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "assumed-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subsequent-trinidad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('numeric_features',\n",
       "                                                 Pipeline(steps=[('selector_num',\n",
       "                                                                  FunctionTransformer(func=<function get_numeric_data_ at 0x7fe24a4070d0>))])),\n",
       "                                                ('text_features',\n",
       "                                                 Pipeline(steps=[('selector_text',\n",
       "                                                                  FunctionTransformer(func=<function get_text_data_ at 0x7fe249c5a9d0>)),\n",
       "                                                                 ('vectorizer',\n",
       "                                                                  TfidfVectorizer(max_df=0.5,\n",
       "                                                                                  max_features=2000,\n",
       "                                                                                  min_df=5))]))])),\n",
       "                ('lr', LogisticRegression(max_iter=1000, penalty='none'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    df = df.copy()\n",
    "    df['textdata'] = clean_text(df['name']+ ' ' + df['description'] + ' ' + df['recent_100_tweets'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df['friends_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=2000, min_df=5)),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000, penalty='none'))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mathematical-malaysia",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../classifiers/classifier_politician_ndtfr.sav']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../classifiers/classifier_politician_ndtfr.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
