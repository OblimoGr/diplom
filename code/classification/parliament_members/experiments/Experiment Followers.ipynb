{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "public-reverse",
   "metadata": {},
   "source": [
    "# Classification Experiment: Followers\n",
    "---\n",
    "This Notebook, includes a series of experiments, on using a node's Followers for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-royal",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incorporate-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "import time \n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-platform",
   "metadata": {},
   "source": [
    "Twitter API Authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unavailable-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_credentials = []\n",
    "with open('../../../../twitter_credentials.json', 'r') as f:\n",
    "    twitter_credentials = json.load(f)\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_credentials['consumer_key'], twitter_credentials['consumer_secret'])\n",
    "auth.set_access_token(twitter_credentials['access_token_key'],twitter_credentials['access_token_secret'])\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, timeout=60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "requested-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function For Text Normalization\n",
    "def clean_text(data):\n",
    "    urls = r'http\\S+'\n",
    "    non_unicode_char = r'\\W'\n",
    "    numbers = r'[0-9_]'\n",
    "    fix_whitespace = r'\\s+'\n",
    "    single_whitespace = ' '\n",
    "    \n",
    "    data = (data.replace([urls], single_whitespace, regex=True)\n",
    "                    .replace([non_unicode_char, numbers], single_whitespace, regex=True)\n",
    "                    .replace(fix_whitespace, single_whitespace, regex=True))\n",
    "    data = data.apply(lambda s: s.lower() if type(s) == str else s)\n",
    "    return data\n",
    "\n",
    "nlp_el = spacy.load('el_core_news_md')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "STOPWORDS = set(list(spacy.lang.en.STOP_WORDS) + list(spacy.lang.el.STOP_WORDS))\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    row = [str(token) for token in nlp_el(row)]\n",
    "    return [w for w in row if w not in STOPWORDS]\n",
    "\n",
    "def tokenize_lemmatize(row):\n",
    "    return [str(token.lemma_) for token in nlp_el(row)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-athletics",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suitable-eclipse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>recent_100_tweets</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>recent_100_friends_nd</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>parliament_member</th>\n",
       "      <th>friends_politician_count_1000</th>\n",
       "      <th>followers_politician_count_1000</th>\n",
       "      <th>mentions_politician_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evefthym</td>\n",
       "      <td>Î•Ï…Î¬Î³Î³ÎµÎ»Î¿Ï‚ Î•Ï…Î¸Ï…Î¼Î¯Î¿Ï…</td>\n",
       "      <td>ÎšÎ±Î½Î±Î»Î¬ÎºÎ¹ \\nÎ Î¬ÏÎ³Î± \\nÎ™Ï‰Î¬Î½Î½Î¹Î½Î±</td>\n",
       "      <td>135</td>\n",
       "      <td>#NewProfilePic https://t.co/h0wlMIS6U0 @wavyp...</td>\n",
       "      <td>156</td>\n",
       "      <td>Panagiotis Vougious ÎšÏÏÎ¹Îµ ÎœÎ¬Ï„ÏƒÎ¹Î¿ Î¹ÏƒÏ‡Ï…ÏÎ¯Î¶ÎµÏ„Î±Î¹ ...</td>\n",
       "      <td>448</td>\n",
       "      <td>False</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qdv6zNFe0nkBH2m</td>\n",
       "      <td>Î§ÏÎ¹ÏƒÏ„Î¯Î½Î± ÎšÎµÏÎ±Î¼ÎµÏ‰Ï‚</td>\n",
       "      <td>Î§Î¹Î¿ÏÎ¼Î¿Ï</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "      <td>Kostas.Vaxevanis Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Ï„Î¿Ï… https://t.co/ArS...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michail_pana</td>\n",
       "      <td>ÎœÎ¹Ï‡Î±Î®Î» Î Î±Î½Î±Î³Î¹Ï‰Ï„ÏŒÏ€Î¿Ï…Î»Î¿Ï‚</td>\n",
       "      <td>Î‘Ï€ÏŒÏƒÏ„ÏÎ±Ï„Î¿Ï‚ ÎÎ±ÏÎ±ÏÏ‡Î¿Ï‚ Ï„Î¿Ï… Î Î¿Î»Î­Î¼Î¹ÎºÎ¿Ï ÎÎ±Ï…Ï„Î¹ÎºÎ¿Ï.\\nÎœ...</td>\n",
       "      <td>368</td>\n",
       "      <td>ÎŸÏÎ¸Î® ÎºÎ±Î¹ Ï„ÎµÎºÎ¼Î·ÏÎ¹Ï‰Î¼Î­Î½Î· Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ·. \\nÎ£Ï…Î³Ï‡Î±ÏÎ·Ï„Î®...</td>\n",
       "      <td>70</td>\n",
       "      <td>Symban 300 UFCğŸ‡¬ğŸ‡·ğŸ‡¦ğŸ‡²ğŸ‡¨ğŸ‡¾ğŸ‡·ğŸ‡¸ğŸ‡¸ğŸ‡¦ğŸ‡«ğŸ‡·ğŸ‡®ğŸ‡±ğŸ‡¦ğŸ‡¹ğŸ‡§ğŸ‡¬ Xara Stefano...</td>\n",
       "      <td>358</td>\n",
       "      <td>False</td>\n",
       "      <td>658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spirtzisforever</td>\n",
       "      <td>Î§ÏÎ®ÏƒÏ„Î¿Ï‚ not Î£Ï€Î¯ÏÏ„Î¶Î·Ï‚</td>\n",
       "      <td>Parody account/Fan account Ï„Î¿Ï… Ï€Î¹Î¿ ÎµÏÏ‰Ï„Î¹ÎºÎ¿Ï Î²Î¿...</td>\n",
       "      <td>7</td>\n",
       "      <td>@NasosNot Î•Ï€Î¹ Î Î‘Î£ÎŸÎš ÎµÎ¯Ï‡Î±Î¼Îµ ÎºÎ»Î±Î´Î¹ÎºÎ­Ï‚, Î±Ï…Ï„ÏŒ Î®Ï„Î±...</td>\n",
       "      <td>7</td>\n",
       "      <td>ÎÎ¬ÏƒÎ¿Ï‚ Î—Î»Î¹ÏŒÏ€Î¿Ï…Î»Î¿Ï‚ Î•ÎºÏ€ÏÏŒÏƒÏ‰Ï€Î¿Ï‚ Î¤ÏÏ€Î¿Ï… Ï„Î¿Ï… Î£Î¥Î¡Î™Î–Î‘-...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FdZwMqKWXciTydT</td>\n",
       "      <td>Î•Î¹ÏÎ®Î½Î· Î“ÎºÎ±ÏÎ±</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>ÎšÎ±Î»ÏŒ Î²ÏÎ¬Î´Ï…!!!!</td>\n",
       "      <td>3</td>\n",
       "      <td>Kostas.Vaxevanis Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Ï„Î¿Ï… https://t.co/ArS...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                    name  \\\n",
       "0         evefthym      Î•Ï…Î¬Î³Î³ÎµÎ»Î¿Ï‚ Î•Ï…Î¸Ï…Î¼Î¯Î¿Ï…   \n",
       "1  Qdv6zNFe0nkBH2m       Î§ÏÎ¹ÏƒÏ„Î¯Î½Î± ÎšÎµÏÎ±Î¼ÎµÏ‰Ï‚   \n",
       "2     michail_pana  ÎœÎ¹Ï‡Î±Î®Î» Î Î±Î½Î±Î³Î¹Ï‰Ï„ÏŒÏ€Î¿Ï…Î»Î¿Ï‚   \n",
       "3  spirtzisforever    Î§ÏÎ®ÏƒÏ„Î¿Ï‚ not Î£Ï€Î¯ÏÏ„Î¶Î·Ï‚   \n",
       "4  FdZwMqKWXciTydT            Î•Î¹ÏÎ®Î½Î· Î“ÎºÎ±ÏÎ±   \n",
       "\n",
       "                                         description  statuses_count  \\\n",
       "0                        ÎšÎ±Î½Î±Î»Î¬ÎºÎ¹ \\nÎ Î¬ÏÎ³Î± \\nÎ™Ï‰Î¬Î½Î½Î¹Î½Î±             135   \n",
       "1                                            Î§Î¹Î¿ÏÎ¼Î¿Ï               0   \n",
       "2  Î‘Ï€ÏŒÏƒÏ„ÏÎ±Ï„Î¿Ï‚ ÎÎ±ÏÎ±ÏÏ‡Î¿Ï‚ Ï„Î¿Ï… Î Î¿Î»Î­Î¼Î¹ÎºÎ¿Ï ÎÎ±Ï…Ï„Î¹ÎºÎ¿Ï.\\nÎœ...             368   \n",
       "3  Parody account/Fan account Ï„Î¿Ï… Ï€Î¹Î¿ ÎµÏÏ‰Ï„Î¹ÎºÎ¿Ï Î²Î¿...               7   \n",
       "4                                                                  1   \n",
       "\n",
       "                                   recent_100_tweets  friends_count  \\\n",
       "0   #NewProfilePic https://t.co/h0wlMIS6U0 @wavyp...            156   \n",
       "1                                                                18   \n",
       "2   ÎŸÏÎ¸Î® ÎºÎ±Î¹ Ï„ÎµÎºÎ¼Î·ÏÎ¹Ï‰Î¼Î­Î½Î· Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ·. \\nÎ£Ï…Î³Ï‡Î±ÏÎ·Ï„Î®...             70   \n",
       "3   @NasosNot Î•Ï€Î¹ Î Î‘Î£ÎŸÎš ÎµÎ¯Ï‡Î±Î¼Îµ ÎºÎ»Î±Î´Î¹ÎºÎ­Ï‚, Î±Ï…Ï„ÏŒ Î®Ï„Î±...              7   \n",
       "4                                     ÎšÎ±Î»ÏŒ Î²ÏÎ¬Î´Ï…!!!!              3   \n",
       "\n",
       "                               recent_100_friends_nd  followers_count  \\\n",
       "0   Panagiotis Vougious ÎšÏÏÎ¹Îµ ÎœÎ¬Ï„ÏƒÎ¹Î¿ Î¹ÏƒÏ‡Ï…ÏÎ¯Î¶ÎµÏ„Î±Î¹ ...              448   \n",
       "1   Kostas.Vaxevanis Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Ï„Î¿Ï… https://t.co/ArS...                6   \n",
       "2   Symban 300 UFCğŸ‡¬ğŸ‡·ğŸ‡¦ğŸ‡²ğŸ‡¨ğŸ‡¾ğŸ‡·ğŸ‡¸ğŸ‡¸ğŸ‡¦ğŸ‡«ğŸ‡·ğŸ‡®ğŸ‡±ğŸ‡¦ğŸ‡¹ğŸ‡§ğŸ‡¬ Xara Stefano...              358   \n",
       "3   ÎÎ¬ÏƒÎ¿Ï‚ Î—Î»Î¹ÏŒÏ€Î¿Ï…Î»Î¿Ï‚ Î•ÎºÏ€ÏÏŒÏƒÏ‰Ï€Î¿Ï‚ Î¤ÏÏ€Î¿Ï… Ï„Î¿Ï… Î£Î¥Î¡Î™Î–Î‘-...                4   \n",
       "4   Kostas.Vaxevanis Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Ï„Î¿Ï… https://t.co/ArS...                0   \n",
       "\n",
       "   default_profile_image  favourites_count  parliament_member  \\\n",
       "0                  False               696                  0   \n",
       "1                  False                 3                  0   \n",
       "2                  False               658                  0   \n",
       "3                  False                 2                  0   \n",
       "4                  False                 0                  0   \n",
       "\n",
       "   friends_politician_count_1000  followers_politician_count_1000  \\\n",
       "0                              5                              3.0   \n",
       "1                              3                              0.0   \n",
       "2                              0                              2.0   \n",
       "3                              1                              0.0   \n",
       "4                              0                              0.0   \n",
       "\n",
       "   mentions_politician_count  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          2  \n",
       "4                          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Training Set\n",
    "training_set = pd.read_csv('../../../../datasets/Greek Politicians/classification/parliament_members_training_set.csv')\n",
    "training_set = training_set.replace(np.nan, '')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-prince",
   "metadata": {},
   "source": [
    "# Only Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "collected-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sublime-duplicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      " Best Params: {'svm__C': 0.1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.7566666666666666\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.52666667 0.52666667 0.68666667 0.69333333\n",
      " 0.75333333 0.74       0.73666667 0.73       0.75666667 0.74333333\n",
      " 0.74333333 0.73333333 0.76333333 0.75       0.75666667 0.75\n",
      " 0.76333333 0.75       0.76333333 0.75       0.76333333 0.75\n",
      " 0.77333333 0.76       0.77333333 0.76       0.77333333 0.76\n",
      " 0.76       0.74666667 0.75666667 0.76       0.76333333 0.76\n",
      " 0.75666667 0.76       0.76333333 0.76      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'knn__n_neighbors': 12, 'knn__weights': 'uniform'}.\n",
      " Score: 0.7733333333333332\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      " Best Params: {'lr__C': 0.1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.7566666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.75666667        nan        nan 0.75666667        nan\n",
      "        nan 0.75666667        nan        nan 0.75666667        nan\n",
      "        nan 0.75666667        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df['followers_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'knn__n_neighbors': [i for i in range(20)],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-license",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civil-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 FunctionTransformer(func=<function get_data_parl_fo at 0x7fa54f1d7040>)),\n",
       "                ('knn', KNeighborsClassifier(n_neighbors=12))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member\n",
    "\n",
    "# Function to select the data\n",
    "def get_data_parl_fo(df):\n",
    "    data = df['followers_politician_count_1000'].to_numpy()\n",
    "    return data.reshape(-1,1)\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_parl_fo)\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=12, weights='uniform'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "seventh-yahoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_parl_fo.sav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'classifier_parl_fo.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-medline",
   "metadata": {},
   "source": [
    "## Friends and Followers Politician COunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "congressional-rating",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unlikely-victim",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      " Best Params: {'svm__C': 10, 'svm__kernel': 'rbf'}.\n",
      " Score: 0.9\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.80666667 0.80666667 0.77       0.82666667\n",
      " 0.89       0.86333333 0.87       0.86666667 0.89666667 0.88333333\n",
      " 0.88666667 0.88333333 0.89       0.88333333 0.88666667 0.88333333\n",
      " 0.89666667 0.88666667 0.89       0.88666667 0.89       0.89\n",
      " 0.89       0.89       0.89       0.88       0.87666667 0.88333333\n",
      " 0.88       0.87666667 0.87333333 0.88333333 0.87333333 0.88\n",
      " 0.87333333 0.88       0.87333333 0.88      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'knn__n_neighbors': 5, 'knn__weights': 'uniform'}.\n",
      " Score: 0.8966666666666667\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      " Best Params: {'lr__C': 0.1, 'lr__penalty': 'l2'}.\n",
      " Score: 0.8666666666666668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.86666667        nan        nan 0.86666667        nan\n",
      "        nan 0.86666667        nan        nan 0.86666667        nan\n",
      "        nan 0.86666667        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df[['friends_politician_count_1000', 'followers_politician_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'knn__n_neighbors': [i for i in range(20)],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-ceramic",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "respiratory-nigeria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 FunctionTransformer(func=<function get_data_ at 0x7f8c4d7d9160>)),\n",
       "                ('svm', SVC(C=10))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member\n",
    "\n",
    "# Function to select the data\n",
    "def get_data_(df):\n",
    "    data = df[['friends_politician_count_1000', 'followers_politician_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "\n",
    "get_data = FunctionTransformer(get_data_)\n",
    "\n",
    "# The pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('selector', get_data),\n",
    "    ('svm', svm.SVC(C=10, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "engaging-baghdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../classifiers/classifier_politician_frfo.sav']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../classifiers/classifier_politician_frfo.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-premises",
   "metadata": {},
   "source": [
    "# Name Description Tweets and Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-republic",
   "metadata": {},
   "source": [
    "## Without NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "rubber-retreat",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_tweets'])\n",
    "X = train\n",
    "y = train.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "proprietary-saying",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.88       0.75       0.85       0.85333333 0.91       0.75666667\n",
      " 0.86666667 0.83333333 0.90333333 0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.90333333 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.90333333 0.77666667 0.88       0.83333333\n",
      " 0.89333333 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.88666667 0.77666667\n",
      " 0.88       0.83333333 0.88666667 0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.91       0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.89333333 0.77666667 0.88       0.83333333 0.89       0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.91       0.76333333\n",
      " 0.87666667 0.83333333 0.9        0.77666667 0.88       0.83333333\n",
      " 0.89333333 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.88333333 0.77666667\n",
      " 0.88       0.83333333 0.89       0.77666667 0.89333333 0.83\n",
      " 0.87666667 0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.90666667 0.77666667 0.88       0.83333333 0.90666667 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90333333 0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.89666667 0.77666667 0.88       0.83333333\n",
      " 0.89666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.88       0.77666667\n",
      " 0.88       0.83333333 0.89       0.77666667 0.89333333 0.83\n",
      " 0.88333333 0.75       0.85       0.85333333 0.91       0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.89666667 0.77666667 0.88       0.83333333 0.89666667 0.77666667\n",
      " 0.89333333 0.83       0.88333333 0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.91       0.76333333\n",
      " 0.87666667 0.83333333 0.89666667 0.77666667 0.88       0.83333333\n",
      " 0.89333333 0.77666667 0.89333333 0.83       0.88333333 0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.91333333 0.76333333 0.87666667 0.83333333 0.89666667 0.77666667\n",
      " 0.88       0.83333333 0.89       0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.89666667 0.76333333 0.87666667 0.83333333\n",
      " 0.89666667 0.77666667 0.88       0.83333333 0.9        0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.91       0.76333333\n",
      " 0.87666667 0.83333333 0.9        0.77666667 0.88       0.83333333\n",
      " 0.9        0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90666667 0.76333333 0.87666667 0.83333333 0.89333333 0.77666667\n",
      " 0.88       0.83333333 0.89       0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.90333333 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.91333333 0.76333333\n",
      " 0.87666667 0.83333333 0.89       0.77666667 0.88       0.83333333\n",
      " 0.89666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90666667 0.76333333 0.87666667 0.83333333 0.89333333 0.77666667\n",
      " 0.88       0.83333333 0.89       0.77666667 0.89333333 0.83\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87666667 0.75666667\n",
      " 0.86666667 0.83666667 0.88       0.76333333 0.87666667 0.83666667\n",
      " 0.86666667 0.77666667 0.88       0.83       0.87       0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87666667 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.87333333 0.77666667 0.88       0.83       0.87333333 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87333333 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.86666667 0.77666667 0.88       0.83       0.86666667 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 10, 'svm__C': 1, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9133333333333333\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.83       0.83       0.78       0.83       0.89       0.88\n",
      " 0.87333333 0.88       0.9        0.89333333 0.89       0.9\n",
      " 0.88666667 0.89333333 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.78       0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89666667 0.89333333\n",
      " 0.89       0.9        0.88666667 0.89333333 0.88666667 0.89333333\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83       0.83\n",
      " 0.78       0.83       0.89333333 0.88333333 0.87333333 0.88\n",
      " 0.89333333 0.89333333 0.89       0.9        0.88666667 0.89333333\n",
      " 0.88666667 0.89       0.89666667 0.89666667 0.89       0.89333333\n",
      " 0.82666667 0.82666667 0.78       0.82666667 0.89333333 0.88333333\n",
      " 0.87333333 0.88       0.89666667 0.89666667 0.89       0.9\n",
      " 0.88666667 0.89333333 0.88666667 0.89333333 0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.78333333 0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.9        0.89666667\n",
      " 0.89       0.9        0.88666667 0.89333333 0.88666667 0.89\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83       0.83\n",
      " 0.78       0.83       0.89333333 0.88333333 0.87666667 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.88666667 0.89333333\n",
      " 0.88666667 0.89       0.89666667 0.9        0.89       0.89333333\n",
      " 0.82333333 0.82333333 0.78       0.82333333 0.89333333 0.88333333\n",
      " 0.87333333 0.88       0.89666667 0.89666667 0.89       0.9\n",
      " 0.88666667 0.89333333 0.88666667 0.89       0.89666667 0.89666667\n",
      " 0.89       0.89333333 0.83       0.83       0.77666667 0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89666667 0.89666667\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83       0.83\n",
      " 0.78       0.83       0.89333333 0.88333333 0.87666667 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.88666667 0.89333333\n",
      " 0.88666667 0.89       0.89666667 0.9        0.89       0.89333333\n",
      " 0.82666667 0.82666667 0.77666667 0.82666667 0.89       0.88\n",
      " 0.87333333 0.88       0.89666667 0.89333333 0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89333333 0.89666667 0.9\n",
      " 0.89       0.89666667 0.83       0.83       0.77666667 0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89333333 0.89333333\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.89666667 0.9        0.89       0.89666667 0.83333333 0.83333333\n",
      " 0.77666667 0.83333333 0.89333333 0.88333333 0.87333333 0.88\n",
      " 0.89333333 0.89333333 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89       0.89666667 0.9        0.89       0.89666667\n",
      " 0.83       0.83       0.77666667 0.83       0.89333333 0.88333333\n",
      " 0.87333333 0.88       0.89666667 0.89333333 0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89       0.89666667 0.89666667\n",
      " 0.89       0.89666667 0.83       0.83       0.78       0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89333333 0.89333333\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.89333333 0.89       0.89666667 0.83       0.83\n",
      " 0.77666667 0.83       0.89333333 0.88333333 0.87333333 0.88\n",
      " 0.89666667 0.89333333 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89       0.89666667 0.89666667 0.89       0.89666667\n",
      " 0.82666667 0.82666667 0.77666667 0.82666667 0.89333333 0.88333333\n",
      " 0.87333333 0.88       0.89333333 0.89333333 0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89666667 0.82666667 0.82666667 0.78       0.82666667\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89666667 0.89333333\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.89666667 0.89       0.89666667 0.83       0.83\n",
      " 0.77666667 0.83       0.89333333 0.88333333 0.87333333 0.88\n",
      " 0.89666667 0.89333333 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89       0.89666667 0.89666667 0.89       0.89666667\n",
      " 0.79333333 0.79333333 0.77333333 0.79333333 0.89       0.87333333\n",
      " 0.88       0.86333333 0.89333333 0.88       0.88333333 0.88333333\n",
      " 0.88666667 0.88333333 0.88666667 0.88666667 0.89666667 0.89666667\n",
      " 0.89       0.89333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.79333333 0.79333333 0.77666667 0.79333333 0.88666667 0.87\n",
      " 0.87666667 0.86666667 0.89333333 0.88333333 0.88666667 0.89\n",
      " 0.89       0.89       0.88333333 0.88666667 0.89666667 0.89666667\n",
      " 0.89       0.89333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.80666667 0.80666667 0.78       0.80666667 0.88666667 0.87666667\n",
      " 0.87333333 0.87333333 0.9        0.89666667 0.89       0.9\n",
      " 0.89       0.89666667 0.88333333 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.9               nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 1, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}.\n",
      " Score: 0.9033333333333333\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.87              nan 0.89666667        nan 0.88\n",
      "        nan 0.89666667        nan 0.9               nan 0.89666667\n",
      "        nan 0.91              nan 0.89666667        nan 0.91333333\n",
      "        nan 0.89666667        nan 0.87              nan 0.89666667\n",
      "        nan 0.88              nan 0.89666667        nan 0.9\n",
      "        nan 0.89666667        nan 0.91333333        nan 0.89666667\n",
      "        nan 0.91              nan 0.89666667        nan 0.87\n",
      "        nan 0.9               nan 0.88333333        nan 0.9\n",
      "        nan 0.89666667        nan 0.9               nan 0.91333333\n",
      "        nan 0.9               nan 0.91333333        nan 0.9\n",
      "        nan 0.87              nan 0.91              nan 0.88333333\n",
      "        nan 0.91              nan 0.9               nan 0.91\n",
      "        nan 0.91333333        nan 0.91              nan 0.91333333\n",
      "        nan 0.91              nan 0.87              nan 0.91333333\n",
      "        nan 0.88              nan 0.91333333        nan 0.9\n",
      "        nan 0.91333333        nan 0.90666667        nan 0.91333333\n",
      "        nan 0.91              nan 0.91333333        nan 0.87\n",
      "        nan 0.89666667        nan 0.88              nan 0.89666667\n",
      "        nan 0.89666667        nan 0.89666667        nan 0.91\n",
      "        nan 0.89666667        nan 0.91333333        nan 0.89666667\n",
      "        nan 0.87              nan 0.90333333        nan 0.87666667\n",
      "        nan 0.90333333        nan 0.88666667        nan 0.90333333\n",
      "        nan 0.90666667        nan 0.90333333        nan 0.90666667\n",
      "        nan 0.90333333        nan 0.87              nan 0.91333333\n",
      "        nan 0.88333333        nan 0.91333333        nan 0.89\n",
      "        nan 0.91333333        nan 0.91              nan 0.91333333\n",
      "        nan 0.91333333        nan 0.91333333        nan 0.87\n",
      "        nan 0.9               nan 0.88              nan 0.9\n",
      "        nan 0.89666667        nan 0.9               nan 0.91\n",
      "        nan 0.9               nan 0.91333333        nan 0.9\n",
      "        nan 0.87333333        nan 0.9               nan 0.89\n",
      "        nan 0.9               nan 0.9               nan 0.9\n",
      "        nan 0.91333333        nan 0.9               nan 0.91333333\n",
      "        nan 0.9               nan 0.87333333        nan 0.9\n",
      "        nan 0.88666667        nan 0.9               nan 0.9\n",
      "        nan 0.9               nan 0.90666667        nan 0.9\n",
      "        nan 0.91333333        nan 0.9               nan 0.87333333\n",
      "        nan 0.90333333        nan 0.88333333        nan 0.90333333\n",
      "        nan 0.90333333        nan 0.90333333        nan 0.91\n",
      "        nan 0.90333333        nan 0.90666667        nan 0.90333333\n",
      "        nan 0.87333333        nan 0.90666667        nan 0.88333333\n",
      "        nan 0.90666667        nan 0.9               nan 0.90666667\n",
      "        nan 0.90333333        nan 0.90666667        nan 0.90333333\n",
      "        nan 0.90666667        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.88333333        nan 0.90333333        nan 0.9\n",
      "        nan 0.90333333        nan 0.91              nan 0.90333333\n",
      "        nan 0.91              nan 0.90333333        nan 0.87333333\n",
      "        nan 0.90333333        nan 0.88              nan 0.90333333\n",
      "        nan 0.90333333        nan 0.90333333        nan 0.91\n",
      "        nan 0.90333333        nan 0.90666667        nan 0.90333333\n",
      "        nan 0.87              nan 0.90333333        nan 0.87666667\n",
      "        nan 0.90333333        nan 0.89333333        nan 0.90333333\n",
      "        nan 0.90666667        nan 0.90333333        nan 0.91\n",
      "        nan 0.90333333        nan 0.87333333        nan 0.91\n",
      "        nan 0.88              nan 0.91              nan 0.9\n",
      "        nan 0.91              nan 0.90666667        nan 0.91\n",
      "        nan 0.91              nan 0.91              nan 0.87333333\n",
      "        nan 0.90666667        nan 0.88              nan 0.90666667\n",
      "        nan 0.90333333        nan 0.90666667        nan 0.91\n",
      "        nan 0.90666667        nan 0.90666667        nan 0.90666667\n",
      "        nan 0.86666667        nan 0.78              nan 0.86333333\n",
      "        nan 0.78              nan 0.86666667        nan 0.78\n",
      "        nan 0.87666667        nan 0.78              nan 0.87333333\n",
      "        nan 0.78              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.75666667        nan 0.86666667\n",
      "        nan 0.75666667        nan 0.87              nan 0.75666667\n",
      "        nan 0.88              nan 0.75666667        nan 0.87333333\n",
      "        nan 0.75666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.79              nan 0.86666667\n",
      "        nan 0.79              nan 0.86666667        nan 0.79\n",
      "        nan 0.87333333        nan 0.79              nan 0.87333333\n",
      "        nan 0.79              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.9133333333333334\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_politician_count_1000', 'followers_politician_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-grain",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "comfortable-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_tweets'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "\n",
    "X = train\n",
    "y = train.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hawaiian-amazon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.88       0.75       0.85       0.85333333 0.91       0.75666667\n",
      " 0.86666667 0.83333333 0.90333333 0.76333333 0.87666667 0.83333333\n",
      " 0.91333333 0.77666667 0.88       0.83333333 0.91666667 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.90333333 0.76333333\n",
      " 0.87666667 0.83333333 0.90333333 0.77666667 0.88       0.83333333\n",
      " 0.9        0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.91666667 0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90333333 0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.90333333 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.91333333 0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.91       0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.90666667 0.77666667 0.89333333 0.83\n",
      " 0.87666667 0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90333333 0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.9        0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90333333 0.75666667 0.86666667 0.83333333 0.91       0.76333333\n",
      " 0.87666667 0.83333333 0.91       0.77666667 0.88       0.83333333\n",
      " 0.91       0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.90666667 0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.91       0.75666667\n",
      " 0.86666667 0.83333333 0.9        0.76333333 0.87666667 0.83333333\n",
      " 0.92       0.77666667 0.88       0.83333333 0.91666667 0.77333333\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.91       0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77333333 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.91333333 0.76333333 0.87666667 0.83333333 0.91       0.77666667\n",
      " 0.88       0.83333333 0.91333333 0.77333333 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.91333333 0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.90666667 0.77666667 0.88       0.83333333 0.90666667 0.77333333\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.91333333 0.76333333\n",
      " 0.87666667 0.83333333 0.91333333 0.77666667 0.88       0.83333333\n",
      " 0.91333333 0.77333333 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90666667 0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.90333333 0.77333333 0.89333333 0.83\n",
      " 0.87666667 0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.9        0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.91       0.77666667 0.88       0.83333333\n",
      " 0.91333333 0.77333333 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90666667 0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.90333333 0.77333333 0.89333333 0.83\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87666667 0.75666667\n",
      " 0.86666667 0.83666667 0.87666667 0.76333333 0.87666667 0.83666667\n",
      " 0.86666667 0.77666667 0.88       0.83       0.86666667 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87333333 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.87       0.77666667 0.88       0.83       0.86666667 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87333333 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.86666667 0.77666667 0.88       0.83       0.86666667 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.75, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9199999999999999\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.83333333 0.83333333 0.78       0.83333333 0.89       0.88\n",
      " 0.87333333 0.88       0.89666667 0.89666667 0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.78       0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.9        0.89666667\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83       0.83\n",
      " 0.78       0.83       0.89333333 0.88333333 0.87333333 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.89666667 0.9        0.89       0.89333333\n",
      " 0.83       0.83       0.78       0.83       0.89       0.88\n",
      " 0.87666667 0.88       0.89333333 0.89333333 0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89333333 0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.78333333 0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89333333 0.89666667\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.89666667 0.89       0.89333333 0.83       0.83\n",
      " 0.78       0.83       0.89333333 0.88333333 0.87666667 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89       0.89666667 0.89666667 0.89       0.89333333\n",
      " 0.82333333 0.82333333 0.77666667 0.82333333 0.89333333 0.88333333\n",
      " 0.87333333 0.88       0.9        0.89666667 0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.78333333 0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89666667 0.89666667\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83       0.83\n",
      " 0.78       0.83       0.89333333 0.88333333 0.87666667 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89       0.89666667 0.89666667 0.89       0.89333333\n",
      " 0.83333333 0.83333333 0.77666667 0.83333333 0.89       0.88\n",
      " 0.87333333 0.88       0.89666667 0.89333333 0.89       0.9\n",
      " 0.88666667 0.89333333 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.77666667 0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.9        0.89666667\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83333333 0.83333333\n",
      " 0.77666667 0.83333333 0.89333333 0.88333333 0.87333333 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89       0.89666667 0.9        0.89       0.89333333\n",
      " 0.83333333 0.83333333 0.77666667 0.83333333 0.89       0.88\n",
      " 0.87333333 0.88       0.89333333 0.89333333 0.89       0.9\n",
      " 0.88666667 0.89333333 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.78       0.83\n",
      " 0.89333333 0.88333333 0.87666667 0.88       0.89666667 0.89666667\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83333333 0.83333333\n",
      " 0.77666667 0.83333333 0.89333333 0.88333333 0.87666667 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.89666667 0.9        0.89       0.89333333\n",
      " 0.82333333 0.82333333 0.77333333 0.82333333 0.89333333 0.88333333\n",
      " 0.87333333 0.88       0.9        0.89666667 0.89       0.9\n",
      " 0.88666667 0.89333333 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89333333 0.83       0.83       0.78       0.83\n",
      " 0.89333333 0.88333333 0.87333333 0.88       0.89666667 0.89666667\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89\n",
      " 0.89666667 0.9        0.89       0.89333333 0.83333333 0.83333333\n",
      " 0.77666667 0.83333333 0.89333333 0.88333333 0.87666667 0.88\n",
      " 0.9        0.89666667 0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.89666667 0.9        0.89       0.89333333\n",
      " 0.79666667 0.79666667 0.77333333 0.79666667 0.89       0.87333333\n",
      " 0.88       0.86333333 0.89333333 0.88       0.88666667 0.88333333\n",
      " 0.88666667 0.88333333 0.88666667 0.88666667 0.89666667 0.89666667\n",
      " 0.89       0.89333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.79       0.79       0.78       0.79       0.89       0.87333333\n",
      " 0.87666667 0.86666667 0.89333333 0.88333333 0.88666667 0.89\n",
      " 0.89       0.89       0.88333333 0.88666667 0.89666667 0.89666667\n",
      " 0.89       0.89333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.80666667 0.80666667 0.77666667 0.80666667 0.88666667 0.87666667\n",
      " 0.87333333 0.87333333 0.9        0.89666667 0.88666667 0.9\n",
      " 0.89       0.89666667 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.9               nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 1, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}.\n",
      " Score: 0.9033333333333333\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.87              nan 0.90666667        nan 0.88\n",
      "        nan 0.90666667        nan 0.89666667        nan 0.90666667\n",
      "        nan 0.91              nan 0.90666667        nan 0.91\n",
      "        nan 0.90666667        nan 0.87              nan 0.91666667\n",
      "        nan 0.88              nan 0.91666667        nan 0.9\n",
      "        nan 0.91666667        nan 0.91333333        nan 0.91666667\n",
      "        nan 0.91333333        nan 0.91666667        nan 0.87\n",
      "        nan 0.9               nan 0.88              nan 0.9\n",
      "        nan 0.89666667        nan 0.9               nan 0.91666667\n",
      "        nan 0.9               nan 0.91333333        nan 0.9\n",
      "        nan 0.87              nan 0.89666667        nan 0.88333333\n",
      "        nan 0.89666667        nan 0.9               nan 0.89666667\n",
      "        nan 0.91              nan 0.89666667        nan 0.91\n",
      "        nan 0.89666667        nan 0.87              nan 0.91333333\n",
      "        nan 0.88              nan 0.91333333        nan 0.89666667\n",
      "        nan 0.91333333        nan 0.91              nan 0.91333333\n",
      "        nan 0.91333333        nan 0.91333333        nan 0.87\n",
      "        nan 0.9               nan 0.88              nan 0.9\n",
      "        nan 0.89666667        nan 0.9               nan 0.91666667\n",
      "        nan 0.9               nan 0.92              nan 0.9\n",
      "        nan 0.87              nan 0.91              nan 0.87666667\n",
      "        nan 0.91              nan 0.88333333        nan 0.91\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.91              nan 0.87              nan 0.91\n",
      "        nan 0.88333333        nan 0.91              nan 0.89\n",
      "        nan 0.91              nan 0.91              nan 0.91\n",
      "        nan 0.91333333        nan 0.91              nan 0.87\n",
      "        nan 0.9               nan 0.88              nan 0.9\n",
      "        nan 0.89666667        nan 0.9               nan 0.91666667\n",
      "        nan 0.9               nan 0.92              nan 0.9\n",
      "        nan 0.87              nan 0.91              nan 0.88\n",
      "        nan 0.91              nan 0.9               nan 0.91\n",
      "        nan 0.91              nan 0.91              nan 0.91666667\n",
      "        nan 0.91              nan 0.87              nan 0.91666667\n",
      "        nan 0.88333333        nan 0.91666667        nan 0.9\n",
      "        nan 0.91666667        nan 0.91666667        nan 0.91666667\n",
      "        nan 0.91666667        nan 0.91666667        nan 0.87\n",
      "        nan 0.90333333        nan 0.88333333        nan 0.90333333\n",
      "        nan 0.89666667        nan 0.90333333        nan 0.91333333\n",
      "        nan 0.90333333        nan 0.91666667        nan 0.90333333\n",
      "        nan 0.87              nan 0.89              nan 0.88666667\n",
      "        nan 0.89              nan 0.90333333        nan 0.89\n",
      "        nan 0.91              nan 0.89              nan 0.91\n",
      "        nan 0.89              nan 0.87              nan 0.91333333\n",
      "        nan 0.88              nan 0.91333333        nan 0.9\n",
      "        nan 0.91333333        nan 0.91333333        nan 0.91333333\n",
      "        nan 0.91333333        nan 0.91333333        nan 0.87\n",
      "        nan 0.9               nan 0.88333333        nan 0.9\n",
      "        nan 0.89666667        nan 0.9               nan 0.91666667\n",
      "        nan 0.9               nan 0.92              nan 0.9\n",
      "        nan 0.87              nan 0.91              nan 0.87666667\n",
      "        nan 0.91              nan 0.88              nan 0.91\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.91              nan 0.87              nan 0.91333333\n",
      "        nan 0.88              nan 0.91333333        nan 0.89333333\n",
      "        nan 0.91333333        nan 0.91333333        nan 0.91333333\n",
      "        nan 0.91666667        nan 0.91333333        nan 0.87\n",
      "        nan 0.9               nan 0.88333333        nan 0.9\n",
      "        nan 0.89666667        nan 0.9               nan 0.91666667\n",
      "        nan 0.9               nan 0.92              nan 0.9\n",
      "        nan 0.86666667        nan 0.76              nan 0.86333333\n",
      "        nan 0.76              nan 0.86666667        nan 0.76\n",
      "        nan 0.87333333        nan 0.76              nan 0.87333333\n",
      "        nan 0.76              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.76666667        nan 0.86666667\n",
      "        nan 0.76666667        nan 0.87              nan 0.76666667\n",
      "        nan 0.87333333        nan 0.76666667        nan 0.87\n",
      "        nan 0.76666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.79              nan 0.86666667\n",
      "        nan 0.79              nan 0.86666667        nan 0.79\n",
      "        nan 0.87333333        nan 0.79              nan 0.87333333\n",
      "        nan 0.79              nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 2000, 'features__text_features__vectorizer__min_df': 10, 'lr__C': 10, 'lr__penalty': 'l2'}.\n",
      " Score: 0.9200000000000002\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    \n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_politician_count_1000', 'followers_politician_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-subdivision",
   "metadata": {},
   "source": [
    "## Lemmatization and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fleet-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training_set.copy()\n",
    "train['textdata'] = clean_text(train['name'] + ' ' + train['description'] + ' ' + train['recent_100_tweets'])\n",
    "train['textdata'] = train['textdata'].apply(lambda row: tokenize_lemmatize(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "train['textdata'] = train['textdata'].apply(lambda row: ' '.join(row))\n",
    "X = train\n",
    "y = train.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "joined-production",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Support Vector Machine -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.88       0.75       0.85       0.85333333 0.91333333 0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.91       0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.90333333 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.91333333 0.76333333 0.87666667 0.83333333 0.89666667 0.77666667\n",
      " 0.88       0.83333333 0.89333333 0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90333333 0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.9        0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91333333 0.75666667 0.86666667 0.83333333 0.90333333 0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90333333 0.76333333 0.87666667 0.83333333 0.90333333 0.77666667\n",
      " 0.88       0.83333333 0.90333333 0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.9        0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.91       0.75666667 0.86666667 0.83333333\n",
      " 0.90333333 0.76333333 0.87666667 0.83333333 0.90333333 0.77666667\n",
      " 0.88       0.83333333 0.90333333 0.77666667 0.89333333 0.83\n",
      " 0.88666667 0.75       0.85       0.85333333 0.91       0.75666667\n",
      " 0.86666667 0.83333333 0.91       0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.90333333 0.77666667\n",
      " 0.89333333 0.83       0.88333333 0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.90666667 0.76333333\n",
      " 0.87666667 0.83333333 0.89333333 0.77666667 0.88       0.83333333\n",
      " 0.89666667 0.77666667 0.89333333 0.83       0.88333333 0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.9        0.77666667\n",
      " 0.88       0.83333333 0.9        0.77666667 0.89333333 0.83\n",
      " 0.88333333 0.75       0.85       0.85333333 0.90666667 0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.9        0.77666667 0.88       0.83333333 0.90666667 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.90666667 0.75666667 0.86666667 0.83333333 0.91       0.76333333\n",
      " 0.87666667 0.83333333 0.90333333 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.91       0.77666667 0.89333333 0.83\n",
      " 0.88       0.75       0.85       0.85333333 0.90333333 0.75666667\n",
      " 0.86666667 0.83333333 0.90666667 0.76333333 0.87666667 0.83333333\n",
      " 0.90333333 0.77666667 0.88       0.83333333 0.90666667 0.77666667\n",
      " 0.89333333 0.83       0.88       0.75       0.85       0.85333333\n",
      " 0.91       0.75666667 0.86666667 0.83333333 0.91333333 0.76333333\n",
      " 0.87666667 0.83333333 0.90666667 0.77666667 0.88       0.83333333\n",
      " 0.90666667 0.77666667 0.89333333 0.83       0.88       0.75\n",
      " 0.85       0.85333333 0.90666667 0.75666667 0.86666667 0.83333333\n",
      " 0.91       0.76333333 0.87666667 0.83333333 0.90666667 0.77666667\n",
      " 0.88       0.83333333 0.91       0.77666667 0.89333333 0.83\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87666667 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.85666667 0.77666667 0.88       0.83       0.85333333 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87333333 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.85666667 0.77666667 0.88       0.83       0.84333333 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87333333 0.75       0.85       0.85333333 0.87666667 0.75666667\n",
      " 0.86666667 0.83666667 0.87333333 0.76333333 0.87666667 0.83666667\n",
      " 0.86       0.77666667 0.88       0.83       0.85666667 0.77666667\n",
      " 0.89333333 0.82666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'svm__C': 0.5, 'svm__kernel': 'linear'}.\n",
      " Score: 0.9133333333333333\n",
      "\n",
      "\n",
      "------------------ kNN -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.82333333 0.82333333 0.78       0.82333333 0.88666667 0.87666667\n",
      " 0.87333333 0.87333333 0.89333333 0.89       0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78       0.81666667\n",
      " 0.88666667 0.87666667 0.87333333 0.87333333 0.89333333 0.89\n",
      " 0.89       0.9        0.88666667 0.89333333 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.81666667 0.81666667\n",
      " 0.78       0.81666667 0.88666667 0.87666667 0.87333333 0.87333333\n",
      " 0.89333333 0.89       0.89       0.9        0.88666667 0.89333333\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.9\n",
      " 0.82       0.82       0.78       0.82       0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89       0.89       0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78333333 0.81666667\n",
      " 0.88666667 0.87666667 0.87       0.87333333 0.89333333 0.89\n",
      " 0.89       0.89666667 0.88666667 0.89333333 0.88666667 0.89333333\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.82       0.82\n",
      " 0.78       0.82       0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89       0.89       0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.88666667 0.89333333 0.89666667 0.90333333 0.89       0.9\n",
      " 0.81333333 0.81333333 0.78       0.81333333 0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89333333 0.89333333 0.89       0.89666667\n",
      " 0.88666667 0.89333333 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.9        0.82       0.82       0.77666667 0.82\n",
      " 0.88666667 0.87666667 0.87       0.87333333 0.89333333 0.89333333\n",
      " 0.89       0.89666667 0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.89666667 0.90333333 0.89       0.9        0.82       0.82\n",
      " 0.78       0.82       0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89       0.89       0.89       0.89666667 0.88666667 0.89333333\n",
      " 0.88666667 0.89333333 0.89666667 0.90333333 0.89       0.9\n",
      " 0.81333333 0.81333333 0.78       0.81333333 0.88666667 0.87666667\n",
      " 0.87       0.87666667 0.89       0.89       0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89666667 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78       0.81666667\n",
      " 0.88666667 0.87666667 0.87       0.87666667 0.89       0.89\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.82333333 0.82333333\n",
      " 0.78       0.82333333 0.89       0.88       0.87333333 0.88\n",
      " 0.89333333 0.89       0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.90333333\n",
      " 0.81333333 0.81333333 0.77666667 0.81333333 0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89333333 0.89       0.89       0.9\n",
      " 0.89       0.89666667 0.88666667 0.89666667 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81333333 0.81333333 0.78       0.81333333\n",
      " 0.88666667 0.87666667 0.87       0.87333333 0.89333333 0.89\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.81333333 0.81333333\n",
      " 0.77666667 0.81333333 0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89333333 0.89       0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.90333333\n",
      " 0.81666667 0.81666667 0.77333333 0.81666667 0.88666667 0.87666667\n",
      " 0.87       0.87333333 0.89333333 0.89       0.89       0.89666667\n",
      " 0.89       0.89666667 0.88666667 0.89333333 0.89666667 0.90333333\n",
      " 0.89       0.90333333 0.81666667 0.81666667 0.78       0.81666667\n",
      " 0.88666667 0.87666667 0.87333333 0.87333333 0.89333333 0.89\n",
      " 0.89       0.9        0.89       0.89666667 0.88666667 0.89666667\n",
      " 0.89666667 0.90333333 0.89       0.90333333 0.81333333 0.81333333\n",
      " 0.77666667 0.81333333 0.88666667 0.87666667 0.87       0.87333333\n",
      " 0.89333333 0.89       0.89       0.9        0.89       0.89666667\n",
      " 0.88666667 0.89666667 0.89666667 0.90333333 0.89       0.90333333\n",
      " 0.79666667 0.79666667 0.77333333 0.79666667 0.89       0.87333333\n",
      " 0.87666667 0.86666667 0.89333333 0.88       0.88666667 0.88666667\n",
      " 0.88666667 0.88333333 0.88333333 0.88666667 0.89666667 0.89666667\n",
      " 0.89       0.89333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8        0.8        0.78       0.8        0.89333333 0.88\n",
      " 0.87666667 0.87       0.89333333 0.88666667 0.88666667 0.89333333\n",
      " 0.89       0.89333333 0.88333333 0.89       0.89666667 0.9\n",
      " 0.89       0.89666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82       0.82       0.77666667 0.82       0.88666667 0.87333333\n",
      " 0.87       0.87       0.9        0.89333333 0.89       0.89333333\n",
      " 0.89       0.89333333 0.88666667 0.89       0.89666667 0.9\n",
      " 0.89       0.89666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': 1000, 'features__text_features__vectorizer__min_df': 1, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}.\n",
      " Score: 0.9033333333333333\n",
      "\n",
      "\n",
      "------------------ Logistic Regression -------------------\n",
      "\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.87333333        nan 0.89333333        nan 0.88333333\n",
      "        nan 0.89333333        nan 0.90666667        nan 0.89333333\n",
      "        nan 0.91              nan 0.89333333        nan 0.90333333\n",
      "        nan 0.89333333        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.88333333        nan 0.90333333        nan 0.90333333\n",
      "        nan 0.90333333        nan 0.91              nan 0.90333333\n",
      "        nan 0.90666667        nan 0.90333333        nan 0.87333333\n",
      "        nan 0.9               nan 0.88333333        nan 0.9\n",
      "        nan 0.90333333        nan 0.9               nan 0.91\n",
      "        nan 0.9               nan 0.90333333        nan 0.9\n",
      "        nan 0.87              nan 0.88666667        nan 0.88333333\n",
      "        nan 0.88666667        nan 0.9               nan 0.88666667\n",
      "        nan 0.91              nan 0.88666667        nan 0.90666667\n",
      "        nan 0.88666667        nan 0.87              nan 0.89\n",
      "        nan 0.88333333        nan 0.89              nan 0.9\n",
      "        nan 0.89              nan 0.91              nan 0.89\n",
      "        nan 0.90666667        nan 0.89              nan 0.87\n",
      "        nan 0.89              nan 0.88333333        nan 0.89\n",
      "        nan 0.90333333        nan 0.89              nan 0.91333333\n",
      "        nan 0.89              nan 0.90333333        nan 0.89\n",
      "        nan 0.87              nan 0.90333333        nan 0.88333333\n",
      "        nan 0.90333333        nan 0.88666667        nan 0.90333333\n",
      "        nan 0.91              nan 0.90333333        nan 0.91333333\n",
      "        nan 0.90333333        nan 0.87              nan 0.91\n",
      "        nan 0.88333333        nan 0.91              nan 0.89333333\n",
      "        nan 0.91              nan 0.91666667        nan 0.91\n",
      "        nan 0.91              nan 0.91              nan 0.87\n",
      "        nan 0.89              nan 0.88333333        nan 0.89\n",
      "        nan 0.90333333        nan 0.89              nan 0.91333333\n",
      "        nan 0.89              nan 0.90333333        nan 0.89\n",
      "        nan 0.87333333        nan 0.9               nan 0.9\n",
      "        nan 0.9               nan 0.90666667        nan 0.9\n",
      "        nan 0.91              nan 0.9               nan 0.91\n",
      "        nan 0.9               nan 0.87333333        nan 0.87666667\n",
      "        nan 0.89666667        nan 0.87666667        nan 0.90666667\n",
      "        nan 0.87666667        nan 0.91              nan 0.87666667\n",
      "        nan 0.90666667        nan 0.87666667        nan 0.87333333\n",
      "        nan 0.89              nan 0.89333333        nan 0.89\n",
      "        nan 0.90666667        nan 0.89              nan 0.91333333\n",
      "        nan 0.89              nan 0.91333333        nan 0.89\n",
      "        nan 0.87333333        nan 0.89              nan 0.89666667\n",
      "        nan 0.89              nan 0.90333333        nan 0.89\n",
      "        nan 0.91              nan 0.89              nan 0.91\n",
      "        nan 0.89              nan 0.87333333        nan 0.87333333\n",
      "        nan 0.89333333        nan 0.87333333        nan 0.90666667\n",
      "        nan 0.87333333        nan 0.91333333        nan 0.87333333\n",
      "        nan 0.90666667        nan 0.87333333        nan 0.87333333\n",
      "        nan 0.90666667        nan 0.89              nan 0.90666667\n",
      "        nan 0.90666667        nan 0.90666667        nan 0.91\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.87333333        nan 0.88333333        nan 0.88333333\n",
      "        nan 0.88333333        nan 0.89666667        nan 0.88333333\n",
      "        nan 0.90666667        nan 0.88333333        nan 0.91333333\n",
      "        nan 0.88333333        nan 0.87333333        nan 0.90333333\n",
      "        nan 0.88666667        nan 0.90333333        nan 0.90666667\n",
      "        nan 0.90333333        nan 0.91666667        nan 0.90333333\n",
      "        nan 0.91333333        nan 0.90333333        nan 0.87333333\n",
      "        nan 0.90666667        nan 0.89              nan 0.90666667\n",
      "        nan 0.90666667        nan 0.90666667        nan 0.91\n",
      "        nan 0.90666667        nan 0.91              nan 0.90666667\n",
      "        nan 0.86666667        nan 0.78666667        nan 0.86666667\n",
      "        nan 0.78666667        nan 0.86666667        nan 0.78666667\n",
      "        nan 0.87              nan 0.78666667        nan 0.87\n",
      "        nan 0.78666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.77333333        nan 0.86666667\n",
      "        nan 0.77333333        nan 0.87              nan 0.77333333\n",
      "        nan 0.86666667        nan 0.77333333        nan 0.86666667\n",
      "        nan 0.77333333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86666667        nan 0.79666667        nan 0.86666667\n",
      "        nan 0.79666667        nan 0.86666667        nan 0.79666667\n",
      "        nan 0.87              nan 0.79666667        nan 0.87\n",
      "        nan 0.79666667        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Params: {'features__text_features__vectorizer__max_df': 0.5, 'features__text_features__vectorizer__max_features': None, 'features__text_features__vectorizer__min_df': 5, 'lr__C': 5, 'lr__penalty': 'l2'}.\n",
      " Score: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_politician_count_1000', 'followers_politician_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('------------------ Support Vector Machine -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'svm__C' : [0.1,0.5,1,5,10],\n",
    "              'svm__kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ kNN -------------------\\n')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'knn__n_neighbors': [1,2,3,4,5,6,7,8,9,10],\n",
    "              'knn__weights': ['uniform', 'distance']\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "\n",
    "print('\\n\\n------------------ Logistic Regression -------------------\\n')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer()),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Paramters for optimization\n",
    "parameters = {'features__text_features__vectorizer__max_df': [0.5, 0.75, 1],\n",
    "              'features__text_features__vectorizer__min_df': [1, 5, 10],\n",
    "              'features__text_features__vectorizer__max_features': [1000, 2000, None],\n",
    "              'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'lr__C': [0.1, 0.5, 1, 5, 10]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs = 4, verbose=1)\n",
    "grid.fit(X, y)\n",
    "    \n",
    "print(f' Best Params: {grid.best_params_}.\\n Score: {grid.best_score_}')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-visibility",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fifth-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = training_set.parliament_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "scenic-brazilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('numeric_features',\n",
       "                                                 Pipeline(steps=[('selector_num',\n",
       "                                                                  FunctionTransformer(func=<function get_numeric_data_ at 0x7f8c4d7d9550>))])),\n",
       "                                                ('text_features',\n",
       "                                                 Pipeline(steps=[('selector_text',\n",
       "                                                                  FunctionTransformer(func=<function get_text_data_ at 0x7f8c4d7d95e0>)),\n",
       "                                                                 ('vectorizer',\n",
       "                                                                  TfidfVectorizer(max_df=0.5,\n",
       "                                                                                  max_features=2000,\n",
       "                                                                                  min_df=10))]))])),\n",
       "                ('lr', LogisticRegression(C=10, max_iter=1000))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_data_(df):\n",
    "    df = df.copy()\n",
    "    df['textdata'] = clean_text(df['name']+ ' ' + df['description'] + ' ' + df['recent_100_tweets'])\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: remove_stopwords(row))\n",
    "    df['textdata'] = df['textdata'].apply(lambda row: ' '.join(row))\n",
    "    return df.textdata\n",
    "\n",
    "get_text_data = FunctionTransformer(get_text_data_)\n",
    "\n",
    "\n",
    "def get_numeric_data_(df):\n",
    "    data = df[['friends_politician_count_1000', 'followers_politician_count_1000']].to_numpy()\n",
    "    return data\n",
    "\n",
    "get_numeric_data = FunctionTransformer(get_numeric_data_)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector_num', get_numeric_data)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector_text', get_text_data),\n",
    "                ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=2000, min_df=10)),\n",
    "            ]))\n",
    "         ])),\n",
    "     ('lr', LogisticRegression(max_iter=1000, penalty='l2',  C=10))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "lyric-economy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../classifiers/classifier_politician_ndtfrfo.sav']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../classifiers/classifier_politician_ndtfrfo.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-awareness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
